SHLOMO KASHANI
DEEP LEARNING INTER VIEWS
By Shlomo Kashani, M.Sc, QMUL, UK.
Published by Shlomo Kashani, T el-A viv , ISRAEL.
V isit: http://www .interviews.ai  
Copyright, 2020This book is protected by copyright.
No part may be reproduced in any manner without written permission from
the publisher .
Printing version: VER  . 7  TH  D  ECEMBER  2020
Printed in the United States of America  .
Library of Congress Cataloging-in-Publication Data
A catalog r ecord for this book is available fr om the Library of
Congr essCOPYRIGHT .
© 2016-2012 Shlomo Kashani, entropy@interviews.ai
LL RIGHTS RESER VED  . N o part of this publication  may be
reproduced, stor ed in a retrieval system, or transmitted in any
form or by any  means, electronic, mechanical, photocopying,
recording, scanning, or otherw ise, except as permitted under
Section 107 or 108 of the 1976 United States Copyright Act, without the
prior written permission of the Publisher .
IMIT OF  L IABILITY  /D ISCLAIMER  of W arranty . While the publisher
and author have used their best ef forts in preparing this book,
they make no re presentations or warranties with respect to th e
accuracy or completeness of the contents of this book and
specifically disclaim any implied warranties of merchantability or fitness
for a  pa rticular purpose. No warranty may be created or extended by sales
representatives or written sales materials. The advice and strategies
contained herein  may not be suitable for your situation. Y ou should consult
with a professional where app ropriate. Neither the publisher  nor author
shall be liable for any loss of profit or any other commercial damages,
including but not limited to s pecial, incidental, consequential, or other
damages.
OTICES. Knowledge and best practice in this field are
constantly changing. As new research and experience broaden
our understandi ng, changes in research methods, professiona l
practices, or medical treatm ent may become necessary .
Practitioners and researchers mu st always rely on their own experience and
knowledge in evaluating and using any information, methods, compounds,or ex periments described herein . In using such information or methods they
should b e mind ful of their own  safety and the safety of others, including
parties f or whom they have a  professional responsibility . T o the fullest
extent of the law , neither the Publisher nor the authors, contributors, or
editors, assume any liability for any injury and/or damage to  persons or
property as a m atter of products liability , negligence or otherwise, or from
any use or ope ration of any methods, products, instruction s, or ideas
contained in the material herein.FOREW ORD .
W e will build a machine that will fly  .
— Joseph Michael Montgolfier , French Inventor/Aeronaut (1740-1810)
EEP lea rning interviews are technical, dense, and thanks to the fields
competitiveness, often high-stakes. The prospect of preparing for one
can be daunting , and the fear of failure can be paralyzing and many
interviewees find their ideas slipping away alongside their confidence.
This boo k was written for you: an aspiring data scientist with a quantitative
background, fac ing down the gauntlet of the interview process in an increasingly
competitive fiel d. For most of you, the interview process is the most significant
hurdle between you and a dream job. Even though you ha ve the ability , the
background, and  the motivation to excel in your tar get position, you might need
some guidance on how to get your foot in the door .
Though this book is highly te chnical it is not too dense to work through
quickly . It aims to be comprehensive, including many of the  terms and topics
involved in modern data scienc e and deep learning. That thor oughness makes it
unique; no other single work of fers such breadth of lea rning tar geted so
specifically at the demands of the interview .
Most comparab le information is available in a variety of form ats, locations,
structures, and resourcesblog po sts, tech articles, and short boo ks scattered across
the internet. Those resources a re simply not adequate to the  demands of deep
learning interview or exam preparation and were not assembled with this explicit
purpose in mind. It is hoped that this book does not suf fer the same shortcomings.
HIS boo ks creation was guided by a few key principles: clarity and
depth, thoroughness and precision, interest and accuracy . The volume
was designed for use by job seekers in the fields of machine learning
and deep learning whose abiliti es and background locate them firmly
within S TEM (science, technology , engineering, and mathematics). The book will
still be of use to other readers, such as those still under going their initial education
in a STEM field.However , it is tailored most directly to the needs of active j ob seek ers and
students attending M.Sc/Ph.D pr ogrammes in AI  . It is, in any case, a book for
engineers, mathematicians, and computer scientists: nowhere does it include the
kind of very basic background m aterial that would allow it to be read by someone
with no prior knowledge of quantitative and mathematical processes.
The boo ks conte nts are a lar ge inventory of numerous topics re levant to deep
learning job interviews and gr aduate level exams. Ideas that are interesting or
pertinent have been excluded if they are not valuable in that context. That places
this work at the  forefront of th e growing trend in education and in business to
emphasize a core set of practical mathematical and computational skills. It is now
widely understo od that the training of every computer scientist must include a
course d ealing with the fundamental theorems of machine learning in a rigorous
manner; Deep Learning appears  in the curriculum of nearly every university; and
this volu me is designed as a co nvenient ongoing reference for  graduates of such
courses and programs.
The boo k is grounded in both academic expertise and on-the- job experience
and thus  has two goals. First, it compresses all of the necessary information into a
coherent package. And second, it renders that information accessible and makes it
easy to navigate. As a result, the book helps the reader develop a thorough
understanding of the principles and concepts underlying practical data science.
None of the textbooks I read met all of those needs, which are:
1 . Appr opriate pr esentation level  . I wante d a friendly introductor y text acc
to g raduate students who hav e not had extensive applied experience a
scientists.
2 . A text that is rigor ous  and builds a solid understanding of the subject w
getting bogged down in too many technicalities.
3 . Logical and notational con sistency among topics  . There  are in
connections between calculus, logistic regression, entropy , and deep le
theory , w hich I feel need to be emphasized and elucidated if the reader is t
understand the field. Dif ferences in notation and presentation style in e
sources make it very dif ficult for students to appreciate these kinds of conne
4 . Manageable size  . I t is very useful to have a text compact enough that all
material in it can be covered in few weeks or months of intensive review
candidates will have only that m uch time to prepare for an interview , so a
text is of no use to them.The text that follows is an attempt to meet all of the above challenges. It will
inevitably prove  more successful at handling some of them than others, but
it has at least made a sincere and devoted ef fort.
A note about Bibliography
The boo k provid es a carefully cu rated bibliography to guide further study , whether
for i nterview preparation or sim ply as a matter of interest or job -relevant research.
A comprehensive bibliography would be far too long to include here, and would be
of li ttle immediate use, so the se lections have been made with d eliberate attention
to the value of each included text.
Only the  most important books and articles on each topic have been included,
and only those w ritten in English that I personally consulted. Each is given a brief
annotation to indicate its scope and applicability . Many of the w orks cited will be
found to  includ e very full bibl iographies of the particular subject treated, and I
recommend turning there if you wish to dive deeper into a spec ific topic, method,
or process.
W e have a web  page for this book, where we list errata, exam ples, and any
additional information. Y ou can access this page at: http://www .interviews.ai  . T o
comment or ask technical questions about this book, send email to:
entropy@interviews.ai  .
I wo uld also li ke to solicit corr ections, criticisms, and suggestions fr om
students and other r eaders. Although I have tried to eliminate err ors over the
multi year pr ocess of writing and r evising this text, a few undoubtedly r emain.
In particular , some typographical infelicities will no doubt f ind their way into
the final version. I hope you will forgive them  .
T HE  A UTHOR  .
T EL  A VIV  ISRAEL, D ECEMBER  , 2020. F IRST PRINTING  , D ECEMBER  2020.ACKNO WLEDGEMENTS .
The tha nks and acknowledgem ents of the publisher are due to the
following: My dear son, Amir I vry , Matthew Isaac Harvey , Sandy Noymer ,
Steve Foot and V elimir Gayevskiy .A  UTHOR  ’  S  B  IOGRAPHY  .
When Shlomo  typed his book in
LA TEX, he wanted it to reflect some of his
passions: AI, design, typograph y , and most
notably coding. On a typical day , his two
halves - the scientist and the artist - spend
hours meticulously designing AI systems,
from epilepsy prediction and pulmonary
nodule detectio n, to training a computer -
vision model on a cluster .
Shlomo spends whole days in a lab full
of GPUs working on his many interesting
research projects. Though research satisfies
his itch for dis covery , his most important
scientific contribution, he says, is helping
other researchers.
And the  results are evident in his publications. But, although th eoretical
studies are important, practical experience has many great virtues. As the
Head of AI at DeepOncology , he developed uses of Deep L earning for
precise tumour detection, expan ding and refining what human experts are
capable of. The work, which relies on CNN’ s, marks the culmination of a
career spent applying AI techniques to problems in medical AI. Shlomo
holds an MSc in  Digital Signal Processing (Distinction) from the University
of London.
A  PERSONAL NOTE  :  In this first volume, I purposely present a coherent,
cumulative, and  content-specific  cor e curriculum  of the data science field,
including topics such as information theory , Bayesian statistics, algorithmic
dif ferentiation, logistic regression, perceptrons, and convolutional neural
networks.I hope you will find this book s timulating. It is my belief that you the
postgraduate students and job-seekers  for whom the book is primarily
meant w ill benefit from reading it; however , it is my hope th at even the
most experienced researchers will find it fascinating as well.
S  HLOMO  K  ASHANI  , T  EL  -A  VIV  , ISRAEL.A  BOUT THE  C  HIEF  E  DIT OR
.
Amir Ivry  is an artificial intelligence scientist,
recognized for his pioneer work on speech-based
applications. He pursues a direct track PhD
(Electrical Engineering, the T echnion, ISRAEL).
At 27, the laureate of over dozen awards and
accomplishments, including the prestigious Jacobs
A ward for Excellent Researchers (19, 20), Amir
has been fortunate enough to  make significant
contributions to AI over nine years of academic
and indu strial ca reer . Setting far -reaching goals, he
holds po sitions a s senior technological lecturer and
a st rategic consultant to startu ps and corporates,
while being an active publishing author of IEEE papers.Contents
I    R u s t y  N a i l
HOW -T O USE THIS BOOK
Introduction
What makes this book so valuable
What will I learn
How to W ork Problems
T ypes of Problems
I I    K i n d e r g a r t e n
LOGISTIC REGRESSION
Introduction
Problems
General Concepts
Odds, Log-odds
The Sigmoid
T ruly Understanding Logistic Regression
The Logit Function and Entropy
Python/PyT orch/CPP
Solutions
General ConceptsOdds, Log-odds
The Sigmoid
T ruly Understanding Logistic Regression
The Logit Function and Entropy
Python, PyT orch, CPP
PROBABILISTIC PROGRAMMING & BA YESIAN DL
Introduction
Problems
Expectation and V ariance
Conditional Probability
Bayes Rule
Maximum Likelihood Estimation
Fisher Information
Posterior & prior predictive distributions
Conjugate priors
Bayesian Deep Learning
Solutions
Expectation and V ariance
Conditional Probability
Bayes Rule
Maximum Likelihood Estimation
Fisher Information
Posterior & prior predictive distributions
Conjugate priors
Bayesian Deep LearningI I I    H i g h  S c h o o l
INFORMA TION THEOR Y
Introduction
Problems
Logarithms in Information Theory
Shannon’ s Entropy
Kullback-Leibler Diver gence (KLD)
Classification and Information Gain
Mutual Information
Mechanical Statistics
Jensen’ s inequality
Solutions
Logarithms in Information Theory
Shannon’ s Entropy
Kullback-Leibler Diver gence
Classification and Information Gain
Mutual Information
Mechanical Statistics
Jensen’ s inequality
DEEP LEARNING: CALCULUS, ALGORITHMIC
DIFFERENTIA TION
Introduction
Problems
AD, Gradient descent & Backpropagation
Numerical dif ferentiation
Directed Acyclic Graphs
The chain ruleT aylor series expansion
Limits and continuity
Partial derivatives
Optimization
The Gradient descent algorithm
The Backpropagation algorithm
Feed forward neural networks
Activation functions, Autograd/JAX
Dual numbers in AD
Forward mode AD
Forward mode AD table construction
Symbolic dif ferentiation
Simple dif ferentiation
The Beta-Binomial model
Solutions
Algorithmic dif ferentiation, Gradient descent
Numerical dif ferentiation
Directed Acyclic Graphs
The chain rule
T aylor series expansion
Limits and continuity
Partial derivatives
Optimization
The Gradient descent algorithm
The Backpropagation algorithm
Feed forward neural networksActivation functions, Autograd/JAX
Dual numbers in AD
Forward mode AD
Forward mode AD table construction
Symbolic dif ferentiation
Simple dif ferentiation
The Beta-Binomial model
I V    Ba c h e l o r s
DEEP LEARNING: NN ENSEMBLES
Introduction
Problems
Bagging, Boosting and Stacking
Approaches for Combining Predictors
Monolithic and Heterogeneous Ensembling
Ensemble Learning
Snapshot Ensembling
Multi-model Ensembling
Learning-rate Schedules in Ensembling
Solutions
Bagging, Boosting and Stacking
Approaches for Combining Predictors
Monolithic and Heterogeneous Ensembling
Ensemble Learning
Snapshot Ensembling
Multi-model EnsemblingLearning-rate Schedules in Ensembling
DEEP LEARNING: CNN FEA TURE EXTRACTION
Introduction
Problems
CNN as Fixed Feature Extractor
Fine-tuning CNNs
Neural style transfer , NST
Solutions
CNN as Fixed Feature Extractor
Fine-tuning CNNs
Neural style transfer
DEEP LEARNING
Introduction
Problems
Cross V alidation
Convolution and correlation
Similarity measures
Perceptrons
Activation functions (rectification)
Performance Metrics
NN Layers, topologies, blocks
T raining, hyperparameters
Optimization, Loss
Solutions
Cross V alidation
Convolution and correlation
Similarity measuresPerceptrons
Activation functions (rectification)
Performance Metrics
NN Layers, topologies, blocks
T raining, hyperparameters
Optimization, Loss
V    P r a c t i c e  Ex a m
JOB INTER VIEW MOCK EXAM
Rules
Problems
Perceptrons
CNN layers
Classification, Logistic regression
Information theory
Feature extraction
Bayesian deep learning
V I    V o l u m e  t w o
V OLUME TWO - PLAN
Introduction
AI system design
Advanced CNN topologies
1D CNN’ s
3D CNN’ s
Data augmentations
Object detectionObject segmentation
Semantic segmentation
Instance segmentation
Image classification
Image captioning
NLP
RNN
LSTM
GANs
Adversarial attacks and defences
V ariational auto encoders
FCN
Seq2Seq
Monte carlo, ELBO, Re-parametrization
T ext to speech
Speech to text
CRF
Quantum computing
RLP A R T  I
R USTY NAILC HAPTER
1     HO W -TO USE THIS BOOK
The true logic of this world is in the  calculus  of
pr obabilities  .
— James C. Maxwell
C o n t e n t s
Intr oduction
What makes this book so valuable
What will I learn
Starting Y our Career
Advancing Y our Career
Diving Into Deep Learning
How to W ork Problems
T ypes of Problems
1 . 1   I n t r o d u c t i o n
First of all, welcome to world of Deep Learning Interviews.
1.1.1   What makes this book so valuableARGETED advertising. Deciphering dead languages. Detecting
malignant tumours. Predicting n atural disasters. Every year we
see doz ens of new uses for deep learning emer ge from
corporate R&R , academia, and plucky entrepreneurs.
Increasingly , deep learning and artificial intelligence are in-grained in our
cultural consciousness. Leading universities are dedicating p rograms to
teaching them, and they make the headlines every few days.
That me ans jobs. It means inten se demand and intense competition. It
means a generation of data scientists and machine learning engineers
making their way into the workforce and using deep learning to change
how things work. This book is f or them, and for you. It is aimed at current
or a spiring experts and students in the field possessed of  a strong
grounding in mathematics, an active imagination, engaged creativity , and
an appre ciation for data. It is hand-tailored to give you the best possible
preparation for deep learning job interviews by guiding you through
hundreds of fully solved questions.
That is what makes the volume so specifically valuable to students and
job seek ers: it provides them with the ability to speak confidently and
quickly on any relevant topic, to answer technical questions clearly and
correctly , and to fully understand the purpose and meaning of interview
questions and answers.
Those are powerful, indispensable advantages to have when walking
into the interview room.
The que stions and problems the book poses are tough enough  to cut
your tee th on-and to dramatically improve your skills but theyre framed
within thought provoking questions, powerful and engaging stories, and
cutting edge scientific informati on. What are bosons and fermions? What
is choriionic villus? Where did the Ebola virus first appear , and how does
it spread? Why is binary options trading so dangerous?
Y our curiosity will pull you thro ugh the books problem sets, fo rmulas,
and inst ructions, and as you pro gress, youll deepen your understanding of
deep learning. There are intricate connections between calculus, logistic
regression, entropy , and deep learning theory; work through the book, and
those connections will feel intuitive.
1.1.2   What will I learnStarting Y our Career
Are you actively pursuing a career in deep learning and data science, or
hoping to do so ? If so, you’re  in luckeverything from deep learning to
artificial intellig ence is in extremely high demand in the contemporary
workforce. Deep learning profe ssionals are highly sought after and also
find themselves among the highest-paid employee groups in companies
around the world.
So your  career choice is spot on, and the financial and inte llectual
benefits of landing a solid job a re tremendous. But those positions have a
high barrier to e ntry: the deep learning interview . These interviews have
become their own tiny industry , with HR employees having to specialize in
the relev ant topics so as to distinguish well-prepared job candi dates from
those w ho simply have a loo se working knowledge of the material.
Outside the interview itself, the  dif ference doesnt always feel important.
Deep learning libraries are so g ood that a machine learning p ipeline can
often be assembled with litt le high-skill input from the researcher
themselves. But that level of ability wont cut it in the interview . Y oull be
asked practical questions, technical questions, and theoretical  questions,
and expected to answer them all confidently and fluently .
For unprepared candidates, tha ts the end of the road. Many g ive up
after repeated post-interview rejections.
Advancing Y our Career
Some of you wi ll be more conf ident. Those of you with years on the job
will be highly m otivated, excep tionally numerate, and prepared to take an
active, h ands-on role in deep lea rning projects. Y ou probably already have
extensive knowledge in applied  mathematics, computer science , statistics,
and economics. Those are all formidable advantages.
But at the same time, its unlike ly that you will have prepared for the
interview itself. Deep learning interviewsespecially those for the most
interesting, autonomous, and c hallenging positionsdemand that you not
only know how to do your job but that you display that knowled ge clearly ,
eloquently , and without hesitation. Some questions will be straightforward
and fam iliar , but others might be  farther afield or draw on areas you havent
encountered since college.
There is simply no reason to lea ve that kind of thing to chance. Make
sure youre prepared. Confirm that you are up-to-date on terms, concepts,and algorithms. Refresh your memory of fundamentals, and how they
inform contemporary research practices. And when the interview comes,
walk into the room knowing that youre ready for whats coming your way .
Diving Into Deep Learning
“Deep Learning  Job Interviews” is or ganized into chapters that each
consist of an Introduction to a topic, Problems illustrating core aspects of
the topic, and complete Solutions. Y ou can expect each question and
problem in this volume to be c lear , practical, and relevant to t he subject.
Problems fall into two groups, conceptual and applica tion-based.
Conceptual problems are aimed  at testing and improving your knowledge
of b asic underlying concepts, while applications are tar geted at practicing
or applying what you’ve learned (most of these are relevant to Python and
PyT orch). The chapters are f ollowed by a reference list o f relevant
formulas and a selective bibliography for guide further reading.
1.1.3   How to W ork Problems
In real life, lik e in exams, you will encounter problems of varying
dif ficulty . A good skill to practice is recognizing the level of dif ficulty a
problem poses. Job interviews will have some easy problems, some
standard problems, and some much harder problems.
Each chapter of this book is usually or ganized into three sections:
Introduction, Pr oblems, and Solutions. As you are attempting to tackle
problems, resist the temptation to prematurely peek at the sol ution; It is
vital to allow y ourself to struggle for a time with the mate rial. Even
professional data scientists do not always know right away how  to resolve
a p roblem. The art is in gathering your thoughts and figuring ou t a strategy
to use what you know to find out what you don’ t.
PRB-1  
  CH.PRB- 1.1.
Pr oblems outlined in gr ey make up the  representative question set.
This set of pr obl ems is intended to cover the most essential ideas  in each
section. These pr oblems ar e usually highly typical of what you’d see on
an interv iew , alt hough some of t hem ar e atypical but carry an important
moral. If you find yourself un confident with the idea behind one ofthese, it’ s pr obably a good id ea to practice similar pr oblems. This
r epr esentative q uestion set is our suggestion for a minimal sele ction of
pr oblems to work on. Y ou ar e highly encouraged to work on mor e  .
SOL-1  
  CH.SOL- 1.1.  I am a solution  .
If you find yourself at a real stand-of f, go ahead and look for a clue in one
of th e re commended theory books. Think about it for a while, an d don’ t be
afraid to read back in the notes to look for a key idea that will help you
proceed. If you still can’ t so lve the problem, well, we in cluded the
Solutions sectio n for a reason! As you’re reading the solutions, try hard to
understand why  we took the steps we did, instead of memorizing step-by-
step how to solve that one particular problem.
If yo u st ruggled with a question  quite a lot, it’ s probably a good  idea to
return to  it in a  few days. That  might have been enough time for you to
internalize the necessary ideas, a nd you might find it easily conquerable. If
you’re s till having troubles, read over the solution again, with an emphasis
on u nderstandin g why each step  makes sense. One of the reasons so many
job candidates are required to demonstrate their ability to resolves data
science problem s on the board, is that it hiring managers assume it reflects
their true problem-solving skills.
In this volume, you will learn l ots of concepts, and be asked to apply
them in  a variety of situations . Often, this will involve answering one
really b ig problem by breaking it up into manageable chunks, solving
those ch unks, then putting the pieces back together . When you see a
particularly long  question, remain calm and look for a way to b reak it into
pieces you can handle.
1.1.4   T ypes of Problems
T wo main types of problems are presented in this book.
C ONCEPTUAL  : T he first categ ory is meant t o test and improve your
understanding of basic underlying concepts. These often involve many
mathematical calculations. They range in dif ficulty from very basic
reviews of definitions to proble ms that require you to be thoughtful about
the concepts covered in the section.An example in Information Theory follows.
PRB-2  
  CH.PRB- 1.2.
What is the distribution of maximum entr opy , that is, the distribution
which ha s the maximum entr opy among all distributions on the bounded
interval  [ a, b  ],( −∞  , + ∞  )
SOL-2  
  CH.SOL- 1.2.
The uniform distribution has the maximum entr opy among all
distributions on the bounded interval:  [ a, b  ],( −∞  , + ∞  ).
The variance of U  ( a, b  ) is σ  2 = 1 /  12( b − a  )2 .
Ther efor e the entr opy is:
A PPLICA TION  : Problems in this category are for practicing skills. It’ s
not enough to un derstand the philosophical grounding of an idea : you have
to be  ab le to apply it in appropr iate situations. This takes practice! mostly
in P ython or in  one of the available Deep Learning Librari es such as
PyT orch.
An example in PyT orch follows.
PRB-3  
  CH.PRB- 1.3.
Describe in your own wor ds, what is the purpose of the follo wing
code in the context of training a Convolutional Neural Network  .
1 self  .  transforms = []
2 if  r otate:
3 self  .  transforms  .  append(RandomRotate())
4 if  flip:
5 self  .  transforms  .  append(RandomFlip())SOL-3  
  CH.SOL- 1.3.
During the training of a Convolutional Neural Network, data
augmentation, and to some extent dr opout ar e used as cor e methods to
decr ease overfitting. Data augmentation is a r egularization scheme that
synthetically expands the d ata-set by utilizing label-pr eserving
transformations to add mor e invariant examples of the same data
samples. It is mo st commonly performed in r eal time on the CPU  during
the train ing phase whilst the actual training mode takes plac e on the
GPU. This may consist for ins tance, random r otations, random flips,
zooming, spatial translations etc  .
P A R T  II
KINDER GAR TENC HAPTER
2     LOGISTIC REGRESSION
Y ou should call it entr opy for two r easons. In the first
place, your uncertainty function has been used in
statistical mechanics under that name. In the second
place, and mor e importantly , no one knows what entr opy
r eally is, so in a debate you will always have the
advantage  .
— John von Neumann to Claude Shannon
C o n t e n t s
Intr oduction
Pr oblems
General Concepts
Odds, Log-odds
The Sigmoid
T ruly Understanding Logistic Regression
The Logit Function and Entropy
Python/PyT orch/CPP
Solutions
General Concepts
Odds, Log-odds
The Sigmoid
T ruly Understanding Logistic RegressionThe Logit Function and Entropy
Python, PyT orch, CPP
2 . 1   I n t r o d u c t i o n
Ultivariable methods are rou tinely utilized in statistical
analyses across a wide range of domains. Logistic regression is
the most frequently used method for modelling binary response
data and binary classification. When the response variable is
binary , it characteristically ta kes the form of 1/0, with 1 normally
indicating a success and 0 a failure. Multivariable methods usua lly assume
a relatio nship be tween two or m ore independent, predictor var iables, and
one dependent, response variable. The predicted value of a response
variable may be expressed as a sum of products, wherein each product is
formed by multiplying the value of the variable and its coef ficient. How
the coe f ficients are computed ? from a respective data se t. Logistic
regression is heavily used in sup ervised machine learning and has become
the workhorse for both binary and multiclass classification problems.
Many of the q uestions introduced in this chapter are crucial for truly
understanding the inner -workings of artificial neural networks.
2 . 2   P r o b l e m s
2.2.1   General Concepts
PRB-4  
  CH.PRB- 2.1.
T rue or False:  For a fixed number of observations in a data set,
intr oducing mor e variables normally generates a model that has a better
fit t o the data. What may be the drawback of such a model fitting
strategy?
PRB-5  
  CH.PRB- 2.2.Define the term  “odds of success”  both qualitatively and formally .
Give a numerical example that str esses the r elation between pr obability
and odds of an event occurring  .
PRB-6  
  CH.PRB- 2.3.
1
.Define w hat is meant by the ter m  “interaction”  , in the context of a
logistic r egr ession pr edictor variable  .
2
.What is the simplest form of an interaction? W rite its formulae  .
3
.What st atistical tests can be used to attest the significance of an
interaction term?
PRB-7  
  CH.PRB- 2.4.
T rue or False:  In machine learning terminology , unsupervised
learning r efers to the mapping of input covariates to a tar get r esponse
variable that is attempted at being pr edicted when the labels ar e known  .
PRB-8  
  CH.PRB- 2.5.
Complete the following sentence:  In th e case of logistic r egr ession,
the r esponse variable is the log of the odds of being classified in [...]  .
PRB-9  
  CH.PRB- 2.6.
Describe how in a logistic r egr e ssion model, a transformation to the
r esponse variab le is applied to yield a pr obability distribution. Why is it
consider ed a mor e informative r epr esentation of the r esponse?
PRB-10  
  CH.PRB- 2.7.
Complete the following sentence:  Minimizing the negative log
likelihood also means maximizing the [...] of selecting the [...] class  .2.2.2   Odds, Log-odds
PRB-1 1  
  CH.PRB- 2.8.
Assume the pr obability of an event occurring is p  = 0.1.
1
.What ar e the  odds  of the event occurring?  .
2
.What ar e the  log-odds  of the event occurring?  .
3
.Construct the  probability  of the event as a ratio that equals 0.1  .
PRB-12  
  CH.PRB- 2.9.
T rue or False:  If the odds of success in a binary r esponse is  4, the
corr esponding pr obability of success is  0.8.
PRB-13  
  CH.PRB- 2.10.
Draw a graph of  odds to probabilities  , mapping the entir e range of
pr obabilities to their r espective odds  .
PRB-14  
  CH.PRB- 2.1 1.
The logistic r egr ession model is a subset of a br oader rang e of
machine learning models known as generalized linear models (GLMs),
which also include analysis of variance (ANOV A), vanilla linear
r egr ession, etc. Ther e ar e thr ee components to a GLM;  identify these
three components for binary logistic regression.
PRB-15  
  CH.PRB- 2.12.
Let us consider  the logit transformation, i.e., log-odds. Assume a
scenario in which the logit forms the linear decision boundary:for a  given vect or of systesmatic components X and pr edictor v ariables
θ. W rite the mathematical expr ession for the hyperplane that describes
the decision boundary  .
PRB-16  
  CH.PRB- 2.13.
T rue or False:  The logit function and the natur al logistic (sigmoid)
function ar e inverses of each other  .
2.2.3   The Sigmoid
The sigm oid (Fi g. 2.1  ) also known as the logistic function, is widely used
in b inary classification and as a neuron activation function in artificial
neural networks.
F IGURE  2.1: Examples of two sigmoid functions  .PRB-17  
  CH.PRB- 2.14.
Compute the derivative of the natural sigmoid function:
PRB-18  
  CH.PRB- 2.15.
Remember that in logistic r egr ession, the hypothesis function for
some parameter vector β and measur ement vector x is defined as:
wher e y holds the hypothesis value  .
Suppose the coefficients of a logistic r egr ession model with
independent variables ar e as follows: β  0  = −1.5, β  1  = 3, β  2  = −0.5.
Assume additionally , that we have an observation with the f ollowing
values for the de pendent variables: x  1  = 1, x  2  = 5. As a r esult, the logit
equation becomes:
1
.What is the value of the  logit for this observation?
2
.What is the value of the  odds for this observation?
3
.What is the value of P  ( y  = 1) for this observation?
2.2.4   T ruly Understanding Logistic RegressionPRB-19  
  CH.PRB- 2.16.
Pr oton t herapy (PT) [  2  ] is a widely adopted form of tr eatment for
many types of cancer including br east and lung cancer (Fig.  2.2  )  .
F IGURE  2.2: Pulmonary nodules (left) and breast cancer (right)  .
A PT device which  was not properly calibrated  is used to simulate
the tr eatment of cancer . As a r esult, the PT beam does not behave
normally . A data scientist collects information r elating to this
simulation. The covariates pr es ented in T able  2.1  ar e collected during
the expe riment. The columns  Y es  and  No  indicate if the tumour was
eradicated or not, r espectively  .
T umour eradication
Cancer T ype Y es No
Breast 560 260
Lung 69 36
T ABLE  2.1: T umour eradication statistics  .
Referring to T able  2.1  :
1
.What is the explanatory variable and what is the r esponse variable?
2
.Explain the use of r elative r isk and odds ratio for measuring
association  .
3 Ar e the two variables positively or negatively associated?.
Find the dir ection and str engt h of the association using bo th
r elative risk and odds ratio  .
4
.Compute a  95% confidence interval (CI) for the measur e of
association  .
5
.Interpr et the r esults and explain their significance  .
PRB-20  
  CH.PRB- 2.17.
Consider a system for radiation  therapy planning (Fig.  2.3  ). Given
a patient with a malignant tumour , the pr oblem is to select the optimal
radiation exposur e time for that patient. A key element in this pr oblem is
estimating the pr obability that a given tumour will be eradicated given
certain c ovariates. A data scientist collects information r elating to this
radiation therapy system  .
F IGURE  2.3: A multi-detector positron scanner used to locate tumours  .
The following covariates ar e  collected; X  1  denotes time in
milliseconds that a patient is irradiated with, X  2  = holds the size of thetumour in centimeters, and Y  notates a binary r esponse variable
indicating if the tumour was eradicated. Assume that each r esponse’
variable Y  i  is a Bernoulli random variable with success parameter p  i  ,
which holds:
The data scientist fits a logistic r egr ession model to the dependent
measur ements and pr oduces these estimated coefficients:
1
.Estimate the pr obability that, given a patient who under goe s the
tr eatment for 40 milliseconds and who is pr esented with a tumour
sized 3.5 centimetr es, the system eradicates the tumour  .
2
.How many milliseconds the pa tient in part (a) would need to be
radiated with to have exactly a 50% chance of eradicating the
tumour?
PRB-21  
  CH.PRB- 2.18.
Recent r esear ch [  3  ] sugge sts that heating mer cury containing
dental amalgam s may cause the r elease of toxic mer cury fumes into the
human airways.  It is also pr esumed that drinking hot coffee, stimulates
the r elease of mer cury vapour fr om amalgam fillings (Fig.  2.4  )  .F IGURE  2.4: A dental amalgam  .
T o study factors that affect migraines, and in particular , patients who
have at least four dental amalgams in their mouth, a data scientist
collects data fr om 200K users w ith and without dental amalgams. The
data scientist then fits a logistic r egr ession model with an indicator of a
second m igraine within a time frame of one hour after the onset of the
first migraine, as the binary r esponse variable (e.g., migraine=1, no
migraine=0). Th e data scientist believes that the fr equency of migraines
may be r elated to the r elease of toxic mer cury fumes  .
Ther e ar e two independent variables:
1
.X  1  = 1 if the patient has at least four amalgams; 0 otherwise  .
2
.X  2  = coffee consumption (0 to 100 hot cups per month)  .
The output fr om training a lo gistic r egr ession classifier is as
follows:
Analysis of LR Parameter Estimates
Parameter Estimate Std.Err Z-val Pr>|Z|
Intercept -6.36347 3.21362 -1.980 0.0477
$X_1$ -1.0241 1 1.17101 -0.875 0.3818
$X_2$ 0.1 1904 0.05497 2.165 0.0304
1
.Using X  1  and X  2  , expr ess the  odds  of a patient hav ing a migraine
for a second time  .2
.Calculate the  probability  of a second migraine for a patient that has
at least four amalgams and drank 100 cups per month?
3
.For users that have at least four amalgams, is  high  coffee intake
associated with an  increased  pr obability of a second migraine?
4
.Is ther e statistic al evidence that having mor e than four amalgams is
directly  associated with a  reduction  in the pr obability of a second
migraine?
PRB-22  
  CH.PRB- 2.19.
T o study factors that affect Alzheimers disease using logistic
r egr ession, a r esear cher considers the link between gum (periodontal)
disease and Alzheimer as a plausible risk factor [  1  ]. The pr edictor
variable is a count of gum bacteria (Fig.  2.5  ) in the mouth  .
F IGURE  2.5: A chain of spherical bacteria  .
The r esp onse va riable, Y  , measur es whether the patient shows any
r emission (e.g. yes=1). The ou tput fr om training a logistic r e gr ession
classifier is as follows:
Parameter DF Estimate Std
Intercept 1 -4.8792 1.0732
gum bacteria 1 0.0258 0.0194
1 Estimate the pr obability of im pr ovement when the count of gum. bacteria of a patient is  33.
2
.Find out the gum bacteria count at which the estimated pr obability
of impr ovement is  0.5.
3
.Find out the est imated odds ratio of impr ovement for an incr ease of
1  in the total gum bacteria count  .
4
.Obtain a  99% confidence inte rval for the tr ue odds ratio of
impr ovement incr ease of  1  in the total gum bacteria count.
Remember that the most common confidence levels ar e  90%, 95%,
99%, and  99.9%. T able  9.1  lists the z values for these levels  .
Confidence Level z
90% 1.645
95% 1.960
99% 2.576
99.9% 3.291
T ABLE  2.2: Common confidence levels  .
PRB-23  
  CH.PRB- 2.20.
Recent r esear ch [  4  ] suggests that cannabis (Fig.  2.6  ) and
cannabinoids administration in particular , may r educe the size of
malignant tumours in rats  .F IGURE  2.6: Cannabis  .
T o study factors affecting tumour shrinkage, a deep learning
r esear cher collects data fr om two gr oups; one gr oup is administer ed
with placebo (a substance that is not medicine) and the other with
cannabinoids. His main r esear ch r evolves ar ound studying the
r elationship (T able  2.3  ) between the anticancer pr operties of
cannabinoids and tumour shrinkage:
T umour Shrinkage In Rats
Group Y es No Sum
Cannabinoids 60 6833 6893
Placebo 130 6778 6909
Sum 190 1361 1 13801
T ABLE  2.3: T umour shrinkage in rats  .
For the true odds ratio:
1
.Find the sample odds ratio  .
2
.Find the sample log-odds ratio  .
3 Compute a 95% confidence inte rval (z  0  .  95  = 1.645; z  0  .  975  = 1.96 ). for the true log odds ratio and true odds ratio  .
2.2.5   The Logit Function and Entropy
PRB-24  
  CH.PRB- 2.21.
The entr opy (see Chapter  4  ) of a single binary outcome with
pr obability p to r eceive  1 is defined as:
1
.At what p does H  ( p  ) attain its maximum value?
2
.What is the r ela tionship betwee n the entr opy H  ( p  ) and the logit
function, given p?
2.2.6   Python/PyT orch/CPP
PRB-25  
  CH.PRB- 2.22.
The following C++ code (Fig.  2.7  ) is part of a (very basic) logistic
r egr ession implementation module. For a theor etical di scussion
underlying this question, r efer to pr oblem  2.17  .
1 #include ...
2 std ::  vector <  double  >  theta { -6  , 0.05  , 1.0  };
3 double  sigmoid  ( double  x) {
4 double  tmp =1.0 /  ( 1.0 +  exp( -  x));
5 std ::  cout <<  "prob="  <<  tmp <<  std ::  endl;
6 r eturn  tmp;
7 }
8 double  hypothesis  (std ::  vector <  double  >  x){
9 double  z;
1
0z =  std ::  inner_product(std ::  begin(x), std ::  end(x),
↪  std ::  begin(theta), 0.0  );1
1std ::  cout <<  "inner_product="  <<  z <<  std ::  endl;
1
2r eturn  sigmoid(z);
1
3}
1
4int  classify  (std ::  vector <  double  >  x){
1
5int  hypo =  hypothesis(x) > 0.5f  ;
1
6std ::  cout <<  "hypo="  <<  hypo <<  std ::  endl;
1
7r eturn  hypo;
1
8}
1
9int  main() {
2
0std ::  vector <  double  >  x1 { 1  , 40  , 3.5  };
2
1classify(x1);
2
2}
F IGURE  2.7: Logistic regression in CPP
1
.Explain the purpose of line  20  , i.e.,  inner_product  .
2
.Explain the purpose of line  25  , i.e.,  hypo(x) > 0.5f  .
3
.What does θ  (theta)  stand for in line 9?
4
.Compile and run the code, you can use:
https://r epl.it/languages/cpp1 1  to evaluate the code  .
What is the output?PRB-26  
  CH.PRB- 2.23.
The following Python code (Fig.  2.8  ) runs a very simple linear
model on a two-dimensional matrix  .
1 import  tor ch
2 import  tor ch.nn  as  nn
3 import  tor ch.nn.functional  as  F
4 fr om  tor ch.autograd  import  V ariable
5
6 lin =  nn .  Linear( 5  , 7  )
7 data =  V ariable(torch .  randn( 3  , 5  ))
8
9 print  (lin(data) .  shape)
1
0>  ?
F IGURE  2.8: A linear model in PyT orch
W ithout actually running the code, determine what is the size of
the matrix printed on line  10 as a  r esult of applying the linear
model on the matrix  .
PRB-27  
  CH.PRB- 2.24.
The following Python code sn ippet (Fig.  2.9  ) is part of a
logistic r egr ession implementation module in Python  .
1 fr om  scipy .special  import  expit
2 import  numpy  as  np
3 import  math
4
5 def  Func001  (x):
6 e_x =  np .  exp(x -  np .  max(x))
7 r eturn  e_x /  e_x .  sum()
8
9 def  Func002  (x):
1
0r eturn  1 /  ( 1 +  math .  exp( -  x))1
1
1
2def  Func003  (x):
1
3r eturn  x *  ( 1-  x)
F IGURE  2.9: Logistic regression methods in Python  .
Analyse the methods  Func001  ,  Func002  and  Func003
pr esented in Fig.  2.9  , find their purposes and  name them  .
PRB-28  
  CH.PRB- 2.25.
The following Python code sn ippet (Fig.  2.10  ) is part of a
machine learning module in Python  .
1
2 fr om  scipy .special  import  expit
3 import  numpy  as  np
4 import  math
5
6 def  Func006  (y_hat, y):
7 if  y == 1  :
8 r eturn  -  np .  log(y_hat)
9 else  :
1
0r eturn  -  np .  log( 1 -  y_hat)
F IGURE  2.10: Logistic regression methods in Python  .
Analyse the method  Func006  pr esented in Fig.  2.10.  What
important concept in machine-learning does it implement?
PRB-29  
  CH.PRB- 2.26.
The follo wing Python code snippet (Fig.  2.1 1  ) pr esents several
differ ent variations of the same function  .1
2 fr om  scipy .special  import  expit
3 import  numpy  as  np
4 import  math
5
6 def  V er001  (x):
7 r eturn  1 /  ( 1 +  math .  exp( -  x))
8
9 def  V er002  (x):
1
0r eturn  1 /  ( 1 +  (np .  exp( -  x)))
1
1
1
2WHO_AM_I = 709
1
3
1
4def  V er003  (x):
1
5r eturn  1 /  ( 1 +  np .  exp( -  (np .  clip(x, -  WHO_AM_I, None  ))))
F IGURE  2.1 1: Logistic regression methods in Python  .
1
.Which mathematical function do these methods implement?
2
.What is significant about the number  709  in line 1 1?
3
.Given a choice, which method would you use?
2 . 3   S o l u t i o n s
2.3.1   General ConceptsSOL-4  
  CH.SOL- 2.1.
T rue.  However , when an excessive and unnecessary number of
variables is used in a logistic r egr ession model, peculiarities (e .g.,
specific attribut es) of the unde rlying data set dispr oportionately
affect the coefficients in the model, a phenomena commonly
r eferr ed to as “o verfitting”. Ther efor e, it is important that a logi stic
r egr ession mode l does not start training with mor e variables tha n is
justified for the given number of observations  .
SOL-5  
  CH.SOL- 2.2.
The odd s of success ar e defi ned as the ratio between the
pr obability of success p ∊  [0, 1] and the pr obability of failur e  1 − p.
Formally:
For instance, assuming the pr ob ability of success of an event is
p  = 0.7. Then, in our example, the odds of success ar e  7 /  3, or  2.333
to  1. Naturally , in the case of equal pr obabilities wher e p  = 0.5, the
odds of success is  1 to  1.
SOL-6  
  CH.SOL- 2.3.
1
.An interaction is the pr oduct o f two single pr edictor variables
implying a  non-additive  effect  .
2
.The simplest interaction model includes a pr edictor variable
formed by multiplying two or dinary pr edictors. Let us assume
two vari ables X and Z. Then, the logistic r egr ession model tha t
employs the simplest form of interaction follows:
wher e the coefficient for the interaction term XZ is
r epr esented by pr edictor β  3  .
3
.For testing the contribution of an interaction, two principal
methods ar e commonly employed; the W ald chi-squar ed test or a
likelihood ratio test between the model with and without the
interaction term . Note: How does interaction r elates to
information theory? What added value does it employ to
enhance model performance?
SOL-7  
  CH.SOL- 2.4.
False.  This is exactly the definition of supervised learning;
when labels ar e known then supervision guides the learning pr ocess
.
SOL-8  
  CH.SOL- 2.5.
In th e case of logistic r egr ession, the r esponse variable is the
log of the odds of being classified in a gr oup of binary or multi-
class r es ponses. This definition essentially demonstrates that o dds
can take the form of a vector  .
SOL-9  
  CH.SOL- 2.6.
When a transfo rmation to the r esponse variable is applied, it
yields a  pr obability distribution over the output classes, which is
bounded between 0 and 1; this transformation can be employed in
several ways, e.g., a softmax la yer , the sigmoid function or cla ssic
normalization. T his r epr esentation facilitates a soft-decision by the
logistic r egr ession model, which permits construction of
pr obability-based pr ocesses over the pr edictions of the mo del.
Note: What ar e the pr os and cons of each of the th r ee
afor ementioned transformations?
SOL-10  
  CH.SOL- 2.7.
Minimizing the negative log likelihood also means maximizing
the  likelihood  of selecting the  correct  class  .
2.3.2   Odds, Log-odds
SOL-1 1  
  CH.SOL- 2.8.
1
.The odds of the event occurring ar e, by definition:
2
.The log-odds of the event occurring ar e simply taken as the log
of the odds:
3
.The pr obability may be constructed by the following
r epr esentation:
or , alternatively:
Note: What is the intuition behind this r epr esentation?
SOL-12  
  CH.SOL- 2.9.T rue.  By definition of odds, it is easy to notice that p = 0.8
satisfies the following r elation:
SOL-13  
  CH.SOL- 2.10.
The graph of odds to pr obabilities is depicted in Figur e  2.12  .
F IGURE  2.12: Odds vs. probability values  .
SOL-14  
  CH.SOL- 2.1 1.
A binary logistic r egr ession GLM consists of ther e components:
1
.Random component: r efers to the pr obability distribution of the
r esponse variable  ( Y  ), e.g., binomial distribution for Y in the
binary logistic r egr ession, which takes on the values Y  = 0 or Y
= 1.
2 Systematic component: describes the explanatory variables:.
( X  1  , X  2  , ..  .) as a combinatio n of linear pr e dictors. The
binary case does not constrain these variables to any degr ee  .
3
.Link function: specifies the link between random and systematic
components. It says how the expected value of the r esponse
r elates to the linear pr edictor of explanatory variables  .
Note: As sume th at Y denotes wh ether a human voice activity
was detected (Y  = 1 ) o r not (Y  = 0 ) in a give time frame.
Pr opose two systematic components and a link function
adjusted for this task  .
SOL-15  
  CH.SOL- 2.12.
The hyperplane is simply defined by:
Note: Recall the use of the logi t function and derive this decis ion
boundary rigor ously  .
SOL-16  
  CH.SOL- 2.13.
T rue.  The logit function is defined as:
for any p  ∊  [0, 1]. A simple  set of algebraic equa tions yields the
inverse r elation:
which exactly describes the r elation between the output and input of
the logistic function, also known as the sigmoid  .
2.3.3   The Sigmoid
SOL-17  
  CH.SOL- 2.14.
Ther e a r e various appr oaches to solve this pr oblem, her e we
pr ovide two; dir ect derivation or derivation via the softmax
function  .
1
.Dir ect derivation:
2
.Softmax derivation:
In a classification pr oblem wit h mutually exclusive classes,
wher e al l of the values ar e posit ive and sum to one, a softmax
activation function may be used. By definition, the softmax
activation function consists of n terms, such that  ∀ i  ∊  [1, n  ]:
T o comp ute the partial derivative of  2.18  , we  tr eat all  θk
wher e k  ≠ i as  constants  and then differ entiate θ  i  using
r egular differ entiation rules. For a given θ  i  , let us define:
and
It ca n now be shown that the derivative with r espect to θ  i
holds:
which can take on the informative form of:
It should be noted that  2.21  holds fo r any constant β, and fo r
β  = 1 it clearly r educes to the sigmoid activation function  .
Note: C haracterize the sigmoid function when its ar gument
appr oaches  0, ∞ and −∞. What undesir ed pr operties of the sigmoid
function do this values entail when consider ed as an activa tion
function?
SOL-18  
  CH.SOL- 2.15.
1
.The logit value is simply obtain ed by substituting the values o f
the dependent variables and model coefficients into the linear
logistic r egr ession model, as follows:
2
.Accor ding to the natural r elation between the logit and the odds,
the following holds:
3
.The  odds ratio  is, by definition:
so the logistic r esponse function is:
2.3.4     T ruly Understanding Logistic Regression
SOL-19  
  CH.SOL- 2.16.
1
.T umour eradication (Y ) is the r esponse variable and cancer
type (X) is the explanatory variable  .
2
.Relative risk (RR) is the ratio of risk of an event in one gr oup
(e.g., exposed gr oup) versus th e risk of the event in the other
gr oup (e.g., non-exposed gr oup ). The odds ratio (OR) is the
ratio of odds of an event in one gr oup versus the odds of the
event in the other gr oup  .
3
.If we calculate odds ratio as a measur e of association:
The odd s ratio is  larger  than one , indica ting that the odd s for
a br east cancer is mor e than the  odds for a lung cancer to be
eradicated. Notice however , that  this r esult is too close to one,
which pr events conclusive decision r egar ding the odds
r elation  .
Additionally , if we calculate r elative risk as a measur e of
association:
4
.The  95% confidence interval for the odds-ratio, θ is computed
fr om the sample confidence interval for log odds ratio:
Ther efor e, the 95% CI for  log ( θ  ) is:Ther efor e, the 95% CI for  is:
5
.The CI  (0.810, 1.909) contains 1, which indicates tha t the true
odds ratio is not significantly differ ent fr om 1 and  there is not
enough evidence  that tumour eradication is dependent on
cancer type  .
SOL-20  
  CH.SOL- 2.17.
1
.By using  the defined values for X  1  and X  2  , and the known
logistic r egr ession model, substitution yields:
2
.The equation for the pr edicted pr obability tells us that:
which is equivalent to constraining:
By takin g the logarithm of both sides, we get that the number
of milliseconds needed is:
SOL-21  
  CH.SOL- 2.18.
For the purpose of this exer cise, it is instructive to pr e-define z as:
1
.By employing the classic logistic r egr ession model:
2
.By s ubstituting the given values of X  1  , X  2  into z  ( X  1  , X  2  ), the
pr obability holds:
3
.Y es. The coefficient for coffee consumption is positive (  0.1 19 )
and the p-value is less than  0.05 (  0.0304 )  .
Note: Can you describe the r elation between these numerical
r elations and the positive conclusion?
4
.No. The p-value for this pr edictor is  0.3818 >  0.05.
Note: Ca n you explain why this inequality implicates a lack of
statistical evidence?
SOL-22  
  CH.SOL- 2.19.
1
.The estimated pr obability of impr ovement is:
Hence  , 
  (33) = 0.21 1868.2
.For  
  ( gum bacteria  ) = 0.5:
3
.The estimated odds ratio ar e given by:
4
.A  99% confidence interval for β is calculated as follows:
Ther efor e, a  99% confidence inter val for the true  odds ratio
exp( β  ) is given by:
SOL-23  
  CH.SOL- 2.20.
1
.The sample odds ratio is:
2
.The estimated standar d err or for  log (
  ) is:3
.Accor ding to pr evious sections, the  95% CI fo r the true log odds
ratio is:
Corr espondingly , the  95% CI for the true odds ratio is:
2.3.5    The Logit Function and Entropy
SOL-24  
  CH.SOL- 2.21.
1
.The entr opy (Fig.  2.13  ) has a maximum value of  log2  (2) for
pr obability p  = 1 /  2, which is the most chaotic distribution. A
lower entr opy is a mor e pr edictable outcome, with zer o
pr oviding full certainty  .
2
.The derivative of the entr opy with r espect to p yields  the
negative of the logit  function:
Note: The curious r eader is encouraged to rigor ously pr ove
this claim  .
F IGURE  2.13: Binary entropy  .
2.3.6     Python, PyT orch, CPP
SOL-25  
  CH.SOL- 2.22.
1
.During infer ence, the purpose of  inner_product  is to multiply
the vector of logistic r egr ession coefficients with the vector of
the input which we like to evaluate, e.g., calculate the
pr obability and binary class  .
2
.The line  hypo(x) > 0.5f  is commonly used for the evaluation of
binary classification wher ein pr obability values above 0.5 (i.e.,
a thr esh old) ar e r egar ded as TRUE wher eas values below 0.5
ar e r egar ded as F ALSE  .
3
.The term θ  (theta)  stands for the lo gistic r egr ession coefficients
which wer e evaluated during training  .
4
.The output is as follows:
1 >  inner_product =-0.5
2 >  prob =0.377541
3 >  hypo =0
F IGURE  2.14: Logistic regression in C++SOL-26  
  CH.SOL- 2.23.
Because the second dimension of  lin  is 7, and the first
dimension of  data  is 3, the r esulting matrix has a shape of
tor ch.Size([3, 7])  .
SOL-27  
  CH.SOL- 2.24.
Ideally , you should be able to r ecognize these functions
immediately upon a r equest fr om the interviewer  .
1
.A softmax function  .
2
.A sigmoid function  .
3
.A derivative of a sigmoid function  .
SOL-28  
  CH.SOL- 2.25.
The function implemented in Fig.  2.10  is the  binary  cr oss-
entr opy function  .
SOL-29  
  CH.SOL- 2.26.
1
.All the methods ar e variations of the sigmoid function  .
2
.In Python, appr oximately  1.797 e  + 308 holds the lar gest
possible valve for a floating p oint variable. The logarithm of
which is evaluated at  709.78. If you try to execute the following
expr ession in Python, it will r esult in  inf  : np.log  (1.8 e  + 308).3
.I would use  V er003  because of its stability . Note: Can you entail
why is this method mor e stable than the others?
C HAPTER
3    PR OB ABILISTIC PR OGRAMMING & B A YESIAN DL
Anyone who considers arithmetical methods of pr oducing
random digits is, of course, in a state of sin  .
— John von Neumann (1903-1957)
C o n t e n t s
Intr oduction
Pr oblems
Expectation and V ariance
Conditional Probability
Bayes Rule
Maximum Likelihood Estimation
Fisher Information
Posterior & prior predictive distributions
Conjugate priors
The Beta-Binomial distribution
Bayesian Deep Learning
Solutions
Expectation and V ariance
Conditional Probability
Bayes Rule
Maximum Likelihood Estimation
Fisher Information
Posterior & prior predictive distributions
Conjugate priorsBayesian Deep Learning
3 . 1   I n t r o d u c t i o n
HE Bay esian school of thought has permeated fields such as
mechanical stati stics, classical probability , and financial mathematics [
13  ]. In tandem, the subject ma tter itself has gained attraction,
particularly in the field of BML. It is not surprising then, that several
new Python based probabilistic programming libraries such as PyMc3 and Stan [
1 1  ] h ave emer ged  and have become widely adopted by the machine learning
community .
This chapter aims to introduce  the Bayesian paradigm and a pply Bayesian
inferences in a variety of problems. In particular , the reader will be introduced with
real-life examples of condition al probability and also discover one of the most
important results in Bayesian statistics: that the family of beta distributions is
conjugate to a binomial likelihood  . It shou ld be stressed that Bayesian inference
is a subject matt er that students evidently find hard to grasp, since it heavily relies
on rigorous probabilistic interpretations of data. Specifically , several obstacles
hamper with the prospect of learning Bayesian statistics:
1 . Students typically under go mer ely basic introduction to classical probabi
statistics. Nonetheless, what follows requires a very solid grounding in these
2 . Many co urses and resources that address Bayesian learning do not cover e
concepts.
3 . A stron g comprehension of Bayesian methods involves num erical train
sophistication levels that go beyond first year calculus.
Conclusively , this chapter may be much harder to understand than other
chapters. Thus, we strongly ur ge the readers to thoroughly solve the following
questions and verify their grasp  of the mathematical concepts in the basis of the
solutions [  8  ].
3 . 2   P r o b l e m s
3.2.1   Expectation and V ariancePRB-30  
  CH.PRB- 3.1.
Define what is meant by a  Bernoulli trial  .
PRB-31  
  CH.PRB- 3.2.
The binomial distribution is often used to model the pr obability that k out of
a gr oup of n objects bar e a spe cific characteristic. Define wha t is meant by a
binomial random variable  X  .
PRB-32  
  CH.PRB- 3.3.
What does the following shorthand stand for?
PRB-33  
  CH.PRB- 3.4.
Find the pr obability mass function (PMF) of the following random variable:
PRB-34  
  CH.PRB- 3.5.
Answer the following questions:
1
.Define what is meant by (mathematical) expectation  .
2
.Define what is meant by variance  .
3
.Derive the expectation and variance of a the binomial random variable X ∼
Binomial( n, p  ) in terms of p and n  .
PRB-35  
  CH.PRB- 3.6.
Pr oton t herapy (PT) is a widely adopted form of tr eatment for many types of
cancer [  6  ]. A PT device which  was not  properl y calibrated  is used to tr eat a
patient with pancr eatic cancer (Fig.  3.1  ). As a r esult, a PT beam randomlyshoots  200 particles independently and  correctly  hits cancer ous cells with a
pr obability of  0.1.
F IGURE  3.1: Histopathology for pancreatic cancer cells  .
1
.Find the  statistical distribution  of the number of corr ect hits on cancer ous
cells in the desc ribed experiment. What ar e the expectation and  variance of
the corr esponding random variable?
2
.A r adiologist using the devic e claims he was able to hit exactly 60
cancer ous cells. How likely is it that he is  wrong  ?
3.2.2   Conditional Probability
PRB-36  
  CH.PRB- 3.7.
Given two events A and B in  pr obability space H, which occur with
pr obabilities P  ( A  ) and P  ( B  ), r espectively:
1
.Define the conditional pr obability of A given B. Mind singular cases  .
2
.Annotate each part of the conditional pr obability formulae  .
3
.Draw an instance of V enn diagr am, depicting the intersection of the events
A and B. Assume that A  ∪  B  = H  .
PRB-37  
  CH.PRB- 3.8.Bayesian infer ence amalgamates data information in the likelihood function
with known  prior  information. Th is is done by conditioning the prior on the
likelihood using the Bayes formulae. Assume two events A and B in pr obability
space H, which occur with pr ob abilities  P  ( A  ) and P  ( B  ), r espectively . Given
that A  ∪  B  = H, s tate the Bay es formulae for this case, interpr et its compone nts
and annotate them  .
PRB-38  
  CH.PRB- 3.9.
Define the terms  likelihood  and  log-likelihood  of a discr ete random
variable X given a fixed parameter of inter est γ. Give a pract ical example of
such scenario and derive its likelihood and log-likelihood  .
PRB-39  
  CH.PRB- 3.10.
Define the term  prior distribution  of a likelihood parameter  γ in the
continuous case  .
PRB-40  
  CH.PRB- 3.1 1.
Show the  relationship  between the prior , posterior and likelihood
pr obabilities  .
PRB-41  
  CH.PRB- 3.12.
In a  Bay esian c ontext, if a first experiment is conducted, and then another
experiment is followed, what  does the  posterior  become for the  next
experiment?
PRB-42  
  CH.PRB- 3.13.
What is the condition under w hich two events A and B ar e said to be
statistically independent  ?
3.2.3   Bayes Rule
PRB-43  
  CH.PRB- 3.14.
In a n experiment conducted in the field of particle physics (F ig.  3.2  ), a
certain p article may be in two distinct  equally probable  quantum states: integerspin or half-integer spin. It is well-known that particles with integer spin ar e
bosons, while particles with half-integer spin ar e fermions [  4  ]  .
F IGURE  3.2: Bosons and fermions: particles with half-integer spin are fermions  .
A ph ysicist is observing two suc h particles, while at least one of which is in
a half-integer state. What is the pr obability that  both  particles ar e fermions?
PRB-44  
  CH.PRB- 3.15.
During pr egnancy , the Placenta Chorion T est [  1  ] is com monly u sed for the
diagnosis of her editary diseases (Fig.  3.3  ). The test has a pr obability of 0.95 of
being corr ect  whether or not  a her editary disease is pr esent  .F IGURE  3.3: Foetal surface of the placenta
It is  known that 1% of pr egnancies r esult in her editary disease s. Calculate
the pr obability of a test indicating that a her editary disease is pr esent  .
PRB-45  
  CH.PRB- 3.16.
The Der cum dis ease [  3  ] is an extr emely rar e disor der of multiple painful
tissue gr owths. In a population in which the ratio of females to males is equal,
5% of females and 0.25% of males have the Der cum disease (Fig.  3.4  )  .
F IGURE  3.4: The Dercum disease
A person is chosen at random and that person has the Der cum disease.
Calculate the pr obability that the person is female  .
PRB-46  
  CH.PRB- 3.17.
Ther e ar e numer ous fraudulent binary options websites scatter ed ar ound the
Internet, and for every site that shuts down, new ones ar e spr outed like
mushr ooms. A  fraudulent  AI  based stock-market pr ediction algorithm utilized
at the New Y ork Stock Exchange, (Fig.  3.6  ) c an corr ectly pr edict if a cert ainbinary option [  7  ] shi fts s tates fr o m 0 to 1 or the  other way ar ound, with  85%
certainty  .
F IGURE  3.5: The New Y ork Stock Exchange  .
A financ ial engi neer has cr eated a portfolio consisting twice as many state-
1 option s then state-0 options. A stock option is selected at random and is
determined by said algorithm to  be in the state of 1. What is the  pr obability that
the pr ediction made by the AI is corr ect?
PRB-47  
  CH.PRB- 3.18.
In a n ex periment conducted by a hedge fund to determine if mo nkeys (Fig.
3.6  ) can outperform  humans in sele cting better stock market portfolios, 0.05 of
humans and 1 out of 15 monkeys could corr ectly pr edict stock market tr ends
corr ectly  .F IGURE  3.6: Hedge funds and monkeys  .
Fr om an equally pr obable pool of humans and monkeys an “expert” is
chosen at random. When tested, that expert was corr ect in pr edicting the stock
market shift. What is the pr obability that the expert is a monkey?
PRB-48  
  CH.PRB- 3.19.
During the cold war , the U.S.A developed a speech to text (STT) algorithm
that could theor etically detect the hidden dialects of Russian sleeper agents.
These agents (Fig.  3.7  ), wer e trained to speak English in Russia and
subsequently se nt to the US to gather intelligence. The FB I was able to
appr ehend ten such hidden Ru ssian spies [  9  ] an d accused them of being
“sleeper” agents  .
F IGURE  3.7: Dialect detection  .The Algorithm r elied on the acoustic pr operties of Russian pr onunciation of
the wor d (v-o-k-s-a-l) which was borr owed fr om English V -a-u-x-h-a-l-l. It was
alleged that it is  impossible for Russians to completely hide  their accent and
hence w hen a Russian would say V -a-u-x-h-a-l-l, the algorithm would yield the
text  “v-o-k-s-a-l”  . T o test the alg orithm at a dipl omatic gathering wher e 20%
of participants ar e Sleeper agents and the r est Americans, a data scientist
randomly choos es a person and  asks him to say V -a-u-x-h-a-l-l . A single letter
is then chosen randomly fr om the wor d that was generated by the algorithm,
which is observed to be an “l”. What is the pr obability that the p erson is indeed
a Russian sleeper agent?
PRB-49  
  CH.PRB- 3.20.
During W orld W ar II, for ces on  both sides of the war r elied on encrypted
communications. The main encr yption scheme used by the German military was
an Enigma machine [  5  ], which  was employed extensiv ely by Nazi Germany .
Statistically , the Enigma machine sent the symbols X and Z Fig. (  3.8  )
accor ding to the following pr obabilities:
F IGURE  3.8: The Morse telegraph code  .
In o ne incident, the German m ilitary sent encoded message s while the
British army used countermeasur es to deliberately tamper with the
transmission. Assume that as a r esult of the British countermeasur es, an X iserr oneously r eceived as a Z (and mutatis mutandis) with a  pr obability  
  . If a
r ecipient in the German militar y r eceived a Z, what is the pr ob ability that a Z
was actually transmitted by the sender?
3.2.4   Maximum Likelihood Estimation
PRB-50  
  CH.PRB- 3.21.
What is  likelihood function  of the independent identically distributed (i.i.d)
random variables:
X  1  , · · ·  , X  n  wher e X  i  ∼  binomial( n, p  ), ∀ i  ∊  [1, n  ],
and wher e p is the parameter of inter est?
PRB-51  
  CH.PRB- 3.22.
How can we derive the  maximum likelihood estimator (MLE)  of the i.i.d
samples X  1  , · · ·, X  n  intr oduced in Q.  3.21  ?
PRB-52  
  CH.PRB- 3.23.
What is  the r elationship between the likelihood function and the log-
likelihood function?
PRB-53  
  CH.PRB- 3.24.
Describe how to analytically find the MLE of a likelihood function?
PRB-54  
  CH.PRB- 3.25.
What is the term used to descri be  the first derivative  of the log-likelihood
function?
PRB-55  
  CH.PRB- 3.26.
Define the term  Fisher information  .
3.2.5   Fisher InformationPRB-56  
  CH.PRB- 3.27.
The 2014 west A frican Ebola (Fig.  9.10  ) epi demic has b ecome the lar gest
and fastest-spr eading outbr eak of the disease in modern history [  2  ] with a
death tool far e xceeding all past outbr eaks combined. Ebola ( named after the
Ebola R iver in Zair e) first emer ged in 1976 in Sudan and Za ir e and infected
over 284 people with a mortality rate of 53%  .
F IGURE  3.9: The Ebola virus  .
This rar e outbr eak, underlined the challenge medical teams ar e facing in
containing epidemics. A junior data scientist at the center for disease contr ol
(CDC) models the possible spr ead and containment of the Ebola virus using a
numerical simu lation. He knows that out of a population of k humans (the
number of trials), x ar e carriers of the virus (success in statistical jar gon). He
believes the sample likelihood o f the virus in the population, follows a Binomial
distribution:
As the senior r esear cher in the team, you guide him that his parameter of
inter est is γ, the  pr oportion of infected humans in the entir e population. The
expectation and variance of the binomial distribution ar e:
Answer the following; for the likelihood function of the form L  x  ( γ  ):
1
.Find the log-likelihood function l  x  ( γ  ) = ln L  x  ( γ  ).2
.Find the gradient of l  x  ( γ  ).
3
.Find the Hessian matrix H  ( γ  ).
4
.Find the Fisher information I  ( γ  ).
5
.In a population spanning 10,000 individuals, 300 wer e infected by Ebola
Find the MLE for  γ and the standar d err or associated with it  .
PRB-57  
  CH.PRB- 3.28.
In th is q uestion, you ar e going t o derive the Fisher information function for
several distributions. Given a pr obability density function (PDF) f  ( X| γ  ), you
ar e pr ovided with the following definitions:
1
.The natural logarithm of the PDF  ln f  ( X| γ  ) = Φ  ( X| γ  ).
2
.The first partial derivative Φ  ′ ( X| γ  ).
3
.The second partial derivative Φ  ′′ ( X| γ  ).
4
.The Fisher Information:
Find the Fisher Information I  ( γ  ) for the following distributions:
1
.The Bernoulli Distribution X ∼  B  (1, γ).
2
.The Poisson Distribution X ∼  Poiss  ( θ  ).PRB-58  
  CH.PRB- 3.29.
1.  T rue or False:  The Fisher Information is used to compute the Cramer -
Rao bound on the variance of any unbiased maximum likelihood estimator  .
2.  T rue or False:  The Fish er Infor mation matrix is also the Hessian of the
symmetrized KL diver gence  .
3.2.6   Posterior & prior predictive distributions
PRB-59  
  CH.PRB- 3.30.
In  chapter 3  we discussed the notion of a prior and a posterior distribution  .
1
.Define the term  posterior distribution  .
2
.Define the term  prior predictive distribution  .
PRB-60  
  CH.PRB- 3.31.
Let y be the number of successes in  5 independent tri als, wher e the
pr obability of success is θ in eac h trial. Suppose your prior dist ribution for θ is
as follows: P  ( θ  = 1 /  2) = 0.25, P  ( θ  = 1 /  6) = 0.5, and P  ( θ  = 1 /  4) = 0.25.
1
.Derive the  posterior distribution  p  ( θ|y  ) after  observing y  .
2
.Derive the  prior predictive distribution  for y  .
3.2.7   Conjugate priors
PRB-61  
  CH.PRB- 3.32.
In chapter  3  we discussed the notion of a prior and a posterior  .
1
.Define the term  conjugate prior  .2
.Define the term  non-informative prior  .
The Beta-Binomial distribution
PRB-62  
  CH.PRB- 3.33.
The Bino mial distribution was discussed extensively in chapter  3  . Her e, we
ar e going to show one of the most important r esults in Bayesian machine
learning. Pr ove that the family of beta distributions is  conjugate to a b inomial
likelihood  , so that if a prior is in that  family th en so is the posterior . That is,
show that:
For instance, for h heads and t tails, the posterior is:
3.2.8   Bayesian Deep Learning
PRB-63  
  CH.PRB- 3.34.
A r e cently published paper pr es ents a new layer for a new Baye sian neural
network (BNN) . The layer behaves as follows. During the feed-forwar d
operation, each of the hidden neur ons H  n  , n  ∊  2  1, 2 in the neural network
(Fig.  3.10  )  may , or may not  fir e independently of each other accor ding to a
known prior distribution  .
F IGURE  3.10: Likelihood in a BNN model  .
The chance of firing, γ, is the  same for each hidden neur on . Using the
formal definitio n, calculate the likelihood function of each of the following
cases:1
.The hidden neur on is distributed accor ding to X ∼  binomial( n  , γ) random
variable and fir es with a pr obab ility of γ. Ther e ar e 100 neur ons and only 20
ar e  fired  .
2
.The hidden neur on is distributed accor ding to X ∼  Uniform  (0, γ) random
variable and fir es with a pr obability of γ  .
PRB-64  
  CH.PRB- 3.35.
Y our colleague, a veteran of the Deep Learning industry , comes up with an
idea for for  a BNN layer entitled  OnOf fLayer  . He suggests that each neur on
will stay  on  (the other state is off) following the distribution f  ( x  ) = e  − x for x >
0 and f  ( x  ) = 0 otherwise (Fig.  3.1 1  ). X indicates the  time in  seconds the
neur on stays on  . In a BNN, 200 such neur ons ar e activated independently in
said On OffLayer . The OnOffLay er is set to off (e.g. not active)  only if at least
150 of the neurons are shut do wn  . Fin d th e pr oba bility that the O nOffLayer
will be active for at least 20 seconds without being shut down  .
F IGURE  3.1 1: OnOf fLayer in a BNN model  .
PRB-65  
  CH.PRB- 3.36.
A Dr opout layer [  12  ] (Fig.  3.12  ) is commonly used to r egularize a neural
network model by randomly equating several outputs (the cr ossed-out  hidden
node H) to 0  .
F IGURE  3.12: A Dropout layer (simplified form)  .
For instance, in PyT or ch [  10  ], a Dr opout layer is declar ed as follows (  3.1
):
1 import  tor ch
2 import  tor ch.nn  as  nn
3 nn .  Dropout( 0.2  )
C ODE  3.1: Dropout in PyT orch
Wher e nn.Dr opout  (0.2) (Line #3 in  3.1  ) ind icates that the pr obability of
zer oing an element is  0.2.
F IGURE  3.13: A Bayesian Neural Network Model
A new data scientist in your team suggests the following pr ocedur e for a
Dr opout layer which is based on Bayesian principles. Each of the neur ons θ  n  in
the neural network in (Fig.  8.33  ) may dr op (or not) independently of each other
exactly like a Bernoulli trial  .
During the training of a neural network, the Dr opout layer randomly dr ops
out outp uts of the pr evious layer , as indicated in (Fig.  3.12  ). Her e, for
illustration purposes, all four neur ons ar e dr opped as depicted by the cr ossed-
out  hidden  nodes H  n  .
Y ou ar e inter est ed in the pr opo rtion of dr opped-out neur ons. Assume that
the c hance of dr op-out, θ, is  the same  for each neur on (e.g. a  uniform prior  for
θ). Compute the  posterior  of θ  .
PRB-66  
  CH.PRB- 3.37.
A new data scientist in your team, who was formerly a Quantum Physicist,
suggests the following pr ocedur e for a Dr opout layer entitled  QuantumDrop
which i s based on Quantum principles and the  Maxwell Boltzmann
distribution  . In the Maxwell-Boltzmann  distribution, the likelihood of finding a
particle with a particular velocity v is pr ovided by:F IGURE  3.14: The Maxwell-Boltzmann distribution  .
In the suggested  QuantumDr op layer (  3.15  ), ea ch of the neur ons behaves
like a molecul e and is distr ibuted accor ding to the Maxwell-Boltzmann
distribution and  fires only when the most probable speed is reached  . This
speed is the v elocity associa ted with the highest point i n the Maxwell
distribution (  3.14  ). U sing calculus, brain power and some mathematical
manipulation, find the  most likely value (speed) at which the neuron will fire  .
F IGURE  3.15: A QuantumDrop layer  .3 . 3   S o l u t i o n s
3.3.1   Expectation and V ariance
SOL-30  
  CH.SOL- 3.1.
The no tion of a Bernoulli t rial r efers to an experiment with two
dichotomous binary outcomes; success (x = 1), and failur e (x = 0)  .
SOL-31  
  CH.SOL- 3.2.
A binomial random variable X  = k r epr esents k successes in n mutually
independent Bernoulli trials  .
SOL-32  
  CH.SOL- 3.3.
The sho rthand X  ∼  Binomial( n, p  ) indicates that the random variable X
has the binomial distribution (Fig.  3.16  ). The positive integer parameter n
indicates the number of Bernou lli trials and the r eal paramete r p  , 0 < p <  1
holds the pr obability of success in each of these trials  .
F IGURE  3.16: The binomial distribution  .
SOL-33  
  CH.SOL- 3.4.The random variable X ∼  Binomial( n, p  ) has the following PMF:
SOL-34  
  CH.SOL- 3.5.
The answers below r egar d a dis cr ete random variable. The curious r eader
is encouraged to expend them to the continuous case  .
1
.For a random va riable X with pr obability mass function P  ( X  = k  ) and a set
of outcomes K, the  expected value of X is defined as:
Note: The expectation of X may also be denoted by µ  X  .
2
.The  variance of X is defined as:
Note: Th e variance of X may also be denoted by  
  , while σ  X  itself
denotes the  standard deviation  of X  .
3
.The population mean and vari ance of a binomial random v ariable with
parameters n and p ar e:
Note: Why is this solution intuitive? What information theory-r elated
phenomenon occurs when p  = 1 /  2 ?
SOL-35  
  CH.SOL- 3.6.
1
.This scenario describes an experiment that is r epeated 200 times
independently with a success pr obability of  0.1. Thus, if the random variableX d enotes the number of times success was obtained, the n it is best
characterized by the binomial distribution with parameters n  = 200 and p  =
0.1. Formally:
The expectation of X is given by:
and its r espective variance is:
2
.Her e we pr opose two distinguished methods to answer the question  .
Primarily , the straightforwar d s olution is to employ the definiti on of the
binomial distribution and substitute the value of X in it. Namely:
This leads to an extr emely high pr obability that the radiologist is
mistaken  .
The following appr oach is longer and mor e advanced, but grants the
r eader with insights and intuition r egar ding the r esults. T o derive how
wr ong the radiologist is, we can employ an appr oximation by considering
the standar d normal distribution. In statistics, the  Z-score  allows us to
understand how  far fr om the mean is a data point in units of standar d
deviation, thus r evealing how likely it is to occur (Fig.  3.17  )  .
F IGURE  3.17: Z-score
Ther efor e, the pr obability of corr ectly hitting 60 cells is:
Again, t he outcome shows the l ikelihood that the radiologist was wr ong
appr oaches  1. Note: Why is the r elation depicted in Fig.  3.17  deduces
that Z is a standar d Gaussian ? Under what terms is this conclusion
valid? Why does eq. (  3.20  ) em ploys the cu mulative distribution function
and not the pr obability mass function?
3.3.2   Conditional Probability
SOL-36  
  CH.SOL- 3.7.
1
.For two events A and B with P  ( B  ) >  0, the  conditional probability of A
given that B has occurr ed is defined as:
It is  easy to note that if P  ( B  ) = 0, this r elation is not defined
mathematically . In this case, P  ( A|B  ) = P  ( A  ∩ B  ) = P  ( A  ).
2
.The annotated pr obabilities ar e displayed in Fig.  3.18  :
F IGURE  3.18: Conditional probability
3 An example of a diagram depicting the intersected events A and B is. displayed in Fig.  3.19:
F IGURE  3.19: V enn diagram of the intersected events A and B in probability space H
SOL-37  
  CH.SOL- 3.8.
The Bayes formulae r eads:
wher e P  ( A  c ) is the complementary pr obability of P  ( A  ). The interpr etation of
the elements in  Bayes formulae is as follows:
Note: What is t he important r ole of the normalization consta nt? Analyze the
cases wher e P  ( B  ) →  0 and P  ( B  ) →  1. The annotated pr obabilities ar e
displayed in (Fig.  3.20  ):F IGURE  3.20: Annotated components of the Bayes formula (eq.  3.23  )
SOL-38  
  CH.SOL- 3.9.
Given X  as a d iscr ete random ly distributed variable and given γ as the
parameter of inter est, the likelihood and the log-likelihood of X given γ follows
r espectively:
The ter m likelihood can be i ntuitively understood fr om this definition; it
deduces how likely is to obtain a value x when a prior information is given
r egar ding its distribution, namel y the parameter γ. For example , let us consider
a biased coin toss with p  h  = γ. Then:
Note: The likelihood function may also follow continuous distributions such
as th e no rmal di stribution. In the latter , it is r ecommended and often obligatory
to emplo y the log-  likelihood. Why? W e encourage the r eader to modify the
above to the continuous case of normal distribution and derive the answer  .
SOL-39  
  CH.SOL- 3.10.
The continuous prior distribution, f  ( Γ  = γ) r epr esents what is known about
the pr obability of the value  γ befor e th e exper iment has commenced. It is termed
as being  subjective  , and ther efor e may vary considerably between r esear chers.
By pr oc eeding the pr evious example, f  ( Γ  = 0.8) holds the pr obability of
randomly flipping a coin that yields “heads” with chance of 80% of times  .SOL-40  
  CH.SOL- 3.1 1.
The essence of Bayesian analysis is to draw infer ence of unknown quantities
or q uantiles fr om the posterio r distribution p  ( Γ  = γ |X  = x  ), which is
traditionally derived fr om prior beliefs and data information. Bayesian
statistical conclusions about chances to obtain the parameter Γ  = γ or
unobserved valu es of random variable X  = x, ar e made in t erms of  probability
statements. These pr obability sta tements ar e conditional on the observed values
of X, which is d enoted as p  ( Γ  = γ |X  = x  ), called p osterior distributions o f
parameter  γ. Bayesian analysis is a practical method for making infer ences
fr om data and prior beliefs usin g pr obability models for quantities we observe
and for quantities which we wish to learn. Bayes rule pr ovides a r elationship of
this form:
SOL-41  
  CH.SOL- 3.12.
The pos terior density summariz es what is known about the parameter of
inter est γ after the data  is observed  . In Baye sian sta tistics, the posterior density
p  ( Γ  = γ |X  = x  ) becomes the  prior  for this next experiment. This is part of the
well-known Bay esian updating mechanism wher ein we update our knowledge to
r eflect th e actua l distribution of data that we observed. T o summarize, fr om the
perspective of Bayes Theor em, we update the  prior distribution  to a  posterior
distribution  after seeing the data  .
SOL-42  
  CH.SOL- 3.13.
T wo events A and B ar e statistically independent if (and only if):
Note: Use conditional pr obabili ty and rationalize this outcome. How does this
pr operty become extr emely useful in practical r esear ches that consider
likelihood of normally distributed featur es?
3.3.3   Bayes RuleSOL-43  
  CH.SOL- 3.14.
Let  γ stand fo r the number of half-integer spin states, and given the prior
knowledge that both states ar e equally pr obable:
Note: Under what statistical pr operty do the above r elations hold?
SOL-44  
  CH.SOL- 3.15.
Let even t A indicate pr esent her editary-disease and let event B to hold a
positive test r esult. The calculated pr obabilities ar e pr esented in T able  3.1  . W e
wer e ask ed to find the pr obability of a test indicating that her ed itary-disease is
pr esent, namely P  ( B  ). Accor ding to the law of total pr obability:
Note: In terms of performance evaluation, P  ( B|A  ) is often r eferr ed to as
the pr obability of  detection and P  ( B|Ā  ) is consider ed the pr obability of false
alarm. Notice that these measur es do not, neither logically nor mathematically ,
combine to pr obability of 1  .
P
ROBABILITYE  XPLANA TION
P(A)= 0.01 The probability of hereditary-disease.
P(
  )=1-
0.01=.99The probability of no hereditary-disease.
P(
  |
)=0.95The probability that the test will yield a negative result [˜B] if
hereditary-disease is NOT present [Ã].
P(B|
  )=1- The probability that the test will yield a positive result [B] if0.95=.05 hereditary-disease is NOT pre sent [Ã] (probability of false
alarm).
P(B|A)=0.95 The probability that the test will yield a positive result [B] if
hereditary-disease is present [A] (probability of detection).
P(
  |A)=1-
0.95=.05The probability that the test will yield a negative result [˜B] if
hereditary-disease is present [A].
T ABLE  3.1: Probability values of hereditary-disease detection  .
SOL-45  
  CH.SOL- 3.16.
W e first enumerate the pr obabilities one by one:
W e ar e asked to find P  ( female|Der cum  ). Using Bayes Rule:
However we ar e missing the term P  ( Der cum  ). T o find it, we ap ply the Law of
T otal Pr obability:
And finally , r eturning to eq. (  3.39  ):
Note: How could this r esult be r eached with one mathematical equation?SOL-46  
  CH.SOL- 3.17.
In or der to solve this pr oblem, we intr oduce the following events:
1
.AI: the AI pr edicts that the state of the stock option is 1  .
2
.State  1 : the state of the stock option is 1  .
3
.State  0 : the state of the stock option is 0  .
A dir ect application of Bayes formulae yields:
SOL-47  
  CH.SOL- 3.18.  In or der to solve this pr oblem, we intr oduce the
following events:
1
.H: a human  .
2
.M: a monkey  .
3
.C: a corr ect pr ediction  .
By employing Bayes theor em and the Law of T otal pr obability:Note: If something seems off in this outcome, do not worry - it is a positive
sign for understanding of conditional pr obability  .
SOL-48  
  CH.SOL- 3.19.
In or der to solve this pr oblem, we intr oduce the following events:
1
.RUS: a Russian sleeper agent is speaking  .
2
.AM: an American is speaking  .
3
.L: the TTS system generates an “l”  .
W e ar e asked to find the value of P  ( RUS|L  ). Using Bayes Theor em we can
write:
W e w er e told tha t the Russians c onsist 1/5 of the attendees at the gathering,
ther efor e:
Additionally , because “v-o-k-s-a-l” has a single l out of a total of six letters:
Additionally , because “V -a-u-x-h-a-l-l” has two l’ s out of a total of eight letters:
An application of the Law of T otal Pr obability yields:
Using Bayes Theor em we can write:
Note: What is th e letter by whic h the algorithm is most likely to discover a
Russian sleeper agent?
SOL-49  
  CH.SOL- 3.20.
W e ar e given that:
P  ( X is err oneously  r eceived as a Z  ) = 1 /  7. Using Bayes Theor em we can
write:
An application of the Law of T otal Pr obability yields:
So, using Bayes Rule, we have that3.3.4   Maximum Likelihood Estimation
SOL-50  
  CH.SOL- 3.21.
For the set of i.i.d samples X  1  , · ·  ·, X  n  , the  likelihood function is the
pr oduct of the pr obability functions:
Note: What is the distribution of X  n when X is a Be rnoulli distribu ted
random variable?
SOL-51  
  CH.SOL- 3.22.
The  maximum likelihood estimator (MLE) of p is the value of all possible p
values that maximizes L  ( p  ). Namely , the p value that r enders the set of
measur ements X  1  , · · ·, X  n  as the  most likely . Formally:
Note: The curious student is highly encouraged to derive  
  fr om L(p).
Notice that L(p) can be extr emely simplified  .
SOL-52  
  CH.SOL- 3.23.
The  log-likelihood is the logarithm of the  likelihood function. Intuitively ,
maximizing the likelihood function L  (γ) is equivalent to maximizing  ln L  (γ) interms of finding the MLE  
  , since  ln is a monotonically incr easing function.
Often, we maximize  ln( f  (γ)) instead of the f  (γ). A comm on example is when L
(γ) is comprised of normally distribution random variables  .
Formally , if X  1  , · · ·, X  n  ar e i.i.d, each with pr obability mass function (PMF) of
f  Xi  ( x  i  |  γ), then
SOL-53  
  CH.SOL- 3.24.
The general pr ocedur e for fin ding the MLE, given that the  likelihood
function is differ entiable, is as follows:
1
.Start by differ entiating the log-likelihood function  ln ( L  ( γ  )) with r espect to
a parameter of inter est γ  .
2
.Equate the r esult to zer o  .
3
.Solve the equation to find  
  that holds:
4
.Compute the second derivative to verify that you indeed have a maximum
rather than a minimum  .
SOL-54  
  CH.SOL- 3.25.
The first derivative of the log-lik elihood function is commonly known as the
Fisher score function  , and is defined as:SOL-55  
  CH.SOL- 3.26.
Fisher information  , i s the term used to describe th e expected value of the
second derivatives (the curvatur e) of the log-likelihood function, and is defined
by:
3.3.5   Fisher Information
SOL-56  
  CH.SOL- 3.27.
1
.Given L  ( γ  ):
2
.T o find the gradient, we differ entiate once:
3
.The Hessian is generated by deriving g  ( γ  ):
4
.The Fisher information is calculated as follows:since:
5
.Equating the gradient to zer o and solving for our parameter γ, we get:
In our case this equates to:  300 /  10000 =  0.03. Regar ding the err or , ther e
is a  close r elationship between the variance of  γ and the Fisher
information, as the former is the inverse of the latter:
Plugging the numbers fr om our question:
Statistically , the  standar d err or that we ar e asked to find is the squar e
r oot of eq.  3.66  which equals  5.3 ×  10−4 . Note: What desir ed pr operty is
r evealed in this experiment? At was cost could we ensur e a low standar d
err or?
SOL-57  
  CH.SOL- 3.28.
The Fisher Information for the distributions is as follows:
1
.Bernoulli:
2
.Poisson:
SOL-58  
  CH.SOL- 3.29.
1
.T rue  .
2
.T rue  .
3.3.6   Posterior & prior predictive distributions
SOL-59  
  CH.SOL- 3.30.
1
.Given a sample of the form  
  = ( x  1  , · · ·, x  n  ) drawn fr om a density p  ( θ  ; 
) and θ is randomly generated accor ding to a prior density of p  (θ). Then theposterior density is defined by:
2
.The prior pr edictive density is:
SOL-60  
  CH.SOL- 3.31.
1
.The posterior p  ( θ|y  ) ∝  p  ( y| θ  ) p  ( θ  ) is:
2
.The prior pr edictive distribution p  ( y  ):
+
3.3.7   Conjugate priors
SOL-61  
  CH.SOL- 3.32.1
.A class F of prior distribution s is said to form a conjugate f amily if the
posterior density is in F for all e ach sample, whenever the prior density is in
F  .
2
.Often we would like a prior that favours no particular values of the
parameter over others. Bayes ian analysis r equir es prior information,
however sometimes ther e is no p articularly useful information befor e data is
collected. In these situations, p riors with “no information” ar e expected
Such priors ar e called  non-informative priors.
SOL-62  
  CH.SOL- 3.33.
If x ∼  B( n  , γ) so
p  ( x| γ  ) ∝  γ  x (1 −  γ)n −x
and the prior for  γ is B  (α, β) so
p  ( γ  ) ∝  γ  α −  1(1 −  γ)β −  1
then the posterior is
γ|x ∼  B  ( α  + x  , β + n − x  )
It is immediately clear the family of beta distributions is conjugate to a
binomial likelihood  .
3.3.8   Bayesian Deep Learning
SOL-63  
  CH.SOL- 3.34.
1
.The hidden neur on is distributed accor ding to:
X ∼  binomial( n  , γ) random variable and fir es with a pr obability of  γ.
Ther e ar e 100 neur ons and only 20 ar e  fired  .
2
.The hidden neur on is distributed accor ding to:X uniform  (0, γ) random variable and fir es with a pr obability of γ  .
The uniform distribution is, of course, a very simple case:
Ther efor e:
SOL-64  
  CH.SOL- 3.35.
The pr ovided distribution is fr om the exponential family . Ther efor e, a single
neur on becomes inactive with a pr obability of:
The OnOffLayer  is off only if at least 150 out of 200 neur ons ar e off. Ther efor e,
this may be r epr esented as a Binomial distribution and the pr obability for the
layer to be off is:
Hence, the pr obability of the layer being active for at least 2 0 seconds is 1
minus this value:
SOL-65  
  CH.SOL- 3.36.
The observed data, e.g the dr opped neur ons ar e distributed accor ding to:
Denoting s and f as success and failur e r espectively , we know that the
likelihood is:
W ith the following parameters  α = β = 1 the beta distribution acts like
Uniform prior:
Hence, the  prior  density is:
Ther efor e the  posterior  is:
SOL-66  
  CH.SOL- 3.37.
Neur ons ar e dr opped wheneve r their value (or the equivalent quantum
term- speed) r each  the most likely value:
Fr om ca lculus, we know that in or der to maximize a function, we have to
equate its first derivative to zer o:
The constants can be taken out as follows:
Applying the chain rule fr om calculus:
W e notice that several terms cancel out:
Now the quadratic equation can be solved yielding:
Ther efor e, this is the most pr obable value at which the dr opout layer will fir e  .
Ref e r e n c e s
[  1
]M. B arati and P . ‘Comparison of complications of chorionic villus samplin
and amniocentesis’. In: 5.4 (2012), pp. 241–244 (cit. on p. 46  ).
[  2
]J. D. e. a. Bell  BP Damon IK . ‘Overview , Control Strategie s, and Lesso
Learned in the CDC Response to the 20142016 Ebola Epidemic .’ In: Morbidi
and Mortality W eekly Report  65.3 (2016), pp. 4–1 1 (cit. on p. 52  ).
[  3
]J. C. Cook and G. P . Gross. Adiposis Dolor osa (Der cum, Anders Disease)
StatPearls [Internet], 2019 (cit. on p. 47  ).
[  4
]G. Ecker . Particles, Field,  Fr om Quantum Mechanics to the Standar d Mod el 
Particle Physics  . Springer ., 2019 (cit. on p. 45  ).
[  5
]K. Gaj and A. Orlowski. ‘Facts  and Myths of Enigma: Breakin g Stereotype
In: International Confer ence on the Theory and Applications of Cryptograph
T echniques  . 2003 (cit. on p. 50  ).
[  6
]B. Gottschalk. ‘T echniques of Proton Radiotherapy: T ransport Theory’. I
arXiv  (2012) (cit. on p. 43  ).
[  7
]T . S. O. of Investor Education and Advocacy . Binary options and Fraud  (c
on p. 48  ).
[  8 E. T . Jaynes. Pr obability Theory as Logic  . Ed. by P . F . Fougère. Maximum] Entropy and Bayesian Methods. Kluwer , Dordrecht, 1990 (cit. on p. 42  ).
[  9
]D. o . J. National Security Division. Conspiracy to Act as Unr egister ed Agen
of a For eign Government  . 2010 (cit. on p. 49  ).
[  10
]A. P aszke et al.  ‘Automatic dif ferentiation in PyT orch’. In: 31st Confer enc
on Neural Information Pr ocessing Systems  . 2017 (cit. on p. 56  ).
[  1 1
]J. Salvatier , T . V . W iecki and C. Fonnesbeck. ‘Probabilistic programming in
Python using PyMC3’. In: PeerJ Computer Science  2 (Jan. 2016), e55 (cit
on p. 42  ).
[  12
]P . Sledzinski et al. ‘The current state and future perspectives of cannabinoid
in cancer biology’. In: Cancer Medicine  7.3 (2018), pp. 765–775 (cit. on p
56  ).
[  13
]E. B. Starikov . ‘Bayesian Statistical Mechanics: Entro py Enthalpy
Compensation and Universal Equation of State at the T ip of Pen’. In
Fr ontiers in Physics  6 (2018), p. 2 (cit. on p. 42  ).P A R T  III
HIGH SCHOOLC HAPTER
4   INFORMA TION THEOR Y
A basic idea in information theory is that information can be
tr eated very much like a physical quantity , such as mass or
ener gy  .
— Claude Shannon, 1985
C o n t e n t s
Intr oduction
Pr oblems
Logarithms in Information Theory
Shannon’ s Entropy
Kullback-Leibler Diver gence (KLD)
Classification and Information Gain
Mutual Information
Mechanical Statistics
Jensen’ s inequality
Solutions
Logarithms in Information Theory
Shannon’ s Entropy
Kullback-Leibler Diver gence
Classification and Information Gain
Mutual Information
Mechanical Statistics
Jensen’ s inequality4 . 1   I n t r o d u c t i o n
NDUCTIVE inference, is the problem of reasoning under conditions
of incomplete information, or uncertainty  . Ac cording to S hannon’ s
theory [  2  ], information and uncertainty are two sides of the same
coin: the  more uncertainty there  is, the more information we gain by
removing the uncertainty . Entro py plays central roles in many scientific realms
ranging from physics and statistics to data science and economics. A basic
problem in information theory is encoding lar ge quantities of information [ 2  ].
Shannon’ s discovery of the fundamental laws of data compression and
transmission marked the birth of information theory . In his fundamental paper
of 1948, “ A Mathematical Theory of Communication  ” [  4  ], Shannon proposed
a measu re of the uncertainty a ssociated with a random memory-less source,
called Entr opy  .
F IGURE  4.1: Mutual information
Entropy first emer ged in thermodynamics in the 18th century by Carnot, [  1  ]
in his pioneering work on steam entitled “ Reflection on the Motive Power of
Fir e  ” (Fig. 4.2  ). Su bsequently it appeared in statistical mechanics where it was
viewed as a measure of disor der  . H owev er , it was Boltzmann ( 4.30  ) who
found the connection between entropy and probability , and the notion ofinformation as used by Shannon is a generalization of the notion of entropy .
Shannon’ s entro py shares some instinct with Boltzmanns entropy , and likewise
the m athematics  developed in information theory is highly relevant in statistical
mechanics.
F IGURE  4.2: Reflection on the motive power of fire  .
The majority  of candi dates I interview fail to come up with an answer to
the following question: what is the entr opy of tossing a non-biased coin?
Surprisingly , even after I explicitly provide them with Shannon’ s formulae for
calculating entropy ( 4.4  ), many are still unable to calculate simple logarithms.
The purp ose of this chapter is to  present the aspiring data scient ist with some of
the most significant notions of entropy and to elucidate its relationship to
probability . Therefore, it is primarily focused on basic quantities in information
theory such as entropy , cross-entropy , conditional entropy , mutual information
and Kul lback-Leibler diver gence, also known as relative entro py . It does not
however , discu ss more advanced topics such as the concept of active
information introduced by Bohm and Hiley [  3  ].
4 . 2   P r o b l e m s
4.2.1   Logarithms in Information TheoryIt is  important to note that all numerical calculations in this chapter use the
binary logarithm log2  . This specific logarithm produces units of bits, the
commonly used units of information in the field on information theory .
PRB-67  
  CH.PRB- 4.1.
Run the following Python code (  4.3  ) in a Python in terpr eter . What ar e
the r esults?
1 import  math
2 import  numpy
3 print  (math .  log( 1.0/0.98  )) # Natural log (ln)
4 print  (numpy .  log( 1.0/0.02  )) # Natural log (ln)
5
6 print  (math .  log10( 1.0/0.98  )) # Common log (base 10)
7 print  (numpy .  log10( 1.0/0.02  )) # Common log (base 10)
8
9 print  (math .  log2( 1.0/0.98  )) # Binary log (base 2)
1
0print  (numpy .  log2( 1.0/0.02  )) # Binary log (base 2)
F IGURE  4.3: Natural (ln), binary (log 2  ) and common (log 10  ) logarithms  .
PRB-68  
  CH.PRB- 4.2.
The thr ee basic laws of logarithms:
1
.First law
Compute the following expr ession:
2
.Second lawCompute the following expr ession:
3.  Third law
Ther efor e, subtracting  log B fr om  log A r esults in  log 
  .
Compute the following expr ession:
4.2.2   Shannon’ s Entropy
PRB-69  
  CH.PRB- 4.3.
W rite Shannon’ s famous general formulae for  uncertainty  .
PRB-70  
  CH.PRB- 4.4.
Choose exactly  one, and only one  answer  .
1
.For an event which is  certain to happen  , what is the entr opy?
(a) 1.0
(b) 0.0
(c) The entr opy is undefined
(d) −  1
(e) 0.5
(f) log2  ( N  ), N being the number of possible events
2
.For N  equiprobable events  , what is the entr opy?(a) 1.0
(b) 0.0
(c) The entr opy is undefined
(d) −  1 (e)  0.5
(f) log2  ( N  )
PRB-71  
  CH.PRB- 4.5.
Shannon found that entr opy was the only function satisfying  three
natural properties  . Enumerate these pr operties  .
PRB-72  
  CH.PRB- 4.6.
In information theory , minus the  logarithm of the pr obability of  a symbol
(essentially the number of bits r equir ed to r epr esent it efficiently in a binary
code) is defined to be the  information  conveyed by transmitting that symbol.
In this context, the entr opy can be interpr eted as the expected information
conveyed by transmitting a single symbol fr om an alphabet in which the
symbols occur with the pr obabilities π  k  .
Mark the correct answer  : Information is a/an  [decrease/increase]  in
uncertainty  .
PRB-73  
  CH.PRB- 4.7.
Claud Shannon’ s paper “A mathematical theory of communication” [  4
], m arked the bi rth of information theory . Published in 1948, it has become
since the Magna Carta of the information age. Describe in your own wor ds
what is meant by the term  Shannon bit  .
PRB-74  
  CH.PRB- 4.8.
W ith r espect to the notion of  surprise  in the context of information
theory:
1
.Define what it actually meant by being  surprised  .
2 Describe how it is r elated to the likelihood of an event happening  ..
3
.T rue or False:  The less  likely the occurr ence of an event, the smaller
information it conveys  .
PRB-75  
  CH.PRB- 4.9.
Assume a sour ce of signals t hat transmits a given message a with
pr obability P  a  . Assume further that the message is encoded into an or der ed
series of ones and zer os (a bit string) and that a r eceiver has a decoder that
converts the bit string back into its r espective message  .
Shannon devised  a formulae tha t describes the  size  that the mean length of
the bit string can be  compressed to  . W rite the formulae  .
PRB-76  
  CH.PRB- 4.10.
Answer the following questions:
1
.Assume a sour ce that pr ovides a constant str eam of N  equally likely
symbols {x  1  , x  2  , . . ., x  n  }. What does Sha nnon’ s formula e (  4.4  ) r educe
to in this particular case?
2
.Assume that each equipr obable pixel in a monochr ome image t hat is fed
to a DL classification pipeline, can have values ranging fr om 0 to 255.
Find the entr opy in  bits  .
PRB-77  
  CH.PRB- 4.1 1.
Given Shannon’ s famous general formulae for uncertainty (  4.4  ):
1
.Plot a graph of the curve of pr obability vs. uncertainty  .
2
.Complete the sentence:  The curve is  [symmetrical/asymmetrical]  .3
.Complete the sentence:  The curve rises to a  [minimum/maximum]  when
the two symbols ar e equally likely (P  a  = 0.5)  .
PRB-78  
  CH.PRB- 4.12.
Assume we ar e pr ovided with biased coin for which the event ‘heads’ is
assigned pr obability p, and ‘tails’ - a pr obability of  1 − p. Using (  4.4  ), the
r espective entr opy is:
Ther efor e, H  ≥ 0 and the maximum possible uncertainty is attained when p  =
1 /  2, is H  max  = log2  2.
Given the above formulation, de scribe a helpful pr operty of the entr opy that
follows from the concavity of the logarithmic function.
PRB-79  
  CH.PRB- 4.13.
T rue or False:  Given random variables X, Y and Z wher e Y  = X  + Z then:
PRB-80  
  CH.PRB- 4.14.
What is the entr o py of a  biased coin  ? S uppose a coi n is biased such  that
the pr obability of ‘heads’ is p  ( x  h  ) = 0.98.
1
.Complete the sentence:  W e can pr edict ‘heads’ for each flip with an
accuracy of [__-_]%  .
2
.Complete the sentence:  If the r esult of the coin toss is ‘heads’, the
amount of Shannon information gained is [___] bits  .
3
.Complete the sentence:  If the r esult of th e coin toss is ‘tails’, the amount
of Shannon information gained is [___] bits  .
4
.Complete the sentence:  It is always true that the mor e information is
associated with an outcome, the  [more/less]  surprising it is  .5
.Pr ovided that the ratio of tosse s r esulting in ‘heads’ is p  ( x  h  ), and the
ratio of tosses r esulting in ‘tails ’ is p  ( x  t  ), and also pr ovided that p  ( x  h
)+ p  ( x  t  ) = 1, what is formulae for the  average surprise  ?
6
.What is the value of the  average surprise  in bits?
4.2.3   Kullback-Leibler Diver gence (KLD)
PRB-81  
  CH.PRB- 4.15.
W rite the formulae for the Kullback-Leibler diver gence between two
discr ete pr obability density functions P and Q  .
PRB-82  
  CH.PRB- 4.16.
Describe one intuitive interpr etation of the KL-diver gence with r espect to
bits  .
PRB-83  
  CH.PRB- 4.17.
1
.T rue or False:  The KL -diver gence is not a symmetric measur e of
similarity , i.e.:
2
.T rue or False:  The KL-diver gence satisfies the triangle inequality  .
3
.T rue or False:  The KL-diver gence is not a distance metric  .
4
.T rue or False:  In information theory , KLD is r egar ded as a measur e of
the information  gained when pr obability distribution Q is used to
appr oximate a true pr obability distribution P  .
5
.T rue or False:  The units of KL-diver gence ar e units of information  .6
.T rue or False:  The KLD is always non-negative, namely:
7
.T rue or False:  In a decision tr ee,  high  information gain indicates that
adding a split to the decision tr ee r esults in a  less  accurate model  .
PRB-84  
  CH.PRB- 4.18.
Given two distributions f  1  and f  2  and their r espective joint distrib ution f,
write the formulae for the  mutual information of f  1  and f  2  .
PRB-85  
  CH.PRB- 4.19.
Y ou ar e pr ovided with uniform distribution of the form:
What is the value of the  Kullback-Liebler distance  KL  p  ǁ  q  ?
4.2.4   Classification and Information Gain
PRB-86  
  CH.PRB- 4.20.
Ther e ar e several measur es by w hich one can determine how to optimally
split att ributes in a decision tr ee. List the thr ee most commonly used
measur es and write their formulae  .
PRB-87  
  CH.PRB- 4.21.
Complete the sentence:  In a decision tr ee, the attribute by which we
choose to split is the one with  [minimum/maximum]  information gain  .
PRB-88  
  CH.PRB- 4.22.T o study  factors  affecting the decision of a fr og to jump (or not), a deep
learning r esear c her fr om a Brazilian rain-for est, collects data pertaining to
several independent binary co-v ariates. The binary r esponse variable  Jump
indicates whether a jump was observed. Referring  to T able (  4.1  ), each r ow
indicates the observed values, columns denote featur es and r ows denote
labelled instances while class label (  Jump  ) d enote s whether the fr og had
jumped  .
Observation Green Lar ge Rain Jump
x  1 0 0 0 −
x  2 0 0 0 −
x  3 1 1 1 −
x  4 1 0 1 +
x  5 0 1 0 +
x  6 0 1 1 +
x  7 0 0 1 +
x  8 1 1 0 +
T ABLE  4.1: Decision trees and frogs  .
W ithout explicitl y determining the information gain values for each of the
thr ee attributes, which attribute should be chosen as the attribute by which
the decision tr ee should be first partitioned?
PRB-89  
  CH.PRB- 4.23.
This question discusses the  link between binary classification,
information gain and decision tr ees. Recent r esear ch [  5  ] suggests that
Cannabis (Fig.  4.4  ), and Cannabinoids administration in particular may
r educe t he size of malignant t umours in r odents. The data (T able  9.2  )
comprises a training set of feat ur e vectors with corr esponding  class labels
which a r esear cher intents classifying using a decision tr ee  .F IGURE  4.4: Cannabis
T o study  factors affecting tumour shrinkage, the deep learning r e sear cher
collects data r egrading two independent binary variables ; θ  1  (T/F)
indicating whether the r odent is a female, and θ  2  (T/F) indicating whether
the r odent was administrated with Cannabinoids. The binary r esponse
variable  , γ, indicates wheth er tumour shrinkage was observed (e.g.
shrinkage=+, no  shrinkage=-). Referring to T able (  9.2  ), each r ow indicates
the observed values, columns (θ  i  ) denote featur es and class label (γ) denotes
whether shrinkage was observed  .
γ θ  1 θ  2
+ T T
- T F
+ T F
+ T T
- F T
T ABLE  4.2: Decision trees and Cannabinoids administration
1
.Describe what is meant by  information gain  .
2
.Describe in your own wor ds how does a decision tr ee work  .
3
.Using log  2  , and the pr ovided dataset, calculate the sample entr opy H  ( γ
).4
.What is the info rmation gain IG  ( X  1  ) H  ( γ  ) − H  ( |θ  1  ) for the pr ovided
training corpus?
PRB-90  
  CH.PRB- 4.24.
T o study factors affecting the expansion of stars, a physicist is pr ovided
with dat a r e-gra ding two independent variables; θ  1  (T/F) indicating whether
a star is dense, and θ  2  (T/F) indicating  whether a star is adjacent to a black-
hole. H e is told that the binary r esponse variable, γ, indic ates whether
expansion was observed  .
e.g.:
expansion=+, no expansion=-. Referring to table (  4.3  ), each r ow indicates
the observed values, columns (θ  i  ) denote featur es and class label (γ) denotes
whether expansion was observed  .
γ  (expansion) θ1  (dense) θ2  (black-hole)
+ F T
+ T T
+ T T
- F T
+ T F
- F F
- F F
T ABLE  4.3: Decision trees and star expansion  .
1
.Using log  2  and the pr ovided dataset, calculate the sample entr opy H  ( γ  )
(expansion) befor e splitting  .
2
.Using log  2  and the pr ovided dataset, calculate the  information gain  of H
( γ  | θ  1  ).
3
.Using log  2  and the pr ovided dataset, calculate the  information gain  of H
( γ  | θ  2  ).PRB-91  
  CH.PRB- 4.25.
T o study  factors affecting tumour shrinkage in humans, a deep learning
r esear cher is pr ovided with dat a r egrading two independent variables; θ  1
(S/M/L) indicating whether the tumour is small(S), medium(M)  or lar ge(L),
and θ  2  (T/F) indicating whether the tumour  has under gone radiation
therapy . He is told that the binary r esponse variable  , γ, indicates whether
tumour shrinkage was observed (e.g. shrinkage=+, no shrinkage=-)  .
Referring to table (  4.4  ), ea ch r ow in dicates the observed values,
columns (θ  i  ) d enote featur es and class labe l (γ) denotes whether shrinkage
was observed  .
γ  (shrinkage) θ  1 θ  2
- S F
+ S T
- M F
+ M T
+ H F
+ H T
T ABLE  4.4: Decision trees and radiation therapy  .
1
.Using log  2  and the pr ovided dataset, calculate the sample entr opy H  ( γ  )
(shrinkage)  .
2
.Using log  2  and the pr ovided dataset, calculate the entr opy of H  ( γ| θ  1  ).
3
.Using log  2  and the pr ovided dataset, calculate the entr opy of H  ( γ| θ  2  ).
4
.T rue or false:  W e shou ld split on a specific variable that minimizes the
information gain, ther efor e we should split on  θ2  (radiation therapy)  .
4.2.5   Mutual Information
PRB-92  
  CH.PRB- 4.26.Shannon described a communications system consisting five elements (
4.5  ), two of which ar e the sour ce S and the destination D  .
F IGURE  4.5: Shannon’ s five element communications system  .
1
.Draw a V enn dia gram depicting the r elationship between the entr opies of
the sour ce H  ( S  ) and of the destination H  ( D  ).
2
.Annotate the part termed  equivocation  .
3
.Annotate the part termed  noise  .
4
.Annotate the part termed  mutual information  .
5
.W rite the formulae for  mutual information  .PRB-93  
  CH.PRB- 4.27.
Complete the sentence:  The r ela tive ent r opy D  ( p||q  ) is the measur e of
(a) [___] between two distributions. It can also be expr essed as a measur e of
the (b)[___] of assuming tha t the distribution is q when the (c)[___]
distribution is p  .
PRB-94  
  CH.PRB- 4.28.
Complete the sentence:  Mutual informat ion is a Shannon entr opy-based
measur e of dependence between random variables. The mutual information
between X and Z can be understood as the (a) [___] of the (b) [___] in X
given Z:
wher e H  is the S hannon entr opy , and H  ( X | Z  ) is the conditional entr opy of
Z given X  .
4.2.6   Mechanical Statistics
Some books have a tendency of sweeping “unseen” problems under the rug. W e
will not do that here. This subsection may look intimidating and for a good
reason; it involves equations that, unless you are a physic ists, you have
probably never encountered bef ore. Nevertheless, the ability to cope with new
concepts lies at the heart of every job interview .
For some of the questions, you may need these constants:
PHYSICAL CONST ANTS
k Boltzmanns constant 1.381 ×  10− 23  J K− 1
c Speed of light in vacum 2.998 ×  108 m s− 1
h Planck’ s constant 6.626 ×  10− 34  J s
PRB-95  
  CH.PRB- 4.29.
What is the expr ession for the Boltzmann pr obability distribution?PRB-96  
  CH.PRB- 4.30.
Information theory , quantum p hysics and thermodynamics ar e closely
inter connected. Ther e ar e severa l equivalent formulations for the second law
of thermodynamics. One appr oach to describing uncertainty stems fr om
Boltzmanns fund amental work on  entropy in st atistical mechanics. Describe
what is meant by  Boltzmanns entropy .
PRB-97  
  CH.PRB- 4.31.
Fr om Boltzmann s perspective, what is the entr opy of an octahedral dice (
4.6  )?
F IGURE  4.6: An octahedral dice  .
4.2.7   Jensen’ s inequality
PRB-98  
  CH.PRB- 4.32.
1
.Define the term concave function  .
2
.Define the term convex function  .
3
.State Jensen’ s inequality and its implications  .
PRB-99  
  CH.PRB- 4.33.T rue or False:  Using Je nsen’ s inequality , it is possible to show that the
KL diver gence is always gr eater or equal to zer o  .
4 . 3   S o l u t i o n s
4.3.1   Logarithms in Information Theory
SOL-67  
  CH.SOL- 4.1.
Numerical r esults (  4.7  ) a r e pr ovided using Python interpr eter version
3.6  .
1 import  math
2 import  numpy
3 print  (math .  log( 1.0/0.98  )) # Natural log (ln)
4 > 0.02020270731751947
5 print  (numpy .  log( 1.0/0.02  )) # Natural log (ln)
6 > 3.912023005428146
7 print  (math .  log10( 1.0/0.98  )) # Common log (base 10)
8 > 0.008773924307505152
9 print  (numpy .  log10( 1.0/0.02  )) # Common log (base 10)
1
0> 1.6989700043360187
1
1print  (math .  log2( 1.0/0.98  s)) # Binary log (base 2)
1
2> 0.02914634565951651
1
3print  (numpy .  log2( 1.0/0.02  )) # Binary log (base 2)
1
4> 5.643856189774724
F IGURE  4.7: Logarithms in information theory  .
SOL-68  
  CH.SOL- 4.2.
The logarithm base is explicitly written in each solution  .1  .
2  .
3  .
4.3.2   Shannon’ s Entropy
SOL-69  
  CH.SOL- 4.3.
Shannons famous general formulae for uncertainty is:
SOL-70  
  CH.SOL- 4.4.
1
.No i nformation is conveyed by an event which is a-priori known to occur
for certain (P  a  = 1), ther efor e the entr opy is  0.
2
.Equipr obable events mean that P  i  = 1 /N  ∀ i  ∊  [1, N  ]. Ther efor e for N
equally-likely events, the entr opy is  log2  ( N  ).
SOL-71  
  CH.SOL- 4.5.
The thr ee pr operties ar e as follows:
1
.H  ( X  ) is always non-negative, since information cannot be lost  .2
.The uniform distribution maximizes H  ( X  ), since it also maximizes
uncertainty  .
3
.The add itivity pr operty which  r elates the sum of entr opies of two
independent eve nts. For instance, in thermodynamics, the total e ntr opy of
two isol ated systems which coexist in equilibrium is the sum of the
entr opies of each system in isolation  .
SOL-72  
  CH.SOL- 4.6.
Information is an  [increase]  in uncertainty  .
SOL-73  
  CH.SOL- 4.7.
The Shannon bit has two distinctive states; it is either 0 or 1, but never
both at the sam e time. Shannon devised an experiment in which ther e is a
question whose only two possible answers wer e  equally likely to happen  .
He then defined  one bit  as the amount of information gained (or
alternatively , the  amount of entr opy r emoved) once an answer to the question
has been learned. He then c ontinued to state that when the a-priori
pr obability of any one possible answer is higher than the other , the answer
would have conveyed less than one bit of information  .
SOL-74  
  CH.SOL- 4.8.
The noti on of surprise is dir ectly r elated to the likelihood of an event
happening. Mathematically is it inversely pr oportional to the pr obability of
that event  .
Accor dingly , lea rning that a h igh-pr obability event has take n place, for
instance the sun rising, is  much less of a surprise  and give s less information
than learning that a low-pr obability event, for instance, rain in a hot summer
day , has  taken place. Ther efor e, the less likely the occurr ence of  an event, the
gr eater information it conveys  .
In th e case whe r e an event is a-priori known to occur for certa in (P  a  = 1 ),
then no information is convey ed by it. On the other hand, an extr emely
intermittent event conveys a lot of information as it  surprises  us and informs
us that a very impr obable state exists  .
SOL-75  
  CH.SOL- 4.9.
This quantity I  Sh  , r epr esented in the formulae is called the  Shannon
information of the source  :
It r e fers to the mean length in bi ts, per message, into which the messages
can be compr essed to. It is then possible for a communications channel to
transmit I  Sh  bits per message with a capacity of I  Sh  .
SOL-76  
  CH.SOL- 4.10.
1
.For N equipr oba ble events it ho lds that P  i  = 1 /N  , ∀ i  ∊  [1, N  ]. Ther efor e
if we substitute this into Shannon’ s equation we get:
Since N does not depend on i, we can pull it out of the sum:
It ca n be shown that for a given number of symbols (i.e., N is fixed) the
uncertainty H has its lar gest value only when the symbols ar e e qually
pr obable  .
2
.The pr obability for each pixel t o be assigned a value in the given range
is:Ther efor e the entr opy is:
SOL-77  
  CH.SOL- 4.1 1.
Refer to Fig.  4.8  for t he corr esponding illustratio n of the graph, wher e
information is shown as a function of p. It is equal to  0 for p  = 0 and for p  =
1. This is r easonable because for such values of p the outcome is certain, so
no information is gained by learning the outcome. The entr opy in maximal
uncertainty equa ls to 1 bit for p  = 0.5. Thus, the information gain is ma ximal
when the  pr obab ilities of two po ssible events ar e equal. Furthermor e, for the
entir e range of p r obabilities between p  = 0.4 and p  = 0.6 the information is
close to 1 bit  .
F IGURE  4.8: H vs. ProbabilitySOL-78  
  CH.SOL- 4.12.
An important set of pr operties of the entr opy follows fr om the concavity
of the entr opy , which follows fr om the concavity of the logarithm. Suppose we
cannot decide whether the actua l pr obability of ‘heads’ is p  1  or p  2  . W e may
decide to assign pr obability q to the first alternative and pr obability  1 − q to
the second. The actual pr obability of ‘heads’ then is the mixtur e qp  1  + (1 − q
) p  2  . The corr esponding entr opies satisfy the inequality:
with equality in the extr eme cases wher e p  1  = p  2  , or q  = 0, or q  = 1.
SOL-79  
  CH.SOL- 4.13.
Given  ( X, Y  ), we can determine X and Z  = Y − X. Conversely , given  ( X, Z
), we c an determine X and Y  = X  + Z. Hence, H  ( X, Y  ) = H  ( X, Z  ) due to the
existence of this bijection  .
SOL-80  
  CH.SOL- 4.14.
The solution and numerical calculations ar e pr ovided using  log2  .
1
.W e can pr edict ‘heads’ for each flip with an accuracy of p  ( x  h  ) = 98 %.
2
.Accor ding to Fig. (  4.9  ), if the r esult o f the coin toss  is ‘heads’, the
amount of Shannon information gained is  log2  (1 /  0.98) [bits]  .
1 import  math
2 import  numpy
3 print  (math .  log2( 1.0/0.98  )) # Binary log (base 2)
4 > 0.02914634565951651
5 print  (numpy .  log2( 1.0/0.02  )) # Binary log (base 2)
6 > 5.643856189774724
F IGURE  4.9: Shannon information gain for a biased coin toss  .3
.Likewise, if the r esult of the coin toss is ‘tails’, the amount of Shannon
information gained is  log2  (1 /  0.02) [bits]  .
4
.It is always true  that the  more  information is associated with an outcome,
the  more  surprising it is  .
5
.The formulae for the  average surprise  is:
6
.The value of the  average surprise  in bits is (  4.10  ):
1 import  autograd.numpy  as  np
2 def  binaryEntropy  (p):
3 r eturn  -  p *  np .  log2(p) -  ( 1-  p) *  np .  log2( 1-  p)
4 print  ( "binaryEntropy(p) is:  {}
↪    bits"  .  format(binaryEntropy( 0.98  )))
5 >  binaryEntropy(p) is  : 0.1414  bits
F IGURE  4.10: A verage surprise
4.3.3   Kullback-Leibler Diver gence
SOL-81  
  CH.SOL- 4.15.
For discr ete pr obability distributions P and Q, the Kullback-Leibler
diver gence  from  P  to  Q, the KLD is defined as:SOL-82  
  CH.SOL- 4.16.
One interpr etation is the following: the KL-diver gence indicates the
average number of  additional bits  r equir ed for transmission of values x ∊  X
which ar e distributed accor ding  to P  ( x  ), but we err oneously encoded them
accor ding to distribution Q  ( x  ). This makes sense since you have to “pay”
for a dditional bits to compensate for not knowing the true distribution, thus
using a code that was optimized accor ding to other distribution . This is one
of the r eason  that the KL-diver gence is also known as r elative entr opy .
Formally , the cr oss entr opy has an information interpr etation quantifying
how many bits ar e wasted by using the wr ong code:
SOL-83  
  CH.SOL- 4.17.
1
.T rue  KLD is a non-symmetric measur e, i.e. D  ( P  ║ Q  ) ≠ D  ( Q  ║ P  ).
2
.False  KLD does not satisfy the triangle inequality  .
3
.T rue  KLD is not a distance metric  .4
.T rue  KLD is r egar ded as a measur e of the information  gain  . Notice that,
however , KLD is the  amount  of information  lost  .
5
.T rue  The units of KL diver gence ar e units of information (bits, nats, etc.)
.
6
.T rue  KLD is a non-negative measur e  .
7
.T rue  Performing splitting based on highly informative event usually
leads to low model generalization and a less accurate one as well  .
SOL-84  
  CH.SOL- 4.18.
Formally , mutual information attempts to measur e how corr elated two
variables ar e with  each other:
Regar ding the question at hand, given two distributions f  1  and f  2  and
their joint distribution f, the  mutual information of f  1  and f  2  is de fined as I  ( f
1  , f  2  ) = H  ( f, f  1  f  2  ). If the two distributions ar e independent, i.e. f  = f  1  · f  2  ,
the m utual information will van ish. This concept has been wid ely used as a
similarity measur e in image analysis  .
SOL-85  
  CH.SOL- 4.19.
4.3.4   Classification and Information Gain
SOL-86  
  CH.SOL- 4.20.
The thr ee most widely used methods ar e:
1  .
2  .
3  .
SOL-87  
  CH.SOL- 4.21.
In a decision tr ee, the attribute  by which we choose to split i s the one
with  [maximum]  information gain  .
SOL-88  
  CH.SOL- 4.22.
TBD
SOL-89  
  CH.SOL- 4.23.
1
.Information gain is the expected r eduction in entr opy caused by
partitioning values in a dataset accor ding to a given attribute  .
2
.A de cision tr ee l earning algorithm chooses the next attribute to partition
the curr ently selected node, by first computing the information gain fr omthe entr opy , for instance, as a splitting criterion  .
3
.Ther e a r e 3 positive examples corr esponding to Shrinkage= +, and 2
negative examples corr esponding to Shrinkage=-. Using the formulae:
and the pr obabilities:
the overall entr opy befor e splitting is (  4.1 1  ):
1 import  autograd.numpy  as  np
2 def  binaryEntropy  (p):
3 r eturn  -  p *  np .  log2(p) -  ( 1-  p) *  np .  log2( 1-  p)
4
5 print  ( "binaryEntropy(p) is:  {}  bits"  .  format(binaryEntropy( 4/7  )))
6 >  binaryEntropy(p) is  : 0.97095  bits
F IGURE  4.1 1: Entropy before splitting  .
4
.If we split on  θ1  , (  4.5  ) the r elative shrinkage fr equency is:
T otal θ1  = T θ1  = F
3 0
1 1
T ABLE 4.5  : Splitting on θ 1  .T o compute the information gai n (IG) based on featur e θ  1  , we must
first compute the entr opy of γ after a split based on θ  1  , H(γ| θ  1  ):
Ther efor e, using the data for the  the r elative shrinkage fr equency (  4.5
), the information gain after splitting on θ  1  is:
Now we know that P  ( θ  1  = T  ) = 4/5  and P  ( θ  1  = F  ) = 1/5  , ther efor e:
SOL-90  
  CH.SOL- 4.24.
Ther e ar e 4 pos itive examples corr esponding to Expansion=+, and 3
negative examples corr esponding to Expansion=-  .
1
.The overall entr opy befor e splitting is (  4.12  ):
1 import  autograd.numpy  as  np
2 def  binaryEntropy  (p):
3 r eturn  -p *  np .  log2(p) -  ( 1-  p) *  np .  log2( 1-  p)
45 print  ( "binaryEntropy(p) is:  {}  bits"  .  format(binaryEntropy( 4/7  )))
6 >  binaryEntropy(p) is  : 0.9852281  bits
F IGURE  4.12: Entropy before splitting  .
2
.If we split on θ  1  , (  4.6  ) the r elative star expansion fr equency is:
T otal θ1  = T θ1  = F
3 1
0 3
T ABLE  4.6: Splitting on θ 1  .
Ther efor e, the information gain after splitting on A is:
Now we know that P  ( θ  1  = T  ) = 3/7  and P  ( θ  1  = F  ) = 4/7  ,
ther efor e:
3
.If we split on  θ2  , (  4.7  ) the r elative star expansion fr equency is:
T otal θ2  = T θ2  = F
+ 3 1
- 1 2
T ABLE  4.7: Splitting on θ 2  .
The information gain after splitting on B is:Now we know that P  ( θ  2  = T  ) = 4/7  and P  ( θ  2  = F  ) = 3/7  ,
ther efor e:
SOL-91  
  CH.SOL- 4.25.
1  .
2  .
3  .4.  False  .
4.3.5   Mutual Information
SOL-92  
  CH.SOL- 4.26.
1
.The diagram is depicted in Fig.  4.13  .
F IGURE  4.13: Mutual Information between H  ( S  ) & H  ( D  )  .
2
.Equivocation  is annotated by E  .
3
.Noise  is annotated by N  .
4
.The intersection  (shaded ar ea) in (  4.13  ) c orr es ponds to  mutual
information of the sour ce H  ( S  ) and of the destination H  ( D  ).5
.The formulae for mutual information is:
SOL-93  
  CH.SOL- 4.27.
The r elative entr opy D  ( p||q  ) is th e me asur e o f  dif ference  between
two dist ributions. It can also be expr essed like a measur e of the
inef ficiency  of assuming that the distribution is q when the  true
distribution is p  .
SOL-94  
  CH.SOL- 4.28.
Mutual information is a Shannon entr opy-based measur e of
dependence bet ween random variables. The mutual inform ation
between X and Z can be understood as the  reduction  of  the
uncertainty  in X given Z:
wher e H  is the Shannon entr opy , and H  ( X | Z  ) is the conditional
entr opy of Z given X  .
4.3.6   Mechanical Statistics
SOL-95  
  CH.SOL- 4.29.
Is this question valuable?
SOL-96  
  CH.SOL- 4.30.
Boltzmann r elat ed the degr ee of disor der of the state of a physical
system to the lo garithm of its pr obability . If, for example, the s ystem
has n non-intera cting and identical particles, each capable of ex isting
in ea ch of K equally likely states, the leading term in the logarithm ofthe pr obability of finding the system in a configuration with n  1
particles in state 1, n  2  in state 2, etc, is given by the  Boltzmann
entropy 
  , wher e π  i  = n  i  /n  .
SOL-97  
  CH.SOL- 4.31.
Ther e ar e  8 equipr obable events in each r oll of the dice, ther efor e:
4.3.7   Jensen’ s inequality
SOL-98  
  CH.SOL- 4.32.
1
.A fu nction f is concave in the ra nge  [ a, b  ] if fϕ  2 is negative in the
range  [ a, b  ].
2
.A function f is convex in the ran ge  [ a, b  ] if fϕ  2 is positive in the
range  [ a, b  ].
3
.The following inequality was published by J.L. Jensen in 1906:
(Jensen’ s Inequality)  Let f be a functi on convex up on  ( a, b  ).
Then for any n  ≥ 2 numbers x  i  ∊  ( a, b  ):
and that  the equality is attained if and only if f is linear or all x  i
ar e equal  .
For a convex down function, the sign of the inequality changes
to  ≥.Jensen’ s inequal ity states that if f is convex in the range  [ a, b  ],
then:
Equality holds if and only if a  = b. Jensen’ s inequality states that
if f is concave in the range  [ a, b  ], then:
Equality holds if and only if a  = b  .
SOL-99  
  CH.SOL- 4.33.
T rue  The non-negativity of KLD can be pr oved using Jensen’ s
inequality  .
Ref e r e n c e s
[  1
]S. Carnot. Reflections on the Motive Power of Fir e: And Other
Papers on the S econd Law of Thermodynamics  . Dover books on
physics. Dover Publications, 2012 (cit. on p. 86  ).
[  2
]T . M . Cover and J. A. Thomas. Elements of Information Theory  .
John W iley and Sons, Inc., 2006 (cit. on p. 86  ).
[  3
]B. J. Hiley . ‘ From the Heisenber g Picture to Bohm: a New
Perspective on Active Information and its relation to Shannon
Information’. In: Pr oc. Conf. Quantum Theory: r econsideration of
foundations  (2002), pp. 141–162 (cit. on p. 87  ).
[  4
]C. Shannon. ‘A mathematical theory of communication’. In: Bell
System T echnical Journal  27 (1948 ), pp. 379–423 (cit. on pp. 86  , 90
).
[  5 P . S ledzinski et al. ‘The curre nt state and future perspectives of] cannabinoids in cancer biology’ . In: Cancer Medicine  7.3 (2018), pp.
765–775 (cit. on p. 95  ).C HAPTER
5    DEEP LEARNING: CALCUL US , ALGORITHMIC DIFFERENTIA TION
The true logic of this world is in the  calculus  of pr obabilities  .
— James C. Maxwell
C o n t e n t s
Intr oduction
Pr oblems
AD, Gradient descent & Backpropagation
Numerical dif ferentiation
Directed Acyclic Graphs
The chain rule
T aylor series expansion
Limits and continuity
Partial derivatives
Optimization
The Gradient descent algorithm
The Backpropagation algorithm
Feed forward neural networks
Activation functions, Autograd/JAX
Dual numbers in AD
Forward mode AD
Forward mode AD table construction
Symbolic dif ferentiation
Simple dif ferentiation
The Beta-Binomial model
SolutionsAlgorithmic dif ferentiation, Gradient descent
Numerical dif ferentiation
Directed Acyclic Graphs
The chain rule
T aylor series expansion
Limits and continuity
Partial derivatives
Optimization
The Gradient descent algorithm
The Backpropagation algorithm
Feed forward neural networks
Activation functions, Autograd/JAX
Dual numbers in AD
Forward mode AD
Forward mode AD table construction
Symbolic dif ferentiation
Simple dif ferentiation
The Beta-Binomial model
5 . 1   I n t r o d u c t i o n
ALCULUS is the mathematics of change; the dif ferentiation of a
function is key t o almost every domain in the scientific and engi neering
realms and calculus is also v ery much central  to DL. A standard
curriculum of first year calculus  includes topics such as limits,
dif ferentiation, the derivative, T aylor series, integration, a nd the
integral. Many aspiring data scientists who lack a relevant
mathematical ba ckground and are shifting careers, hope to eas ily enter the field but
frequently encounter a mental barricade.
f  ( x  ) f  ′ ( x  )
sin( x  ) cos( x  )
cos( x  ) −  sin( x  )
log( x  )
e  xe  x
Thanks to the ra pid advances in processing power and the prolif eration of GPUs, it
is po ssible to lend the burden of computation to a computer with high ef ficiency and
precision. For instance, extremely fast implementations of backpropagation, the
gradient descent algorithm, and automatic differ entiation  (AD) [  5  ] brought artificial
intelligence from a mere concept to reality .
Calculus is frequently taught in a way that is very burdensome to the student,
therefore I tried incorporating t he writing of Python code snip pets into the learning
process and the usage of:
DAGs  ( D  irected A  cyclic G  raphs). Gradient descent is the essence of optimization in
deep learning, which requires ef ficient access to first and second order derivatives that
AD fram eworks provide. While  older AD frameworks were written in C++ ([  4  ]), the
newer ones are Python-based such as Autograd ([  10  ]) and JAX ([  3  ], [  1  ]).
Derivatives are also crucial in g raphics applications. For example, in a rendering
technique entitled global illumination  , photons bounce in a synthetically generated
scene w hile thei r direction and colour has to be determined using derivatives base d on
the spec ific ma terial each photon hits. In ray tracing algorithms, the colour of the
pixels is determined by tracing the trajectory the photons trav el from the eye of the
observer through a synthetic 3D scene.
A functi on is usually represente d by a DAG  . F or instance, o ne commonly used
form is to represent intermedia te values as nodes and operatio ns as arcs ( 5.2  ). One
other commonly used form is to represent not only the values but also the operations
as nodes ( 5.1 1  ).
The first representation of a function by a DAG goes back to [  7  ].
F IGURE  5.1: Intermediate value theorem
Manual dif ferentiation is tedious and error -prone and practically unusable for real-
time graphics applications wh erein numerous successive derivatives have to be
repeatedly calculated. Symbolic dif ferentiation on the other han d, is a computer based
method that uses a collection of dif ferentiation rules to analytically calculate an exact
derivative of a function resulting in a purely symbolic deriva tives. Many symbolic
dif ferentiation li braries utilize what is known as operator -overloading  ([  9  ]) for both
the forward and reverse forms of dif ferentiation, albeit they are not quite as fast as AD.
5 . 2   P r o b l e m s
5.2.1   AD, Gradient descent & Backpropagation
AD [ 5  ] is the applicat ion of the chai n rule to functions by comput ers in order to
automatically c ompute derivatives. AD plays a significant role in training deep
learning algorith ms and in order to understand AD you need a solid grounding in
Calculus. As opposed to numeri cal dif ferentiation, AD is a pro cedure for establishing
exact  derivatives with out any truncation errors. AD breaks a computer program into a
series of fundamental mathematical operations, and the gradient or Hessian of the
computer program is found by successive application of the chain rule ( 5.1  ) to it’ s
elementary constituents.
For instance, in the C++ pro gramming language, two techniques ([ 4  ]) are
commonly utiliz ed in transforming a program that calculates numerical values of a
function into a program which calculates numerical values for derivatives of that
function; (1) an operator ove rloading approach and (2) systematic source code
transformation.
One not able feature of AD is  that the values of the derivatives produced by
applying AD, as opposed to num erical dif ferentiation (finite di f ference formulas), are
exact and accurate  . T wo variants of AD are wi dely adopted by the scientific
community: the forward mode or the reverse mode where the underlying distinction
between them is the order in wh ich the chain rule is being utilized. The forward mode,
also entitled tangent mode, pro pagates derivatives from the dependent towards the
independent variables, whereas the reverse or adjoint mode does exactly the opposite.
AD makes heavy use of a conc ept known as dual numbers (DN) first introduced by
Clif ford ([  2  ]).F IGURE  5.2: A Computation graph with intermediate values as nodes and operations as arcs  .
5.2.2   Numerical dif ferentiation
PRB-100  
  CH.PRB- 5.1.
1  . W rite the formulae for the  finite dif ference rule  used in numerical
differ entiation  .
2  . What is the main pr oblem with this formulae?
3  . Indicate one pr oblem with softwar e tools which utilize numerical differ entiation
and successive operations on floating point numbers  .
PRB-101  
  CH.PRB- 5.2.
1  . Given a function f  ( x  ) and a point a, de fine the  instantaneous r ate of change  of
f  ( x  ) at a  .
2  . What other commonly used alt ernative name does the instantaneous rate of
change have?
3  . Given a function f  ( x  ) and a point a, define the  tangent line  of f  ( x  ) at a  .
5.2.3   Directed Acyclic Graphs
There are two possible ways to traverse a DAG  ( D  irected A  cyclic G  raph). One
method is simple. Start at the bottom and go through all nodes to the top of the
computational tree. That is nothing else than passing the corresponding computation
sequence top down. Based on this method, the so called forward mode or of AD was
developed [  8  ]. In contrast to this forward m ode the reverse mode was first used by
Speelpenning [  13  ] w ho passed th e underlying graph top down and propagated the
gradient backwards.PRB-102  
  CH.PRB- 5.3.
1  . State the definition of the derivative f  ( c  ) of a function f  ( x  ) at x  = c  .
2  . W ith r espect to the  DAG  depicted in  5.3  :
F IGURE  5.3: An expression g raph for g  ( x  ). Constants are shown in gray , crossed-out since derivatives
should not be propagated to constant operands  .
(a) T raverse the graph  5.3  and find the function g  ( x  ) it r epr esents  .
(b) Using the definition of the derivative, find f  ′ (9).
PRB-103  
  CH.PRB- 5.4.
1  . W ith r espect to the expr ession graph depicted in  5.4  , traverse the graph and
find the function g  ( x  ) it r epr esents  .
F IGURE  5.4: An expression g raph for g  ( x  ). Constants are shown in gray , crossed-out since derivatives
should not be propagated to constant operands  .
2  . Using the definition of the derivative find the derivative of g  ( x  ).5.2.4   The chain rule
PRB-104  
  CH.PRB- 5.5.
1  . The  chain rule  is key concept in differ entiation. Define it  .
2  . Elaborate how the chain rule is utilized in the context of neural networks  .
5.2.5   T aylor series expansion
The idea behind a T aylor ser ies is that if you know a fu nction and all its
derivatives at one point x  = a  , you  can approx imate the function at other points near a
. As an example, take 
  . Y ou can use T aylor series to approximate 
by knowing f  (9) and all the derivatives f  ′ (9), f  ′′ (9).
The Mac Laurin series ( 5.2  ) is a special cas e of T aylor serie s when f  (0), f  ′ (0) are
known:
For instance, the Maclaurin expansion of cos  ( x  ) is:
When evaluated at 0 results in:
PRB-105  
  CH.PRB- 5.6.
Find the T aylor series expansion for:
1  .
2  .
3  .
4  .
PRB-106  
  CH.PRB- 5.7.
Find the T aylor series expansion for:
PRB-107  
  CH.PRB- 5.8.
Find the T aylor series expansion center ed at x  = −  3 for:
PRB-108  
  CH.PRB- 5.9.
Find the  101 th degr ee T aylor polynomial center ed at x  = 0 for:
PRB-109  
  CH.PRB- 5.10.
At x  = 1, compute the first 7 terms of the T aylor series expansion of:
5.2.6   Limits and continuityTheor em 1  (L ’Hopital’ s rule).
PRB-1 10  
  CH.PRB- 5.1 1.
Find the following limits:
1  .   
2  .   
3  .   
5.2.7   Partial derivatives
PRB-1 1 1  
  CH.PRB- 5.12.
1  . T rue or false  : When applyin g a partial derivative, ther e ar e two variable s
consider ed constants - the dependent and independent variable  .
2  . Given g  ( x, y  ), find its partial derivative with r espect to x:
PRB-1 12  
  CH.PRB- 5.13.
The gradient of a two-dimensional function is given by
1  . Find the gradient of the function:2  . Given the function:
evaluate it at  ( −  1, 0), dir ected at  (1, 1).
PRB-1 13  
  CH.PRB- 5.14.
Find the partial derivatives of:
PRB-1 14  
  CH.PRB- 5.15.
Find the partial derivatives of:
5.2.8   Optimization
PRB-1 15  
  CH.PRB- 5.16.
1  . Wher e is f  ( x  ) well defined?
2  . Wher e is f  ( x  ) incr easing and decr easing?
3  . Wher e is f  ( x  ) r eaching minimum and maximum values  .
PRB-1 16  
  CH.PRB- 5.17.
Consider f  ( x  ) = 2 x  3 − x  .
1  . Derive f  ( x  ) and conclude on its behavior  .2  . Derive once again and discuss the concavity of the function f  ( x  ).
PRB-1 17  
  CH.PRB- 5.18.
Consider the function
and find maximum, minimum, and saddle points  .
5.2.9   The Gradient descent algorithm
PRB-1 18  
  CH.PRB- 5.19.
The gradient descent algorithm can be utilized for the minimization of convex
functions. Statio nary points ar e r equir ed in or der to minimize a convex function. A
very simple appr oach for findin g stationary points is to start at an arbitrary point,
and move along the gradient at that point towar ds the next point, and r epeat until
conver ging to a stationary point  .
1  . How is the vector of all partial derivatives for a function f  ( x  ) entitled?
2  . Complete the sentence: when sear ching for a minima, if the derivative is
positive, the function is  increasing/decreasing  .
3  . The function x  2 as depicted in  5.5  , h as a d erivative of f  ′ ( x  ) = 2 x. Evaluated at
x  = −  1, the derivative equals f  ′ ( x  = −  1) = −  2. At x  = −  1, the function is
decreasing  as x gets lar ger . W e will happen if we wish to find a  minima  using
gradient descen t, and incr ease (decr ease) x by  the size of the gradient  , and
then again r epeatedly keep jumping?
4  . How this phenomena can be alleviated?
5  . T rue or False:  The gradient descent algorithm is guaranteed to find a loca
minimum if the learning rate is corr ectly decr eased and a finite local minimum
exists  .F IGURE  5.5: x  2 Function
PRB-1 19  
  CH.PRB- 5.20.
1  . In a least-squa r es linear r egr ession pr oblem, adding an L2 r egularization
penalty cannot decr ease the L2 err or of the solution w on the training data?
2  . Is the data linearly separable?
3  . What is loss function for linear r egr ession?
4  . What is the gradient descent algorithm to minimize a function f  ( x  ) ?
5.2.10   The Backpropagation algorithmThe most important, expensive  and hard to implement part  of any hardware
realization of ANNs is the no n-linear activation function of a neuron. Commonly
applied activatio n functions are the sigmoid and the hyperboli c tangent. In the most
used learning algorithm in prese nt day applications, back-propagation, the derivatives
of the sigmoid function are needed when back propagating the errors.
The backpropag ation algorithm  looks for the minimum of the error function in
weight space using the method of gradient descent.
PRB-120  
  CH.PRB- 5.21.
1  . During t he train ing of an ANN, a sigmoid layer applies the sigmoid function to
every element in the forwar d pass, while in the backwar d pass the chain rule is
being ut ilized as part of the backpr opagation algorithm. W ith  r espect to the
backpr opagation algorithm, given a sigmoid  
  activation
function, and a J as the cost function, annotate each part of equation  ( 5.21  ):
2  . Code snippet  5.6  pr ovides a pur e Python-based (e.g. not using Autograd)
implementation of the forwar d pass for the sigmoid function. Complete the
backwar d pass that dir ectly computes the analytical gradients  .
1 class  Sigmoid  :
2 def  forward  ( self  ,x):
3 self  .  x =  x
4 r eturn  1/  ( 1+  np .  exp( -  x))
5 def  backward  ( self  , grad):
6 grad_input =  [ ???  ]
7 r eturn  grad_input
F IGURE  5.6: Forward pass for the sigmoid function  .
PRB-121  
  CH.PRB- 5.22.
This question deals with the effe ct of customized transfer functions. Consider a
neural network with hidden units that use x  3 and output units that use  sin(2 x  ) as
transfer functions. Using the chain rule, starting fr om ∂E/∂y  k  , derive the formulasfor the weight updates ∆w  jk  and ∆w  ij  . No tice - do not include partial derivatives
in your final answer  .
5.2.1 1   Feed forward neural networks
Understanding the inner -workings of Feed Forward Neural Ne tworks (FFNN) is
crucial to the understanding of other , more advanced Neural Networks such as CNN’ s.
A Neur al Netw ork (NN) is an interconnected assembly of simple
processing elements, units  or nodes  , whose functionality is loosely based
on the animal neuron. The processing ability of the network is stored in
the inter -unit connection strengt hs, or weights  , obtained by a process of
adaptation to, or learning  from, a set of training patterns. [  6  ]
The Backpr opagation Algorithm  is the most widely used learning algorithm for
FFNN. Backpro pagation is a training method that uses the Generalized Delta Rule  . Its
basic ide a is to perform a gradient descent on the total square d error of the network
output, considered as a function of the weights. It was first described by W erbos  and
made popular by Rumelhart  ’ s, Hinton  ’ s and W illiams  ’ paper [  12  ].
5.2.12   Activation functions, Autograd/JAX
Activation func tions, and most commonly the sigmoid activation function, are
heavily used for the construction of NNs. W e utilize Autograd ([ 10  ]) and the recently
published JAX ([ 1  ]) library to learn about the relationship between activation
functions and the Backpropagation algorithm.
Using a logistic, or sigmoid, activation function has some benefits in being able to
easily take derivatives and then interpret them using a logistic regression model.
Autograd is a core module in PyT orch ([  1 1  ]) and adds inherit support for automatic
dif ferentiation for all operatio ns on tensors and functions. Moreover , one can
implement his own custom Aut ograd function by sub classing t he autograd F unction
and implementing the forward and backward passes which operate on PyT orch tensors.
PyT orch provides a simple syntax ( 5.7  ) wh ich is tran sparent to both CPU/GPU
support.
import  tor ch
fr om  tor ch.autograd  import  Function
class  DLFunction  (Function):@staticmethod
def  forward  (ctx, input  ):
...
@staticmethod
def  backward  (ctx, grad_output):
...
F IGURE  5.7: PyT orch syntax for autograd  .
PRB-122  
  CH.PRB- 5.23.
1  . T rue or false:  In Auto grad, i f any input te nsor of an operation has
r equir es_grad=T rue, the computation will be tracked. After computing the
backwar d pass, a gradient w .r .t. this tensor is accumulated into .grad attribute
2  . T rue or false:  In Autog rad, multiple calls to backwar d will sum up pr eviousl y
computed gradients if they ar e not zer oed  .
PRB-123  
  CH.PRB- 5.24.
Y our frie nd, a veteran of the DL community wants to use logistic r egr ession and
implement custom activation fun ctions using Autograd. Logistic r egr ession is used
when the variable y that we want to pr edict can only take on discr ete values (i.e.
classification). C onsidering a binary classification pr oblem (y =  0 or y = 1) (  5.8  ),
the hypothesis function could b e defined so that it is bounded between  [0, 1] in
which we use so me form of logistic function, such as the  sigmoid function  . Other ,
mor e eff icient fu nctions exist such as the  ReLU  (Rectified Linear Unit) which we
discussed later  .F IGURE  5.8: A typical binary classification problem  .
1  . Given the sigmoid function:  
  what is the exp r ession for the
hypothesis in logistic r egr ession?
2  . What is the decision boundary?
3  . What does h  Θ  ( x  ) = 0.8 mean?
4  . Using an Autograd based Python pr ogram, implement both the forwar d and
backwar d pass for the sigmoid activation function and evaluate it’ s derivative at
x  = 1
5  . Using an Autograd based Python pr ogram, implement both the forwar d and
backwar d pass for the ReLU activation function and evaluate it’ s derivative at x
= 1
PRB-124  
  CH.PRB- 5.25.
Y our friend, a veteran of the DL community wants to implement a custom
activation funct ion using Autograd and asks for your help. Consider a function
accepting two tensors as input and an output obeying the following mapping:
5.2.13   Dual numbers in AD
Dual nu mbers (D N) are analogous to complex numbers and augment real numbers
with a dual element by adjoining an infinitesimal element d  , for which d  2 = 0.
PRB-125  
  CH.PRB- 5.26.
1  . Explain how AD uses floating po int numerical rather than symbolic expr essions
.
2  . Explain the notion of DN as intr oduced by ([  2  ])  .
3  . What arithmetic operations ar e possible on DN?  .
4  . Explain the r elationship between a T aylor series and DN  .
PRB-126  
  CH.PRB- 5.27.
1  . Expand the following function using DN:
2  . W ith r espect to the expr ession graph depicted in  5.9  :
F IGURE  5.9: An expression g raph for g  ( x  ). Constants are shown in gray , crossed-out since derivatives
should not be propagated to constant operands  .
(a) T raverse the graph  5.9  and find the function g  ( x  ) it r epr esents  .
(b) Expand the function g  ( x  ) using DN  .
3  . Show that the  general identity  :holds in this particular case too  .
4  . Using the derived DN, evaluate the function g  ( x  ) at x  = 2.
5  . Using an Autograd based Pytho n pr ogram implement the function and evaluate
it’ s derivative at x  = 2.
PRB-127  
  CH.PRB- 5.28.
W ith r espect to the expr ession graph depicted in  5.10  :
F IGURE  5.10: An expression graph for g  ( x  ). Constants are shown in gray , crossed-out since derivatives
should not be propagated to constant operands  .
1  . T raverse the graph  5.10  and find the function g  ( x  ) it r epr esents  .
2  . Expand the function g  ( x  ) using DN  .
3  . Using the derived DN, evaluate the function g  ( x  ) at x  = 5.
4  . Using an AutoGrad based Pytho n pr ogram implement the function and evaluate
it’ s derivative at x  = 5.
5.2.14   Forward mode AD
PRB-128  
  CH.PRB- 5.29.
When differ entiating a function  using forwar d-mode AD, the computation of
such an expr ession can be com puted fr om its corr esponding dir ected a-cyclical
graph by pr opagating the numerical values  .1  . Find the function, g  ( A, B, C  ) r epr esented by the expr ession graph in  5.1 1  .
F IGURE  5.1 1: A computation graph for g  ( x  )
2  . Find the partial derivatives for the function g  ( x  ).
PRB-129  
  CH.PRB- 5.30.
Answer the following given tha t a computational graph of a function has N
inputs and M outputs  .
1  . T rue or False  ?:
(a) Forwar d and r everse mode AD always yield the same r esult  .
(b) In r everse mode AD ther e ar e fewer operations (time) and less spac
intermediates (memory)  .
(c) The cost for forwar d mode gr ows with N  .
(d) The cost for r everse mode gr ows with M  .
PRB-130  
  CH.PRB- 5.31.
1  . T ransform the sour ce code in code snippet  5.1  into a function g  ( x  1  , x  2  ).
C ODE  5.1: A function, g  ( x  1  , x  2  ) in the C programming language.
1 float  g  (  float  x1 ,  float  x2) {
2 float  v1, v2, v3 , v4 , v5;
3 v1  =  x1;
4 v2  =  x2;
5 v3  =  v1  *  v2;6 v4  =  ln (v1 );
7 v5  =  v3  +  v4;
8 return  v5;
9 }
2  . T ransform the function g  ( x  1  , x  2  ) into an expr ession graph  .
3  . Find the partial derivatives for the function g  ( x  1  , x  2  ).
5.2.15   Forward mode AD table construction
PRB-131  
  CH.PRB- 5.32.
1  . Given the function:
and the graph  5.1  , ann otate each vertex (edge) of  the graph with the partial
derivatives that would be pr opagated in forwar d mode AD  .
2  . T ransform the graph into a  table  that computes the  function  : g  ( x  1  , x  2  )
evaluated at  ( x  1; x  2) = ( e  2 ; π  ) using forwar d-mode AD  .
3  . W rite and run a Python code snippet to pr ove your r esults ar e corr ect  .
4  . Describe the r ole of  seed values  in forwar d-mode AD  .
5  . T ransform the graph into a  table  that computes the  derivative  of g  ( x  1  , x  2  )
evaluated at  ( x  1; x  2) = ( e  2 ; π  ) using forwar d-mode AD for x  1  as the chosen
independent variable  .
6  . W rite and run a Python code snippet to pr ove your r esults ar e corr ect  .
5.2.16   Symbolic dif ferentiation
In th is s ection, w e introduce the basic functionality of the SymPy (SYMbolic Python)
library commonly used for symbolic mathematics as a means to deepen your
understanding in both Python and calculus. If you are using Sympy in a Jupyter
notebook in Google Colab (e.g. https://colab.research.google.com/  ) then renderingsympy equation s requires MathJax to be available within each cell output. The
following is a hook function that will make this possible:
C ODE  5.2: Sympy in Google Colab
1 fr om  IPython.display  import  Math, HTML
2 def  enable_sympy_in_cell  ():
3 display(HTML( "<script
     src='  https://cdnjs.cloudflare.com/ajax/libs/  "
4 "mathjax/2.7.3/latest.js?config=default'>
5 </script> "))
6 get_ipython() .  events .  register( 'pre_run_cell'  ,
     enable_sympy_in_cell)
After successfully registering this hook, SymPy rendering ( 5.3  ) will work
correctly:
C ODE  5.3: Rendering Sympy in Google Colab
1 import  sympy
2 fr om  sympy  import  *
3 init_printing()
4 x, y , z =  symbols( 'x y z'  )
5 Integral(sqrt( 1/  x), (x, 0  , oo))
It is also recommended to use the latest version of Sympy:
C ODE  5.4: Updating Sympy
> pip install --upgrade sympy
5.2.17   Simple dif ferentiation
PRB-132  
  CH.PRB- 5.33.
Answer the following questions:
1  . Which differ entiation method is inher ently pr one to r ounding err ors?
2  . Define the term symbolic differ entiation  .PRB-133  
  CH.PRB- 5.34.
Answer the following questions:
1  . Implement the sigmoid function  
  symbolically using a
Python based SymPy pr ogram  .
2  . Differ entiate th e sigmoid function using SymPy and compa r e it with the
analytical derivation σ  ′ ( x  ) = σ  ( x  )(1 − σ  ( x  )).
3  . Using SymPy , evaluate the gradient of the sigmoid function at x  = 0.
4  . Using SymPy , plot the r esulting gradient of the sigmoid function  .
5.2.18   The Beta-Binomial model
PRB-134  
  CH.PRB- 5.35.
Y ou will most likely not be given such a long pr ogramming task during a face-
to-face interview . Nevertheless, an extensive home pr ogramming assignment is
typically given at many of the start-ups I am familiar with. Y ou should allocate
ar ound appr oximately four to six hours to completely answer all questions in this
pr oblem  .
W e discussed the Beta-Binomial model extensively in chapter  3  . Rec all that the
Beta-Binomial distribution is fr equently used in Bayesian statistics to model the
number of successes in n trials. W e now employ SymPy to do the  same; demonstrate
computationally how  a prior distribu tion is updated to develop into a posterior
distribution after observing th e data via the r elationship of the Beta-Binomial
distribution  .
Pr ovided the pr obability of success, the number of successe s after n trials
follows a binomial distribution. Note that the beta distribution is a conjugate prior
for t he p arameter of the binomial distribution. In this case, the likelihood function
is binomial, and a beta prior distribution yields a beta posterior distribution  .
Recall that for the Beta-Binomial distribution the following r elationships exist:
1  . Likelihood: The  starting point f or our infer ence pr oblem is the  Likelihood, thepr obability of the  observed  data. Find the Likelihood function symbolically
using sy mpy . Convert the SymPy r epr esentation to a pur ely Numpy based
callable function  with a Lambda expr ession. Evaluate the Likelihood function a
θ  = 0.5 with 50 successful trials out of 100  .
2  . Prior: T he Beta Distribution. D efine the Beta distribution which will act as our
prior  distribution symbolically using sympy . Convert the SymPy r epr esentation
to a pur e ly Num py based callable function. Evaluate the Beta Distribution at θ  
0.5, a  : 2, b  : 7
3  . Plot the Beta distribution, using the Numpy based function  .
4  . Posterior: Find the  posterior  distribution by multiplying our Beta prior by the
Binomial Like lihood symbolically using sympy . Convert the SymPy
r epr esentation to a pur ely Numpy based callable function . Evaluate the
Posterior Distribution at θ  : 0.5, a  : 2, b  : 7
5  . Plot the posterior distribution, using the Numpy based function  .
6  . Show tha t the posterior distribut ion has the same functional dep endence on θ as
the prior , and it is just  another Beta distribution  .
7  . Given:
Prior  : Beta( θ  | a  = 2, b  = 7) = 56 θ  (− θ  + 1)6 and:
Likelihood  : Bin( r  = 3| n  = 6, θ  ) = 19600 θ  3 ( −θ  + 1)47 find the r esulting
posterior distribution and plot it  .
5 . 3   S o l u t i o n s
5.3.1   Algorithmic dif ferentiation, Gradient descent
5.3.2   Numerical dif ferentiation
SOL-100  
  CH.SOL- 5.1.
1  . The formulae is:
2  . The mai n pr oble m with this form ulae is that it suffers fr om numerical instability
for small values of h  .
3  . In some numerical softwar e systems, the number  
  may be r epr esented as the
a floating point number ≈  1.414213562. Ther efor e, the r esult of:
float  
  * float  
  may equal ˇ  2.000000446.
SOL-101  
  CH.SOL- 5.2.
1  . The instantaneous rate of change equals:
2  . The instantaneous rate of change of f  ( x  ) at a is also commonly known as the
tangent line of f  ( x  ) at a  .
3  . Given a function  f  ( x  ) and a point a, th e tangent (Fig.  5.12  ) lin e of f  ( x  ) at a is
a line that touches f  ( a  ) but does not cr oss f  ( x  ) (sufficiently close to a)  .
F IGURE  5.12: A T angent line
5.3.3   Directed Acyclic Graphs
SOL-102  
  CH.SOL- 5.3.
1  . The definition is:
2  . If we traverse the graph  5.3  fr om left to right we derive the following function:
SOL-103  
  CH.SOL- 5.4.
1  . The function g  ( x  ) = 2 x  2 − x  + 1 r epr esents the expr ession graph depicted in
5.4.
2  . By the definition:5.3.4   The chain rule
SOL-104  
  CH.SOL- 5.5.
1  . The cha in rule states that the pa rtial derivative of E  = E  ( x, y  ) with r espect to x
can be calculated via another variable y  = y  ( x  ), as follows:
2  . For inst ance, the chain rule [  8  ] is applied in neural networks to calculate the
change in its we ights r esulting fr om tuning the cost function. This derivative is
calculated via a chain of partial derivatives (e.g. of the activation functions)  .
5.3.5   T aylor series expansion
SOL-105  
  CH.SOL- 5.6.
1  .
2  .
3  .
4  .
SOL-106  
  CH.SOL- 5.7.
SOL-107  
  CH.SOL- 5.8.
In this case, all derivatives can be computed:SOL-108  
  CH.SOL- 5.9.
The immediate answer is 1. Refer to eq.  5.35  to verify this logical consequence  .
SOL-109  
  CH.SOL- 5.10.
By employing eq.  5.36  , one  can substi tute x by  3 − x and  genera te the first 7
terms of the x-dependable outcome befor e assigning the point x  = 1.
5.3.6   Limits and continuity
SOL-1 10  
  CH.SOL- 5.1 1.
1  . W ith an indeterminate form  0 /  0, L ’Hopital’ s rule holds. W e look at
which equals to the original limit  .
2  . Again, we yield  0 /  0 at interim, so we look at the first or der derivative
The original limit is also equal to  1.3  . This tim e, the intermediate form is of ∞/∞ and L ’Hopital applies as well. The
quotient of the derivatives is
As x → ∞, this goes to ∞, so the original limit is equal to ∞ also  .
5.3.7   Partial derivatives
SOL-1 1 1  
  CH.SOL- 5.12.
1  . T rue  .
2  . By tr eating y as constant, one can derive that
SOL-1 12  
  CH.SOL- 5.13.
1  .
2  . It can be shown that  ∇ g  ( x, y  ) = (2 xy  + y  2 ) i  + ( x  2 + 2 xy  − 1) j  at  ( −  1, 0)
equals  (0, 0). Accor ding to the definition of dir ectional derivative:
SOL-1 13  
  CH.SOL- 5.14.
SOL-1 14  
  CH.SOL- 5.15.
5.3.8   Optimization
SOL-1 15  
  CH.SOL- 5.16.
1  . The function is only defined wher e x ≠ −  2, in the domain of:
( −∞, −  2) ∪  ( −  2, + 1  ).
2  . By a simple quotient-based derivation:
Namely , expect for the ill-define d x  = −  2, the critical point of x  = 0.5 should
be consider ed. For x >  0.5, the derivative is positive and the function
incr eases, in contrast to x <  0.5.
3  . The r equested coor dinate is  (0.5, 0.2).
SOL-1 16  
  CH.SOL- 5.17.1  . f′  ( x  ) = 6 x  2 −  1, which entails the behavior of the function changes ar ound the
points  
  . The derivative is negative between  
  and
 , i.e., it decr eases in the domain, and incr eases otherwise  .
2  . The second derivative is f′′  ( x  ) = 12 x, which  means the function is c oncave for
negative x values and convex otherwise  .
SOL-1 17  
  CH.SOL- 5.18.
The function should be derived  accor ding to each variable sep arately and be
equated to 0, as follows:
So, the solution to these equations yield the coor dinate  (0, 0), and f  (0, 0) = 0.
Let us derive the second or der derivative, as follows:
Also, the following r elation exists:
Thus, the critical point  (0, 0) is a minimum  .
5.3.9   The Gradient descent algorithm
SOL-1 18  
  CH.SOL- 5.19.
1  . It is the gradient of a function which is mathematically r epr esented by:3  .
2  . Incr easing  .
3  . W e will keep jumping between the same two points without ever r eaching a
minima  .
4  . This phe nomena can be alleviated by using a  learning rate or step size  . For
instance, x + = 2 * η wher e η is a learning rate with small va lue such as η =
0.25  .
5  . T rue  .
SOL-1 19  
  CH.SOL- 5.20.
1  . The L2 err or is alr eady minimz ed by the unr egularized solution, so no form of
r egularization can impr ove on that  .
2  . the point (5,5) has two classes, so the classes cannot be separated by any line  .
4  . Simple but fundamental algorithm for minimizing f. Just r epeate dly move in the
dir ection of the negative gradient
(a) Start with initial guess θ  (0) , step size η
(b) For k  = 1, 2, 3, . . .:
i  . Compute the gradient  ∇ f  ( θ  ( k− 1) )
ii  . Check if gradient is close to zer o; is so stop, otherwise continue
iii  . Update  θ( k ) = θ( k −1) − η  ∇ f  ( θ  ( k −1) )
(c) Return final θ  (k) as appr oximate solution θ  *
5.3.10   The Backpropagation algorithmSOL-120  
  CH.SOL- 5.21.
1  . The annotated parts of equation  ( 5.21  ) appear in  ( 5.46  ):
2  . Code snippet  5.13  pr ovides an implementation of both the forwar d and
backwar d pass the sigmoid function  .
1 class  Sigmoid  :
2 def  forward  ( self  ,x):
3 self  .  x =  x
4 r eturn  1/  (1 +  np .  exp( -  x))
5
6 def  backward  ( self  , grad):
7 grad_input = self  .  x *  ( 1-  self  .  x) *  grad
8 r eturn  grad_input
F IGURE  5.13: Forward and backward for the sigmoid activation function in pure Python  .
SOL-121  
  CH.SOL- 5.22.
The key concept  in this question  is understanding mer ely the transfer function
and its derivatives ar e changing comparing to traditional activation functions,
namely:
5.3.1 1   Feed forward neural networks
5.3.12   Activation functions, Autograd/JAX
SOL-122  
  CH.SOL- 5.23.
1  . T rue  .
2  . T rue  .
SOL-123  
  CH.SOL- 5.24.
The answers ar e as follows:
1  .
 .
2  . The decision boundary for the l ogistic sigmoid function is wher e hΘ  ( x  ) = 0.5
(values less than 0.5 means false, values equal to or mor e than 0.5 means true)  
3  . That ther e is a 80% chance that the instance is of the corr esponding class,
ther efor e:
hΘ  ( x  ) = g  ( Θ  0  + Θ  1  x  1  + Θ  2  x  2  ) and we pr edict y=1 if x  0  + x  1  + x  4  . The code snippet in  5.14  implements the function using Autograd  .
1 fr om  tor ch.autograd  import  Function
2 class  Sigmoid  (Function):
3 @staticmethod
4 def  forward  (ctx, x):
5 output = 1 /  ( 1 +  torch .  exp( -  x))
6 ctx .  save_for_backward(output)
7 r eturn  output
8
9 @staticmethod
1
0def  backward  (ctx, grad_output):
1
1output,    =  ctx .  saved_tensors
1
2grad_x =  output *  (1 -  output) *  grad_output
1
3r eturn  grad_x
F IGURE  5.14: Forward and backward for the sigmoid function in Autograd  .
5  . The code snippet in  5.15  implements the function using Autograd  .
1 fr om  tor ch.autograd  import  Function
2 class  ReLU  (torch .  autograd .  Function):
3 @staticmethod
4 def  forward  (ctx, input  ):
5 ctx .  save_for_backward( input  )
6 r eturn  input  .  clamp( min  =0  )
7
8 @staticmethod
9 def  backward  (ctx, grad_output):
1
0input  , =  ctx .  saved_tensors
1
1grad_input =  grad_output .  clone()
1
2grad_input[ input  < 0  ] = 0
1
3r eturn  grad_input
F IGURE  5.15: Forward and backward for the ReLU function in Autograd  .
SOL-124  
  CH.SOL- 5.25.  The answers ar e as follows:
The code snippet in  5.16  implements the function using Autograd  .
1 fr om  tor ch.autograd  import  Function
2 class  EQ  (Function):
3 @staticmethod
4 def  forward  (ctx, x1, x2):
5 # save everything to compute the gradient
6 ctx .  save_for_backward(x1, x2)
7 r eturn  (x1 *  x2) .  abs()
8 @staticmethod
9 def  backward  (ctx, grad_output):
1
0x1, x2 =  ctx .  saved_tensors
1
1r eturn  (grad_output *  x1 .  sign() *  x2 .  abs(), \
1
2grad_output *  x1 .  abs() *  x2 .  sign()) **2
F IGURE  5.16: Forward and backward for equation  ( 5.22  )  .
5.3.13   Dual numbers in AD
SOL-125  
  CH.SOL- 5.26.
The answers ar e as follows:
1
.The pr oc edur e of AD is to use verbatim text of a computer pr ogram which
calculates a numerical value an d to transform it into the text of  a computer
pr ogram called the transformed pr ogram which calculates the desir ed
derivative values. The transformed computer pr ogram carries out these
derivative calculations by r epea ted use of the chain rule however applied to
actual floating point values  rather than to a symbolic r epr esentation  .
2
.Dual numbers extend all numbe rs by adding a second componen t x  ↦  x + ẋ
d  wher e x  + ẋ is the  dual part  .
3
.The following arithmetic operations ar e possible on DN:
1  .
4
.For f  ( x  + ẋ  d  ) the T aylor series expansion is:
The  immediate  and important r esult is that all h igher -or der terms (n >  =
2 )  disappear  which pr ovides closed-form mathematical expr ession that
r epr esents a function and its derivative  .
SOL-126  
  CH.SOL- 5.27.
The answers ar e as follows:
2
.If we traverse the graph  5.9  fr om left to right we drive the following simple
function:
3
.W e know that:
Now if we expand the function using DN:
Rearranging:
But since g  ( x  ) = 3 * x  + 2 then:
4
.Evaluating the function g  ( x  ) at x  = 2 using DN we get:
5
.The code snippet in  5.17  implements the function using Autograd  .
1 import  autograd.numpy  as  np
2 fr om  autograd  import  grad
3 x = np .  array([ 2.0  ], dtype= float  )
4 def  f1  (x):
5 r eturn  3*  x + 2
6 grad_f1 =  grad(f1)
7 print  (f1(x)) # > 8.0
8 print  (grad_f1(x)) # > 3.0
F IGURE  5.17: Autograd
SOL-127  
  CH.SOL- 5.28.  The answers ar e as follows:
1
.If we traverse the graph  5.9  fr om left to right we drive the following
function:
2
.W e know that:
Now if we expand the function using DN we get:However by definition  ( d  2 ) = 0 and ther efor e that term vanishes.
Rearranging the terms:
But since g  ( x  ) = (5 * x  2 + 4 * x  + 1) then:
3
.Evaluating the function g  ( x  ) at x  = 5 using DN we get:
4
.The code snippet in  5.18  implements the function using Autograd  .
1 import  autograd.numpy  as  np
2 fr om  autograd  import  grad
3 x =  np .  array([ 5.0  ], dtype= float  )
4 def  f1  (x):
5 r eturn  5 *  x **2  + 4*  x +  1
6 grad_f1 =  grad(f1)
7 print  (f1(x)) # > 146.0
8 print  (grad_f1(x)) # > 54.0
F IGURE  5.18: Autograd
5.3.14   Forward mode AD
SOL-128  
  CH.SOL- 5.29.
The answers ar e as follows:
1 The function g  ( x  ) r epr esented by the expr ession graph in  5.1 1  is:.
2
.For a logarithmic function:
Ther efor e, the partial derivatives for the function g  ( x  ) ar e:
SOL-129  
  CH.SOL- 5.30.  The answers ar e as follows:
1
.T rue. Both dir ections yield the exact same r esults  .
2
.T rue. Reverse mode is mor e efficient than forwar d mode AD (why?)  .
3
.T rue  .
4
.T rue  .
SOL-130  
  CH.SOL- 5.31.
The answers ar e as follows:
1
.The function is2
.The graph associated with the forwar d mode AD is as follows:
F IGURE  5.19: A Computation graph for g  ( x  1  , x  2  ) in  5.1
3
.The partial derivatives ar e:
5.3.15   Forward mode AD table construction
SOL-131  
  CH.SOL- 5.32.
The answers ar e as follows:
1
.The graph with the intermediate values is depicted in (  5.20  )F IGURE  5.20: A derivative graph for g  ( x  1  , x  2  ) in  5.1
2
.Forwar d mode AD for g  ( x  1  , x  2  ) = ln ( x  1  ) + x  1  x  2  evaluated at  ( x  1  , x  2  )
= ( e  2 , ˇ  ).
Forward-mode function evaluation
v  −  1  = x  1 = e  2
v  0  = x  2 = π
v  1  = ln v  −  1 = ln ( e  2 ) = 2
v  2  = v  −  1  × v  0= e  2 × π  =
23.2134
v  3  = v  1  + v  22 + 23.2134 =
25.2134
f  = v  3 = ≈  25.2134
T ABLE  5.1: Forwa rd-mode AD table for y  = g  ( x  1  , x  2  ) = ln( x  1  )+ x  1  x  2  evaluated at ( x  1  , x  2  ) = ( e
2 ; π  ) and setting ẋ  1  = 1 to compute 
  .
3
.The following Python code (  5.21  ) pr oves that the numerical r esults ar e
corr ect:
1 import  math
2 print  (math .  log(math .  e *  math .  e) +  math .  e *  math .  e *  math .  pi)
3 > 25.2134
F IGURE  5.21: Python code- AD of the function g  ( x  1  , x  2  )4
.Seed va lues indicate the values by which the dependent and independent
variables ar e initialized to befor e being pr opagated in a computation graph.
For instance:
Ther efor e we set  ẋ  1  = 1 to compute  
  .
5
.Her e we construct a table for the forwar d-mode AD for the deri vative of f  ( x
1  , x  2  ) = ln ( x  1  ) + x  1  x  2  evaluated at  ( x  1  , x  2  ) = ( e  2 , π  ) while setting  ẋ  1
= 1 to compute  
  .. In forwar d-mode AD a derivative is called a  tangent  .
In the derivatio n that follows, note that mathematically using manual
differ entiation:
and also since  
  then
 .T ABLE  5.3: Forwa rd-mode AD table for y  = g  ( x  1  , x  2  ) = ln( x  1  )+ x  1  x  2  evaluated at ( x  1  , x  2  ) = ( e
2 ; π  ) and setting ẋ  1  = 1 (seed values are mentioned here:  3  ) to compute 
  .
6
.The following Python code (  5.22  ) pr oves that the numerical r esults ar e
corr ect:
1 import  autograd.numpy  as  np
2 fr om  autograd  import  grad
3 import  math
4
5 x1 =  math .  e *  math .  e
6 x2 =  math .  pi
7
8 def  f1  (x1,x2):
9 r eturn  (np .  log(x1) +  x1 *  x2)1
0
1
1grad_f1 =  grad(f1)
1
2
1
3print  (f1(x1,x2)) # > 25.2134
1
4print  (grad_f1(x1,x2)) # > 3.2769
F IGURE  5.22: Python code- AD of the function g  ( x  1  , x  2  )
5.3.16   Symbolic dif ferentiation
5.3.17   Simple dif ferentiation
SOL-132  
  CH.SOL- 5.33.
The answers ar e as follows:
1
.Appr oximate m ethods such as numerical differ entiation suf fer fr om
numerical instability and truncation err ors  .
2
.In symbo lic differ entiation, a symbolic expr ession for the derivative of a
function is calculated. This appr oach is quite slow and r equir es  symbols
parsing and manipulation  . For example, the number  
  is r epr esented
in S ymPy as t he object Pow(2,1/2). Since SymPy employees exact
r epr esentations Pow(2,1/2)*Pow(2,1/2) will always equal 2  .
SOL-133  
  CH.SOL- 5.34.
1
.First:
1 import  sympy
2 sympy .  init_printing()
3 fr om  sympy  import  Symbol
4 fr om  sympy  import  dif f, exp, sin, sqrt
5 y =  Symbol( 'y'  )
6 y =  sympy .  Symbol( "y"  )7 sigmoid = 1/  ( 1+  sympy .  exp( -  y))
F IGURE  5.23: Sigmoid in SymPy
2
.Second:
1 sig_der=sym .  dif f(sigmoid, y)
F IGURE  5.24: Sigmoid gradient in SymPy
3
.Thir d:
1 sig_der .  evalf(subs={y: 0  })
2 > 0.25
F IGURE  5.25: Sigmoid gradient in SymPy
4
.The plot is depicted in  5.26  .
1 p = sym .  plot(sig_der);
F IGURE  5.26: SymPy gradient of the Sigmoid() function5.3.18   The Beta-Binomial model
SOL-134  
  CH.SOL- 5.35.
T o corr e ctly r end er the generated LaT eX in this pr oblem, we import and
configur e several libraries as depicted in  5.27  .
1 import  numpy  as  np
2 import  scipy .stats  as  st
3 import  matplotlib.pyplot  as  plt
4 import  sympy  as  sp
5 sp .  interactive .  printing  .
6 init_printing(use_latex =  T rue  )
7 fr om  IPython.display  import  display , Math, Latex
8 maths =  lambda  s: display(Math(s))
9 latex =  lambda  s: display(Latex(s))
F IGURE  5.27: SymPy imports
1
.The Lik elihood function can be cr eated as follows. Note the specific
details of generating the Factorial function in SymPy  .
1 n =  sp .  Symbol( 'n'  , integer= T rue  , positive= T rue  )
2 r =  sp .  Symbol( 'r'  , integer= T rue  , positive= T rue  )
3 theta =  sp .  Symbol( 'theta'  )
4 # Cr eate the function symbolically
5 fr om  sympy  import  factorial
6 cNkSym =  (factorial(n)) /  (factorial(r) *  factorial(n -  r))
7 cNkSym .  evalf()
8 binomSym =  cNkSym *  ((theta **  r) *  (1 -  theta) **  (n -  r))
9 binomSym .  evalf()
1
0#Convert it to a Numpy-callable function
1
1binomLambda =  sp .  Lambda((theta,r ,n), binomSym)
1
2maths( r"\operatorname  {Bin}  (r|n,\theta) = "  )
1
3display (binomLambda .  expr)
1
4#Evaluating the SymPy version r esults in:1
5>  binomSym .  subs({theta: 0.5  ,r: 50  ,n: 100  })
1
6#Evaluating the pur e Numpy version r esults in:
1
7>  binomLambda( 0.5  , 50  , 100  ) = 0.07958923
F IGURE  5.28: Likelihood function using SymPy
The Symbolic r epr esentation r esults in the following LaT eX:
2
.The Beta distribution can be cr eated as follows  .
1 a =  sp .  Symbol( 'a'  , integer= False  , positive= T rue  )
2 b =  sp .  Symbol( 'b'  , integer= False  , positive= T rue  )
3 #mu = sp.Symbol('mu', integer=False, positive=T rue)
4 # Cr eate the function symbolically
5 G =  sp .  gamma
6 # The normalisation factor
7 BetaNormSym =  G(a +  b) /  (G(a) *  G(b))
8 # The functional form
9 BetaFSym =  theta **  (a -1  ) *  ( 1-  theta) **  (b -1  )
1
0BetaSym =  BetaNormSym *  BetaFSym
1
1BetaSym .  evalf() # this works
1
2# T urn Beta into a function
1
3BetaLambda =  sp .  Lambda((theta,a,b), BetaNormSym *  BetaFSym)
1
4maths( r"\operatorname  {Beta}  (\theta|a,b) = "  )
1
5display(BetaSym)
1
6#Evaluating the SymPy version r esults in:
1
7>  BetaLambda( 0.5  , 2  , 7  )= 0.4375
1
8#Evaluating the pur e Numpy version r esults in:
1
9>  BetaSym .  subs({theta: 0.5  ,a: 2  ,b: 7  })= 0.4375F IGURE  5.29: Beta distribution using SymPy
The r esult is:
3
.The plot is depicted in  5.30  .
1 %  pylab inline
2 mus =  arange( 0  , 1  , .01  )
3 # Plot for various values of a and b
4 for  ab in  [( .1  , .1  ),( .5  , .5  ),( 2  , 20  ),( 2  , 3  ), ( 1  , 1  )]:
5 plot(mus, vectorize(BetaLambda)(mus, *  ab), label= "a=  %s  b=  %s  "  %  ab)
6 legend(loc =0  )
7 xlabel( r"$\theta$"  , size =22  )
F IGURE  5.30: A plot of the Beta distribution
4
.W e can find the  posterior distribution by multiplying our Beta prior by
the Binomial Likelihood  .
1 a =  sp .  Symbol( 'a'  , integer= False  , positive= T rue  )
2 b =  sp .  Symbol( 'b'  , integer= False  , positive= T rue  )
3 BetaBinSym =  BetaSym *  binomSym
4 # T urn Beta-bin into a function5 BetaBinLambda =  sp .  Lambda((theta,a,b,n,r), BetaBinSym)
6 BetaBinSym =  BetaBinSym .  powsimp()
7 display(BetaBinSym)
8 maths( r"\operatorname  {Beta}  (\theta|a,b) \times
  
       \operatorname  {Bin}  (r|n,\theta) \propto  %s  "  %
  
       sp .  latex(BetaBinSym))
9 >  BetaBinSym .  subs({theta: 0.5  ,a: 2  ,b: 7  ,n: 10  ,r: 3  }) = 0.051269
1
0>  BetaBinLambda ( 0.5  , 2  , 7  , 10  , 3  ) = 0.051269
F IGURE  5.31: A plot of the Beta distribution
The r esult is:
So the posterior  distribution has the same functional dependence on θ
as the prior , it is just another Beta distribution  .
5
.Mathematically , the r elationship is as follows:
1 prior =  BetaLambda(theta, 2  , 7  )
2 maths( "\mathbf  {Prior}  :\operatorname  {Beta}  (  \t  heta|a=2,b=7) =  %s  "  %
  
       sp .  latex(prior))
3 likelihood =  binomLambda(theta, 3  , 50  ) # = binomLambda(0.5,3,10)
4 maths( "\mathbf  {Likelihood}  : \operatorname  {Bin}  (r=3|n=6,  \t  heta) =
  
       %s  "  %  sp .  latex(likelihood))
5 posterior =  prior *  likelihood
6 posterior =  posterior .  powsimp()
7 maths( r"\mathbf{Posterior
  
       (normalised)}:\operatorname  {Beta}  (\theta|2,7) \times  
       \operatorname  {Bin}  (3|50,\theta)=  %s  "
8 posterior .  subs({theta: 0.5  })
9 plt .  plot(mus, (sp .  lambdify(theta,posterior))(mus), 'r'  )
1
0xlabel( "$  \\  theta$"  , size =22  )
F IGURE  5.32: A plot of the Posterior with the provided data samples  .
Ref e r e n c e s
[  1
]J. Bradbury et al. JAX: composable transformations of NumPy
pr ograms  . 2018 (cit. on pp. 123  , 136  ).
[  2
]W . K. Clif ford. ‘Preliminary Sketch of Bi-quaternions’. In: Pr oceedings
of the London Mathematical Society 4  (1873), pp. 381–95 (cit. on pp.
125  , 138  ).
[  3
]R. Frostig et al. JAX: Autograd and XLA  . 2018 (cit. on p. 123  ).
[  4
]A. Griewank, D. Juedes and J. Utke. ‘Algorithm 755; ADOL-C: a
package for the automatic dif ferentiation of algorithms written in
C/C++’. In: ACM T ransactions on Mathematical Softwar e  22.2 (June
1996), pp. 131–167 (cit. on pp. 123  , 125  ).
[  5
]A. Griew ank and A. W alther . Evaluating Derivatives: Principles and
T echniques of Algorithmic Differ entiation  . Second. USA: Society for
Industrial and Applied Mathematics, 2008 (cit. on pp. 123  , 124  ).[  6
]K. Gurney . An Intr oduction to Neural Networks  . 1 Gunpowder Square,
London EC4A 3DE, UK: UCL Press, 1998 (cit. on p. 135  ).
[  7
]L. V . Kantorovich. ‘On a mathematical symbolism convenient for
performing machine calculations’. In: Dokl. A kad. Na uk SSSR  . V ol.
1 13. 4. 1957, pp. 738–741 (cit. on p. 123  ).
[  8
]G. Kedem. ‘Automatic dif ferentiation of computer programs’. In: ACM
T ransactions on Mathematical Softwar e (T OMS)  6.2 (198 0), pp. 150–
165 (cit. on pp. 126  , 149  ).
[  9
]S. Laue. On the Equivalence of Forwar d Mode Automatic
Differ entiation and Symbolic Differ entiation  . 2019. arXiv: 1904.02990
[cs.SC]  (cit. on p. 124  ).
[  10
]D. M aclaurin, D. Duvenaud and R. P . Adams. ‘Autograd: Ef fortless
gradients in numpy’. In: ICML 2015 AutoML W orkshop  . V ol. 238.
2015 (cit. on pp. 123  , 136  ).
[  1 1
]A. Pasz ke et al. ‘Automatic dif ferentiation in PyT orch’. In: ( 2017)
(cit. on p. 136  ).
[  12
]D. Rumelhart, G. Hinton and R. W illiams. ‘Learning representations
by back propagating errors’. In: Natur e 323  (1986), pp. 533–536 (cit.
on p. 136  ).
[  13
]B. Speelpenning. Compiling fast partial derivatives of functions given
by algorithms  . T ech. rep. Illinois Univ Urbana Dept of Computer
Science, 1980 (cit. on p. 126  ).P A R T  IV
B A CHELORSC HAPTER
6   DEEP LEARNING: NN ENSEMBLES
The saddest aspect of life right now is that gathers knowledge
faster than society gathers wisdom  .
— Isaac Asimov
C o n t e n t s
Intr oduction
Pr oblems
Bagging, Boosting and Stacking
Approaches for Combining Predictors
Monolithic and Heterogeneous Ensembling
Ensemble Learning
Snapshot Ensembling
Multi-model Ensembling
Learning-rate Schedules in Ensembling
Solutions
Bagging, Boosting and Stacking
Approaches for Combining Predictors
Monolithic and Heterogeneous Ensembling
Ensemble Learning
Snapshot Ensembling
Multi-model Ensembling
Learning-rate Schedules in Ensembling6 . 1   I n t r o d u c t i o n
Ntuition and practice demonstrate that a poor or an inferior choice
may be altogeth er prevented merely by motivating a group (or an
ensemble) of people with diverse perspectives to make a mutually
acceptable cho ice. Likewise, in many cases, neural network
ensembles significantly improve the generalization ability of single-model based
AI systems [  5  , 1 1  ]. Sh ortly following the foundat ion of Kaggle, research in the
field ha d started blooming; not only because researchers are advocating and
using advanced ensembling approaches in almost every competition, but also by
the empirical  success of the top winning mod els. Though the whole process of
training ensemb les typically involves the utilization of doze ns of GPUs and
prolonged training periods, ensembling approaches enhance the predictive power
of a single model. Though ensembling obviously has a significant impact on the
performance of AI systems in general, research shows its ef fe ct is particularly
dramatic in the field of neural networks [ Russakovsky_2015  , 1  , 4  , 7  , 13  ].
Therefore, whil e we could examine combinations of any type of learning
algorithms, the focus of this chapter is the combination of neural networks.
6 . 2   P r o b l e m s
6.2.1   Bagging, Boosting and Stacking
PRB-135  
  CH.PRB- 6.1.
Mark all the ap pr oaches which can be utilized to boost a single model
performance:
(i) Majority V oting
(ii) Using K-identical base-learning algorithms
(iii) Using K-differ ent base-learning algorithms
(iv) Using K-differ ent data-folds
(v) Using K-differ ent random number seeds(vi) A combination of all the above appr oaches
PRB-136  
  CH.PRB- 6.2.
An ar gument erupts between two senior data-scientists r egar ding the
choice of an ap pr oach for training of a very small medical corpus. One
suggest that bagging is superior while the other suggests stacking. Which
technique, bagg ing or stacking, in your opinion is  superior  ? Ex plain in det ail
.
(i)  Stacking since each classier is trained on all of the available data  .
(ii)  Bagging since we can combine as many classifiers as we want by
training each on a differ ent sub-set of the training corpus  .
PRB-137  
  CH.PRB- 6.3.
Complete the sentence: A rando m for est is a type of a decision tr ee which
utilizes  [bagging/boosting]
PRB-138  
  CH.PRB- 6.4.
The algorithm depicted in Fig.  6.1  was found in a n old book ab out
ensembling. Name the algorithm  .
F IGURE  6.1: A specific ensembling approachPRB-139  
  CH.PRB- 6.5.
Fig.  6.2  depicts a part o f a specific ensembling appr oach applied to th e
models x  1  , x  2  ...x  k  . In your opinion, which appr oach is being utilized?
F IGURE  6.2: A specific ensembling approach
(i) Bootstrap aggr egation
(ii) Snapshot ensembling
(iii) Stacking
(iv) Classical committee machines
PRB-140  
  CH.PRB- 6.6.
Consider trainin g corpus consisting of balls which ar e glued t ogether as
triangles, each of which has either 1, 3, 6, 10, 15, 21, 28, 36, or 45 balls  .
1
.W e draw  several samples fr om th is corpus as pr esented in Fig.  6.3  wher ein
each sample is equipr obable. What type of sampling appr oach is being
utilized her e?
F IGURE  6.3: Sampling approaches
(i) Sampling without r eplacement
(ii) Sampling with r eplacement2
.T wo sam ples ar e  drawn one after the other . In which of the following cases
is the covariance between the two samples equals zer o?
(i) Sampling without r eplacement
(ii) Sampling with r eplacement
3
.During training, the corpus sampled with r eplacement and is divided into
several folds as pr esented in Fig.  6.4.
F IGURE  6.4: Sampling approaches
If 10  balls glued together is a sample event that we know is har d to
corr ectly classify , then it is impossible that we ar e using:
(i) Bagging
(ii) Boosting
6.2.2   Approaches for Combining Predictors
PRB-141  
  CH.PRB- 6.7.
Ther e ar e several methods by which the outputs of base classifiers can be
combined to yield a single pr ediction. Fig.  6.5  depicts part of a specific
ensembling appr oach applied to several CNN model pr edictions  for a labelled
data-set. Which appr oach is being utilized?
(i)   Majority voting for binary classification
(ii)  W eighted majority voting for binary classification
(iii)  Majority voting for class pr obabilities(iv)  W eighted majority class pr obabilities
(v)   An algebraic weighted average for class pr obabilities
(vi)  An adaptive weighted majority voting for combining multiple
classifiers
1 l =  []
2 for  i,f in  enumerate  (filelist):
3 temp =  pd .  read_csv(f)
4 l .  append(temp)
5 arr =  np .  stack(l,axis =-1  )
6 avg_results =  pd .  DataFrame(arr[:,: -1  ,:].mean(axis =2  ))
7 avg_results[ 'image'  ] =  l[ 0  ][ 'image'  ]
8 avg_results .  columns =  l[ 0  ] .  columns
F IGURE  6.5: PyT orch code snippet for an ensemble
PRB-142  
  CH.PRB- 6.8.
Read the paper  Neural Network Ensembles  [  3  ] and then  complete the
sentence  : I f the average err or rate for a specific instance in the corpus is  less
than [...]% and the r espective classifiers in the ensemble pr oduce independent
[...], then when the number of classifiers combined appr oach es infinity , the
expected err or can be diminished to zer o  .
PRB-143  
  CH.PRB- 6.9.
T rue or false:  A perfect ensemble comprises of highly corr ect classifiers
that differ as much as possible  .
PRB-144  
  CH.PRB- 6.10.
T rue or false:  In bagging, we r e-sample the training corpus with
r eplacement and ther efor e this may lead to some instances being r epr esented
numer ous times while other instances not to be r epr esented at all  .
6.2.3   Monolithic and Heterogeneous EnsemblingPRB-145  
  CH.PRB- 6.1 1.
1
.T rue or false:  T raining an ensemble of a single monolithic ar chitectur e
r esults i n lower model diversity and possibly decr eased model pr ediction
accuracy  .
2
.T rue or false:  The gen eralization accuracy of an ensemble incr eases with
the number of well-trained models it consists of  .
3
.T rue or false:  Bootstrap aggr egation (or bagging), r efers to a pr ocess
wher ein a CNN ensemble is being trained using a random subset of the
training corpus  .
4
.T rue or false:  Bagging assum es that if the single pr edictors have
independent errors  , the n a majority  vote of their outputs should be better
than the individual pr edictions  .
PRB-146  
  CH.PRB- 6.12.
Refer to the papers:  Dropout as a Bayesian Approximation  [  2  ] and  Can
Y ou T rust Y our Models Uncertainty?  [  12  ] and answer the following
question: Do deep ensembles achieve a better performance on out-of-
distribution uncertainty benchmarks compar ed with Monte-Carlo (MC)-
dr opout?
PRB-147  
  CH.PRB- 6.13.
1
.In a transfer -lea rning experime nt conducted by a r esear cher , a number of
ImageNet-pr etrained CNN classifiers, selected fr om T able  6.1  ar e trained
on five differ ent folds drawn fr om the same corpus. Their outputs ar e fused
together pr oducing a composite machine. Ensembles of these
convolutional ne ural networks ar chitectur es have been extensiv ely studies
an evaluated in various ensembling appr oaches [  4  ,  9  ]. Is it likely that the
composite machine will pr oduce a pr ediction with higher accuracy than
that of any individual classifier? Explain why  .
CNN Model Classes Image SizeT op-1
accuracyResNet152 1000 224 78.428
DPN98 1000 224 79.224
SeNet154 1000 224 81.304
SeResneXT10
11000 224 80.236
DenseNet161 1000 224 77.560
InceptionV4 1000 299 80.062
T ABLE  6.1: Imag eNet-pretrained CNNs. Ensembles of these CNN architectures have been
extensively studies and evaluated in various ensembling approaches  .
2
.T rue or False  : In a classification task, the r esult of ensembling is always
superior  .
3
.T rue or False  : In an ensemble, we want differ ently trained models
conver ge to differ ent local minima  .
PRB-148  
  CH.PRB- 6.14.
In committee machines, mark a ll the combiners that do not make dir ect
use of the input:
(i) A mixtur e of experts
(ii) Bagging
(iii) Ensemble averaging
(iv) Boosting
PRB-149  
  CH.PRB- 6.15.
T rue or False  : Considering a binary classification pr oblem (y  = 0 or y  =
1 ), ensemble averaging, wher ein the outputs of individual models ar e linearly
combined to pr oduce a fused output is a form of a  static  committee machine  .F IGURE  6.6: A typical binary classification problem  .
PRB-150  
  CH.PRB- 6.16.
T rue or false:  When using a single model, the risk of overfitting the data
incr eases when the number of adjustable parameters is lar ge compar ed to
car dinality (i.e., size of the set) of the training corpus  .
PRB-151  
  CH.PRB- 6.17.
T rue or false:  If we have a com mittee of K trained models and the err ors
ar e uncorr elated, then by averaging them the average err or of a model is
r educed by a factor of K  .
6.2.4   Ensemble Learning
PRB-152  
  CH.PRB- 6.18.
1
.Define ensemble learning in the context of machine learning  .
2
.Pr ovide examples of ensemble methods in classical machine-learning  .3
.T rue or false:  Ensemble metho ds usually have str onger generalization
ability  .
4
.Complete the sentence: Bagging is  variance/bias  r eduction scheme while
boosting r educed  variance/bias  .
6.2.5   Snapshot Ensembling
PRB-153  
  CH.PRB- 6.19.
Y our colleague, a well-known expert in ensembling methods, writes the
following pseudo  code in Python  shown in Fig.  6.7  for t he training of a neural
network. This runs inside a standar d loop in each training and validation step
.
1 import  tor chvision.models  as  models
2 ...
3 models =  [ 'resnext'  ]
4 for  m in  models:
5 train ...
6 compute V AL loss ...
7 amend LR ...
8 if  (val_acc > 90.0  ):
9 saveModel()
F IGURE  6.7: PyT orch code snippet for an ensemble
1
.What typ e of ensembling can be used with this appr oach? Explain in detail
.
2
.What is  the main advantage of snapshot ensembling? What ar e the
disadvantages, if any?
PRB-154  
  CH.PRB- 6.20.
Assume further that your colleag ue amends the code as follows in Fig.  6.8
.
1 import  tor chvision.models  as  models2 import  random
3 import  np
4 ...
5 models =  [ 'resnext'  ]
6 for  m in  models:
7 train ...
8 compute loss ...
9 amend LR ...
1
0manualSeed =  draw a new random number
1
1random .  seed(manualSeed)
1
2np .  random .  seed(manualSeed)
1
3torch .  manual_seed(manualSeed)
1
4if  (val_acc > 90.0  ):
1
5saveModel()
F IGURE  6.8: PyT orch code snippet for an ensemble
Explain in detail what would be the possible effects of adding lines 10-13  .
6.2.6   Multi-model Ensembling
PRB-155  
  CH.PRB- 6.21.
1
.Assume your colleague, a veteran in DL and an expert in ensembling
methods writes the following Pseudo code shown in Fig.  6.9  for the
training of several neural netwo rks. This code snippet is execut ed inside a
standar d loop in each and every training/validation epoch  .
1 import  tor chvision.models  as  models
2 ...
3 models =  [ 'resnext'  , 'vgg'  , 'dense'  ]
4 for  m in  models:
5 train ...
6 compute loss/acc ...7 if  (val_acc > 90.0  ):
8 saveModel()
F IGURE  6.9: PyT orch code snippet for an ensemble
What type of ensembling is bein g utilized in this appr oach? Ex plain in
detail  .
2
.Name on e metho d by which NN models may be combined to yield a single
pr ediction  .
6.2.7   Learning-rate Schedules in Ensembling
PRB-156  
  CH.PRB- 6.22.
1
.Referring to Fig. (  6.10  ) w hich depicts a specific learning rate schedule,
describe the basic notion behind its mechanism  .
F IGURE  6.10: A learning rate schedule  .
2
.Explain how cyclic learning rates [  10  ] ca n be  effective for the training of
convolutional ne ural networks such as the ones in the code snip pet of Fig.
6.10  .3
.Explain how a cyclic cosine annealing schedule as pr oposed by Loshchilov
[  10  ] and [  13  ] is used to conver ge to multiple local minima  .
6 . 3   S o l u t i o n s
6.3.1   Bagging, Boosting and Stacking
SOL-135  
  CH.SOL- 6.1.
All the pr esented options ar e corr ect  .
SOL-136  
  CH.SOL- 6.2.
The corr ect cho ice would be sta cking. In cases wher e the given  corpus is
small, we would most likely pr efer training our models on the  full  data-set  .
SOL-137  
  CH.SOL- 6.3.
A random for est is a type of a decision tr ee which utilizes bagging  .
SOL-138  
  CH.SOL- 6.4.
The pr esented algorithm is a classic bagging  .
SOL-139  
  CH.SOL- 6.5.
The app r oach w hich is depicted is the  first phase  of stacking. In stacking,
we first (phase 0) pr edict using several base learners and then use a
generalizer (phase 1) that learns on top of the base learners pr edictions  .
SOL-140  
  CH.SOL- 6.6.
1
.Sampling with r eplacement2
.Sampling without r eplacement
3
.This may be mostly a r esult of bagging, since in boosting we w ould have
expected miss-corr ectly classified observations to r epeatedly appear in
subsequent samples  .
6.3.2   Approaches for Combining Predictors
SOL-141  
  CH.SOL- 6.7.
An Algebraic weighted average for class pr obabilities  .
SOL-142  
  CH.SOL- 6.8.
This is true, [  3  ] pr ovides a mathematical pr oof  .
SOL-143  
  CH.SOL- 6.9.
This is true. For extension, see instance [  8  ]  .
SOL-144  
  CH.SOL- 6.10.
This is true. In a bagging a ppr oach, we first randomly draw (with
r eplacement), K examples  wher e K is the size of the original training corpus
ther efor e leading to an imbalanced r epr esentation of the instances  .
6.3.3   Monolithic and Heterogeneous Ensembling
SOL-145  
  CH.SOL- 6.1 1.
1
.T rue  Due to  their lack of diversity , an ensemble of monoli thic
ar chitectur es tends to perform worse than an heter ogeneous ensemble  .
2
.T rue  This has be consistently demonstrated in [  1 1  ,  5  ]  .3
.T rue  In [  6  ] the r e i s a discussion about both using the whole corpus and a
subset much like in bagging  .
4
.T rue  The tota l err or decr eases with the addition of pr edictors to the
ensemble  .
SOL-146  
  CH.SOL- 6.12.
Y es, they do  .
SOL-147  
  CH.SOL- 6.13.
1
.Y es, it is very likely , especially if their err ors ar e independent  .
2
.T rue  It m ay b e pr ove n that ensembles of models perform at least as good
as each of the ensemble members it consists of  .
3
.T rue  Differ ent local minima add to the diversification of the models  .
SOL-148  
  CH.SOL- 6.14.
Boosting is the only one that does not  .
SOL-149  
  CH.SOL- 6.15.
False  By d efinition, static committee m achines use  only  the output of the
single pr edictors  .
SOL-150  
  CH.SOL- 6.16.
T rue
SOL-151  
  CH.SOL- 6.17.False  Though this may be theor etically true, in practice the err ors ar e
rar ely uncorr elated and ther efor e the actual err or can not be r educed by a
factor of K  .
6.3.4   Ensemble Learning
SOL-152  
  CH.SOL- 6.18.
1
.Ensemble learning is an excellent machine learning idea which displays
noticeable benefits in many applications, one such notable example is the
widespr ead use of ensembles in Kaggle competitions. In an ensemble
several individual models (for instance ResNet18 and VGG16) which wer e
trained on the s ame corpus, work in tandem and during infer ence, their
pr edictions ar e fused by a pr e-defined strategy to yield a single pr ediction  .
2
.In classi cal machine learning Ensemble methods usually r efer to bagging,
boosting and the linear combina tion of r egr ession or classification models
.
3
.T rue  The str onger generalization abi lity stems fr om the voting power of
diverse models which ar e joined together  .
4
.Bagging is  variance  r eduction scheme while boosting r educed  bias  .
6.3.5   Snapshot Ensembling
SOL-153  
  CH.SOL- 6.19.
1
.Since only a si ngle model ie b eing utilized, this type of ens embling is
known a s snapshot ensembling. Using this appr oach, during the training
of a  neural network and in each epoch, a snapshot, e.g. the w eights of a
trained instance of a model (a PTH file in PyT or ch nomenclatur e) ar e
persisted into permanent storage whenever a certain performance metrics,
such as  accuracy or loss is  being surpassed. Ther efor e the name
“snapshot”; we ights of the neural network ar e being snapshot at specificinstances in time. After several such epochs the top-5 p erforming
Snapshots which conver ged to local minima [  4  ] a r e combined as part of
an ensemble to yield a single pr ediction  .
2
.Advantages: dur ing a single training cycle, many model instances may be
collected. Disadvantages: inher ent lack of diversity by virtue of the fact
that the same models is being r epeatedly used  .
SOL-154  
  CH.SOL- 6.20.
Changing the random seed at e ach iteration/epoch, helps in intr oducing
variation which may contribut e to diversifying the trained neural network
models  .
6.3.6   Multi-model Ensembling
SOL-155  
  CH.SOL- 6.21.
1
.Multi-model ensembling  .
2
.Both averaging and majority voting  .
6.3.7   Learning-rate Schedules in Ensembling
SOL-156  
  CH.SOL- 6.22.
1
.Capturing the best model of each training cycle allows to obtain multiple
models settled on various local optima fr om cycle to cycle at the cost of
training a single mode
2
.The app r oach is based on the non-convex natur e of neural networks and
the ability to co nver ge and escape fr om local minima using a specific
schedule to adjust the learning rate during training  .3
.Instead of monotonically decr ea sing the learning rate, this meth od lets the
learning rate cyclically vary between r easonable boundary values  .
Ref e r e n c e s
[  1
]B. Chu et al. ‘Best Practices for Fine-T uning V isual Classifiers to Ne
Domains’. In: Computer V ision – ECCV 2016 W orkshops  . E d. by G. Hua an
H. J égou. Cham: Springer International Publishing, 2016, pp. 435–442 (c
on p. 184  ).
[  2
]Y . Gal and Z. G hahramani. ‘Dro pout as a Bayesian approximation’. In: arX
pr eprint arXiv:1506.02157  (2015) (cit. on p. 190  ).
[  3
]L. K . Hansen an d P . Salamon. ‘Neural Network Ensembles’. In: IEEE T ran
Pattern Anal. Mach. Intell  . 12 (1990), pp. 993–1001 (cit. on pp. 189  , 197  )
[  4
]G. Huang et al. ‘Snapshot ensembles: T rain 1, get M for free. arXiv 2017’. I
arXiv pr eprint arXiv:1704.00109  () (cit. on pp. 184  , 190  , 200  ).
[  5
]J. H uggins, T . C ampbell and T . Broderick. ‘Coresets for scalable Bayesia
logistic regressio n’. In: Advances in Neural Information  Pr ocessing Systems
2016, pp. 4080– 4088 (cit. on pp. 184  , 198  ).
[  6
]C. Ju, A. Bibaut and M. van der  Laan. ‘The relative performance of ensemb
methods with deep convolutional neural networks for image classification
In: Journal of Applied Statistics  45.15 (2018), pp. 2800–2818 (cit. on p. 19
).
[  7
]S. Kornblith, J. Shlens and Q. V . Le. Do Better ImageNet Models T ransf
Better?  2018. arXiv: 1805.08974 [cs.CV]  (cit. on p. 184  ).
[  8
]A. K rogh and J. V edelsby . ‘Neural Network Ensembles, Cross V alidation, an
Active Learning’. In: NIPS  . 1994 (cit. on p. 197  ).
[  9
]S. L ee et al. ‘Stochastic multiple choice learning for training  diverse dee
ensembles’. In: Advances in Neural Information Pr ocessing Systems  . 201
pp. 21 19– 2127 (cit. on p. 190  ).
[  10
]I. Loshc hilov and F . Hutter . ‘Sg dr: Stochastic gradient descent with warm
restarts’. In: arXiv pr eprint arXiv:1608.03983  (2016) (cit. on p. 196  ).
[  1 1
]P . Oshir o et al.(2012)Oshiro and Baranauskas. ‘How many trees in a
random forest?’ In: International W o rkshop on Machine Learning and DataMining in Pattern Recognition  . 2012 (cit. on pp. 184  , 198  ).
[  12
]Y . Ovad ia et al. ‘Can you trust your model’ s uncertainty?  Evaluating
predictive unce rtainty under dataset shift’. In: Advances in Neura
Information Pr ocessing Systems  . 2019, p. 13991 (cit. on p. 190  ).
[  13
]L. N. Smith. ‘Cyclical learning rates for training neural networks’. In: 201
IEEE W inter Confer ence on Applications of Computer V ision (W ACV)  
IEEE. 2017, pp. 464– 472 (cit. on pp. 184  , 196  ).C HAPTER
7   DEEP LEARNING: CNN FEA TURE EXTRA CTION
What goes up must come down  .
— Isaac Newton
C o n t e n t s
Intr oduction
Pr oblems
CNN as Fixed Feature Extractor
Fine-tuning CNNs
Neural style transfer , NST
Solutions
CNN as Fixed Feature Extractor
Fine-tuning CNNs
Neural style transfer
7 . 1   I n t r o d u c t i o n
HE extraction of an n-dimens ional feature vector (FV) or an
embedding  from one (or more) layers of a pre-trained CNN, is
termed featur e extraction  (FE). U sually , FE works by first
removing the last fully connecte d (FC) layer from a CNN and then
treating the remaining layers of the CNN as a fixed FE. As exemplified in Fig.
( 7.1  ) an d Fi g. ( 7.2  ), applying this method to the ResNet34 architecture, the
resulting FV consists of 512 f loating point values. Likewise , applying thesame logic on th e ResNet152 architecture, the resulting FV has  2048 floating
point elements.
F IGURE  7.1: A one-dimensio nal 512-element embedding for a single image from the Res Net34
architecture. While any neural network can be used for FE, depicted is the ResNet CNN
architecture with 34 layers  .
1 import  tor chvision.models  as  models
2 ...
3 res_model =  models .  resnet34(pretrained =  T rue  )
F IGURE  7.2: PyT orch decleration for a pre-trained ResNet34 CNN (simplified)  .
The prem ise beh ind FE is that CNNs which were originally trai ned on the
ImageNet Lar ge Scale V isual Recognition Competition [  7  ], can be adapted
and used (for instance in a classification task) on a compl etely dif ferent
(tar get) domain without any additional training  of the CNN layers. The
power o f a CNN to do so lies in its ability to generalize well beyond the
original data-se t it was trained  on, therefore FE on a new tar get data-set
involves no training and requires only inference.
7 . 2   P r o b l e m s
7.2.1   CNN as Fixed Feature Extractor
Before a ttempting the problems in this chapter you are highly encouraged to
read the following papers [  1  , 3  , 7  ]. In many DL job interviews, you will be
presented with a paper you have never seen before and subsequently be askedquestions about  it; so reading these references would be an excellent
simulation of this real-life task.
PRB-157  
  CH.PRB- 7.1.
T rue or False  : Wh ile AlexNet [  4  ] used  1 1 ×  1 1 sized filters, the main
novelty pr esented in the VGG [  8  ] ar chitectur e was utilizing filters with
much smaller spatial extent, sized  3 ×  3.
PRB-158  
  CH.PRB- 7.2.
T rue or False  : Unlike CNN ar chitectur es such as AlexNet or VGG,
ResNet  does not  have any hidden FC layers  .
PRB-159  
  CH.PRB- 7.3.
Assuming the VGG-Net has  138, 357 , 544 floating point parameters,
what is the physical size in M ega-Bytes (MB) r equir ed for persisting a
trained instance of VGG-Net on permanent storage?
PRB-160  
  CH.PRB- 7.4.
T rue or False  : Most attempts at r esear ching image r epr esentation
using FE, focused solely on r e using the activations obtained fr om layers
close to the output of the CNN, and mor e specifically the full y-connected
layers  .
PRB-161  
  CH.PRB- 7.5.
T rue or False  : FE in the context of deep learning is particularly useful
when the tar get pr oblem does not include enough labeled data to
successfully train CNN that generalizes well  .
PRB-162  
  CH.PRB- 7.6.
Why is a CNN t rained on the Im ageNet dataset [  7  ] a good  candid ate
for a sour ce pr oblem?PRB-163  
  CH.PRB- 7.7.
Complete the missing parts r egar ding the VGG19 CNN ar chitectur e:
1
.The VGG19 CNN consists of [...] layers  .
2
.It consists of [...] convolutional and 3 [...] layers  .
3
.The input image size is [...]  .
4
.The number of input channels is [...]  .
5
.Every image has its mean RGB value [subtracted / added]  .
6
.Each convolutional layer has a [small/lar ge] kernel sized [...]  .
7
.The number of pixels for padding and stride is [...]  .
8
.Ther e ar e 5 [...]  layers having a  kernel size of [...] and a stride of [...]
pixels  .
9
.For non-linearity a [r ectified linear unit (ReLU [  5  ])/sigmoid] is used  .
10  . The [...] FC layers ar e part of the linear classifier  .
1 1  . The first two FC layers consist of [...] featur es  .
12  . The last FC layer has only [...] featur es  .
13  . The last FC layer is terminated by a [...] activation layer  .
14  . Dr opout [is / is not] being used between the FC layers  .PRB-164  
  CH.PRB- 7.8.
The follo wing q uestion discusse s the method of fixed featur e extraction
fr om layers of the VGG19 ar chitectur e [  8  ] for the classification of
pancr eatic cance r . It depicts FE principles which ar e applicable with minor
modifications to  other CNNs as well. Ther efor e, if you happen to encounter
a sim ilar question in a job inter view , you ar e likely be able to cope with it
by utilizing the same logic. In Fig. (  9.7  ) thr ee differ ent classes of
pancr eatic cancer ar e displayed: A, B and C, curated fr om a dataset of  4 K
Whole Slide Images (WSI) labeled by a boar d certified pathologist. Y our
task is to use FE to corr ectly classify the images in the dataset  .
F IGURE  7.3: A dataset of 4K histopathology WSI from three severity classes: A, B and C  .
T able (  9.3  ) pr esen ts an incomplete listing of the of the VGG19
ar chitectur e [  8  ]. A s depicted, for each layer the number of filters (i. e.,
neur ons with unique set of parameters), learnable parameters (weights,
biases), and FV size ar e pr esented  .
Layer name #Filters#Parameter
s# Features
conv4_3 512 2.3M 512
fc6 4,096 103M 4,096
fc7 4,096 17M 4,096
output 1,000 4M -
T otal 13,416 138M 12,416
T ABLE  7.1: Incomplete listing of the VGG19 architecture1
.Describe how the VGG19 CN N may be used as fixed FE for a
classification task. In your answer be as detailed as possible r egar ding
the stages of FE and the method used for classification  .
2
.Referring to T able (  9.3  ), suggest  three  differ ent ways in which featur es
can be extracted fr om a trained VGG19 CNN model. In each case, state
the extracted featur e layer name and the size of the r esulting FE  .
3
.After su ccessfully extracting th e featur es for the  4 K images fr om the
dataset, how can you now  classify  the images into their r espective
categories?
PRB-165  
  CH.PRB- 7.9.
Still r eferring to T able (  9.3  ), a data scientist suggests using the output
layer of the VGG19 CNN as a fixed FE. What is the main advantage of
using this layer over using for instance, the fc  7 layer? (Hint: think about
an ensemble of featur e extractors)
PRB-166  
  CH.PRB- 7.10.
Still r eferring to T able (  9.3  ) a nd also to the code snippet in Fig. (  7.4  ),
which r epr esents a new CNN derived fr om the VGG19 CNN:
1 import  tor chvision.models  as  models
2 ...
3 class  VGG19FE  (torch .  nn .  Module):
4 def  __init__  ( self  ):
5 super  (VGG19FE, self  ) .  __init__  ()
6 original_model =  models .  VGG19(pretrained=[ ???  ])
7 self  .  real_name =  ((( type  (original_model) .  __name__  )))
8 self  .  real_name =  "vgg19"
9
1
0self  .  features =  [ ???  ]
1
1self  .  classifier =  torch .  nn .  Sequential([ ???  ])
1
2self  .  num_feats =  [ ???  ]1
3
1
4def  forward  ( self  , x):
1
5f =  self  .  features(x)
1
6f =  f .  view(f.size( 0  ), -1  )
1
7f =  [ ???  ]
1
8print  (f .  data .  size())
1
9r eturn  f
F IGURE  7.4: PyT orch code snippet for extracting the fc  7 laye r fro m a pre-trained VGG19 CNN
model  .
1
.Complete line 6; what should be the value of  pr etrained  ?
2
.Complete line 10; what should be the value of  self.featur es  ?
3
.Complete line 12; what should be the value of  self.num_feats  ?
4
.Complete line 17; what should be the value of  f  ?
PRB-167  
  CH.PRB- 7.1 1.
W e ar e still r eferring to T able (  9.3  ) and using the skeleton code
pr ovided in Fig. (  7.5  ) to derive a new CNN entitled  ResNetBottom  fr om
the ResNet34 CNN, to extract a 512-dimensional FV for a given input
image. Complete the code as follows:
1
.The value of  self.featur es  in line 7  .
2 The  forwar d  method in line 1 1  ..
1 import  tor chvision.models  as  models
2 res_model =  models .  resnet34(pretrained =  T rue  )
3 class  ResNetBottom  (torch .  nn .  Module):
4 def  __init__  ( self  , original_model):
5 super  (ResNetBottom, self  ) .  __init__  ()
6 self  .  features =  [ ???  ]
7
8 def  forward( self  , x):
9 x =  [ ???  ]
1
0x =  x .  view(x .  size( 0  ), -1  )
1
1r eturn  x
F IGURE  7.5: PyT orch code s keleton for extracting a 512-dimensional FV from a pre-trai ned
ResNet34 CNN model  .
PRB-168  
  CH.PRB- 7.12.
Still r eferring to T able (  9.3  ), the PyT or ch b ased pseudo code snippet in
Fig. (  7.6  ) r eturns the 512-dimensional FV fr om the modified ResNet34
CNN, given a 3-channel RGB image as an input  .
1 import  tor chvision.models  as  models
2 fr om  tor chvision  import  transforms
3 ...
4
5 test_trans =  transforms .  Compose([
6 transforms .  Resize(imgnet_size),
7 transforms .  T oT ensor(),
8 transforms .  Normalize([ 0.485  , 0.456  , 0.406  ],
9 [ 0.229  , 0.224  , 0.225  ])])
1
0
1
1def  ResNet34FE  (image, model):
1 f =  None2
1
3image =  test_trans(image)
1
4image =  V ariable(image, requires_grad =  False  ) .  cuda()
1
5image =  image .  cuda()
1
6f =  model(image)
1
7f =  f .  view(f .  size( 1  ), -1  )
1
8print  ( "Size :  {}  "  .  format(f .  shape))
1
9f =  f .  view(f .  size( 1  ), -1  )
2
0print  ( "Size :  {}  "  .  format(f .  shape))
2
1f =f .  cpu() .  detach() .  numpy()[ 0  ]
2
2print  ( "Size :  {}  "  .  format(f .  shape))
2
3r eturn  f
F IGURE  7.6: PyT orch code s keleton for extracting a 512-dimensional FV from a pre-trai ned
ResNet34 CNN model  .
Answer the following questions r egar ding the code in Fig. (  7.6  ):
1
.What is the purpose of  test_trans  in line 5?
2
.Why is the parameter  r equir es_grad  set to False in line 14?
3
.What is the purpose of  f.cpu()  in line 23?
4
.What is the purpose of  detach()  in line 23?
5
.What is the purpose of  numpy()[0]  in line 23?7.2.2   Fine-tuning CNNs
PRB-169  
  CH.PRB- 7.13.
Define the term  fine-tuning (FT) of an ImageNet pre-trained CNN
.
PRB-170  
  CH.PRB- 7.14.
Describe thr ee differ ent methods by which one can fine-tune an
ImageNet pr e-trained CNN  .
PRB-171  
  CH.PRB- 7.15.
Melanoma is a lethal form of malignant skin cancer , fr equ ently
misdiagnosed a s a benign skin lesion or even left completely
undiagnosed  .
In the United States alone, melanoma accounts for an estimated  6,
750 deaths p er annu m [  6  ]. W ith a  5-year  survival rate o f  98%, early
diagnosis and tr eatment is now mor e likely and possibly the most
suitable means for melanoma r elated death r eduction. Dermoscopy
images, shown in Fig. (  7.7  ) ar e widely used in the detection and
diagnosis of skin lesions. Dermatologists, r elying on personal
experience, ar e involved in a laborious task of manually se ar ching
dermoscopy images for lesions  .
Ther efor e, ther e is a very r eal need for automated analysis tools,
pr oviding assistance to clinician s scr eening for skin metastases. In this
question, you ar e tasked with ad dr essing some of the fundamental issues
DL r esear chers face when building deep learning pipelines. As
suggested in [  3  ], you ar e going to use ImageNet pr e-trained CNN to
r esolve a classification task  .F IGURE  7.7: Skin lesion categories.  An exemplary visualization of melanoma  .
1
.Given th at the skin lesions fall  into seven distinct categories, and
you ar e training using cr oss-entr opy loss, how should the classes be
r epr esented so that a typical PyT or ch training loop will success fully
conver ge?
2
.Suggest several data augmentation techniques to augment the data  .
3
.W rite a code snippet in PyT or c h to adapt the CNN so that it can
pr edict  7 classes instead of the original sour ce size of  1000.
4
.In or der to fine tune our CNN, the (original) output layer with  1000
classes was r emoved and the CNN was adjusted so that the ( new)
classification layer comprised seven softmax neur ons emitting
posterior pr obabilities of class membership for each lesion type  .
7.2.3   Neural style transfer , NST
Before attempt ing the problems in the section, you are  strongly
recommended to read the paper: “ A Neura l Algorithm of Artistic S tyle  ” [  2
].
PRB-172  
  CH.PRB- 7.16.
Briefly describe how neural style transfer (NST) [  2  ] works  .
PRB-173  
  CH.PRB- 7.17.Complete the sentence  : When using the VGG-19 CNN [  8  ] for
neural-style transfer , ther e differ ent images ar e involved. Namely they
ar e: [...], [...] and [...]  .
PRB-174  
  CH.PRB- 7.18.
Refer to Fig.  7.8  and answer the following questions:
F IGURE  7.8: Artistic style transfer using the style of Francis Picabia’ s Udnie painting  .
1
.Which loss is being utilized during the training pr ocess?
2
.Briefly describe the use of activations in the training pr ocess  .
PRB-175  
  CH.PRB- 7.19.
Still r eferring to Fig.  7.8  :1
.How ar e the activations utilized in comparing the content of the
content image to the content of the combined image?  .
2
.How ar e the act ivations utilized in comparing the style of the co ntent
image to the  style of the combined image?  .
PRB-176  
  CH.PRB- 7.20.
Still r eferring to Fig.  7.8  . For a new style transfer algorithm, a data
scientist extract s a featur e vector fr om an image using a pr e-trained
ResNet34 CNN (  7.9  )  .
1 import  tor chvision.models  as  models
2 ...
3 res_model =  models .  resnet34(pretrained =  T rue  )
F IGURE  7.9: PyT orch declaration for a pre-trained ResNet34 CNN  .
He then defines the cosine similarity between two vectors:
u  = {u  1  , u  2  , . . ., u  n  } and :
v  = {v  1  , v  2  , . . ., v  n  }
as:
Thus, the cosine similarity between two vectors measur es the  cosine
of the angle  between the vectors irr espective of their magnitude. It is
calculated as the dot pr oduct of two numeric vectors, and is normalized
by the pr oduct of the length of the vectors  .
Answer the following questions:
1
.Define the term  Gram matrix.
2 Explain in detail how vector sim ilarity is utilised in the calculation. of the  Gram matrix during the training of NST  .
7 . 3   S o l u t i o n s
7.3.1   CNN as Fixed Feature Extractor
SOL-157  
  CH.SOL- 7.1.
T rue. The incr eased depth in V GG-Net was made possible using
smaller filters without substantially incr easing the number of learnable
parameters. Albeit an unwanted side effect of the usage of smaller filters
is the incr ease in the number of filters per -layer  .
SOL-158  
  CH.SOL- 7.2.
T rue. The ResNet ar chitectur e terminates with a global average
pooling layer followed by a K-w ay FC layer with a softmax activation
function, wher e K is the number  of classes (ImageNet has 1000 classes).
Ther efor e, the ResNet has no hidden FC layers  .
SOL-159  
  CH.SOL- 7.3.  Note that  1 bit  = 0.000000125 MB,
ther efor e:
SOL-160  
  CH.SOL- 7.4.
T rue. Th er e ar e dozens of published papers supporting this cl aim.
Y ou ar e encouraged to sear ch them on Arxiv or Google Scholar  .
SOL-161  
  CH.SOL- 7.5.
T rue. One of the  major hur dles of training a medical AI system i s the
lack of annotated data. Ther efor e, extensive r esear ch is conducted toexploit w ays for FE and transf er learning, e.g., in the application of
ImageNet trained CNNs, to tar get datasets in which labeled data is
scar ce  .
SOL-162  
  CH.SOL- 7.6.
Ther e ar e two main r easons why this is possible:
1
.The hug e number of images ins ide the ImageNet dataset ensur es a
CNN model that generalizes to additional domains, like the
histopathology domain, which is substantially differ ent fr om the
original domain the model was trained one (e.g., cats and dogs)  .
2
.A massi ve array of disparate visual patterns is pr oduced by an
ImageNet trained CNN, since it consists of  1, 000 differ ent gr oups  .
SOL-163  
  CH.SOL- 7.7.
Complete the missing parts r egar ding the VGG19 CNN ar chitectur e:
1
.The VGG19 CNN consists of  19  layers  .
2
.It consists of  5  convolutional and  3 FC  layers  .
3
.The input image size is  244  , the defa ult size most ImageNet t rained
CNNs work on  .
4
.The number of input channels is  3  .
5
.Every image has its mean RGB value  subtracted  . (why?)
6
.Each convolutional layer has a  small  kernel sized  3  ×  3  . (why?)7
.The number of pixels for padding and stride is the same and equals
1  .
8
.Ther e ar e 5  convolutional  layers h aving a kernel size of  2  ×  2  and
a stride of  2  pixels  .
9
.For non-linearity a  r ectified linear unit (ReLU [  5  ])  is used  .
10
.The  3  FC layers ar e part of the linear classifier  .
1 1  . The first two FC layers consist of  4096  featur es  .
12
.The last FC layer has only  1000  featur es  .
13
.The last FC layer is terminated by a  softmax  activation layer  .
14
.Dr opout  is  being used between the FC layers  .
SOL-164  
  CH.SOL- 7.8.
1
.One or mor e layers of the VGG19 CNN ar e selected for extraction
and a new CNN is designed on top of it. Thus, during infer ence  our
tar get l ayers ar e extracted and not the original softmax l ayer .
Subsequently , we iterate and run  inference  over all the images in
our panc r eatic cancer data-set, extract the featur es, and persist them
to perm anent storage such as a solid-state drive (SSD) de vice.
Ultimately , each image has a corr esponding FV  .
2
.Regar ding the VGG19 CNN, ther e ar e numer ous ways of extracting
and combining featur es fr om differ ent layers. Of course, these
differ ent layers, e.g., the FC, conv4_3, and fc7 layer may be
combined together to form a lar ger featur e vector . T o deter mine
which method works best, you shall have to experiment on yourdata-set; ther e is no way of a-priory determining the op timal
combination of layers. Her e ar e several examples:
(a) Accessing the  last FC layer  r esulting in a  1000 -D FV . The outpu
the scor e for each of the  1000 classes of the ImageNet data-set  .
(b) Removing the  last FC layer  leaves the fc7 l ayer , r esulting in
4096 -D FV  .
(c) Dir ectly accessing the  conv4_3 layer  r esults in a  512 -D FV  .
3
.Once th e FVs ar e extracted, we can train any linear classifier such
as a n SVM or softmax classifier  on the FV data-set, and not on the
original images  .
SOL-165  
  CH.SOL- 7.9.
One ben efit of using the FC lay er is that other ImageNet CNNs  can
be used in tande m with the VGG19 to cr eate an ensemble since they all
pr oduce the same  1000 -D sized FV  .
SOL-166  
  CH.SOL- 7.10.  The full code is pr esented in Fig. (  7.10  )  .
1 import  tor chvision.models  as  models
2 ...
3 class  VGG19FE  (torch .  nn .  Module):
4 def  __init__  ( self  ):
5 super  (VGG19FE, self  ) .  __init__  ()
6 original_model =  models .  VGG19(pretrained =  T rue  )
7 self  .  real_name =  ((( type  (original_model) .  __name__  )))
8 self  .  real_name =  "vgg19"
9
1
0self  .  features =  original_model .  features
1
1self  .  classifier =  torch .  nn .  Sequential(
1
2(*list(original_model .  classifier .
1 children())[: -1  ]))3
1
4self  .  num_feats =  4096
1
5
1
6def  forward( self  , x):
1
7f =  self  .  features(x)
1
8f =  f .  view(f .  size( 0  ), -1  ) # (1, 4096) -> (4096,)
1
9f =  self  .  classifier(f)
2
0print  (f .  data .  size())
2
1r eturn  f
F IGURE  7.10: PyT o rch code snippet for extracting the fc  7 layer from a pre-trained VGG19
CNN model  .
1
.The value of the parameter  pr etrained  should be T rue in or der to
instruct PyT or ch to load an ImageNet trained weights  .
2
.The value of  self.featur es  should be  original_model.featur es  . This
is becau se we like to r etain the layers of the original clas sifier
(original_model)  .
3
.The value of  self.num_feats  should be  4096  . (Why?)
4
.The value of  f  should be  self.classifier(f)  since our newly cr eated
CNN has to be invoked to generate the FV  .
SOL-167  
  CH.SOL- 7.1 1.
1
.Line num ber 7 in Fig. (  7.1 1  ) takes car e of extracting the the
corr ect 512-D FV  .
2 Line num ber 1 1 in Fig. (  7.1 1  ) extracts the corr ect 512-D FV by. cr eating a sequential module on top of the existing featur es  .
1 import  tor chvision.models  as  models
2 res_model =  models .  resnet34(pretrained =  T rue  )
3 class  ResNetBottom  (torch .  nn .  Module):
4 def  __init__  ( self  , original_model):
5 super  (ResNetBottom, self  ) .  __init__  ()
6 self  .  features =  [ ???  ]
7 def  forward  ( self  , x):
8 x =  [ ???  ]
9 x =  x .  view(x .  size( 0  ), -1  )
1
0r eturn  x
F IGURE  7.1 1: PyT o rch code snippet for extractin g the fc  7 layer from a pre-trained
VGG19 CNN model  .
SOL-168  
  CH.SOL- 7.12.
1
.T ransforms ar e incorporated into deep learning pipelines in
or der to apply one or mor e operations on images which ar e
r epr esented as tensors. Differ en t transforms ar e usually utilized
during training and infer ence. For instance, during training we
can use a transform to augm ent our data-set, while during
infer ence our transform may be limited only to normalizing an
image. PyT or ch allows the use of transforms either during
training or infer ence. The purpose of  test_trans  in line 5 is to
normalize the data  .
2
.The parameter  r equir es_grad  is set to False in line 14 sinc e
during infer ence the computation of gradients is obsolete  .
3
.The purpose of  f.cpu()  in line 1 1 is to move a tensor that was
allocated on the GPU to the C PU. This may be r equir ed if we
want to apply a CPU-based m ethod fr om the Python numpy
package on a T ensor that does not live in the CPU  .4
.detach()  in line  23 r eturns a newly cr eated tensor without
affecting the curr ent tensor . It a lso detaches the output fr om th e
curr ent compu tational grap h, hence no gradient is
backpr opagated for this specific variable  .
5
.The purpose of  numpy()[0]  in line  23 is to convert the variable
(an array) to a numpy compatible variable and also to r etriev e
the first element of the array  .
7.3.2   Fine-tuning CNNs
SOL-169  
  CH.SOL- 7.13.
The term fine-tuning (FT) of an ImageNet pr e-trained CNN r efers
to the method b y which one or mor e of the weights of the CNN  ar e
r e-trained on a new tar get dat a-set, which may or may-not have
similarities with the ImageNet data-set  .
SOL-170  
  CH.SOL- 7.14.  The thr ee methods ar e as follows:
1
.Replacing and r e-training  only the classifier  (usually the FC
layer) of the ImageNet pr e-trained CNN, on a tar get data-set  .
2
.FT  all o f the layers  of th e ImageNet pr e-trained C NN, on a
tar get data-set  .
3
.FT  part of the layers  of the ImageNet pr e-trained CNN, on a
tar get data-set  .
SOL-171  
  CH.SOL- 7.15.
1
.The categories have to be r epr esented numerically . One such
option is pr esented in Code (  7.1  )  .
1 'MEL'  : 0  , 'NV'  : 1  , 'BCC'  : 2  , 'AKIEC'  : 3  , 'BKL'  : 4  , 'DF'  : 5  ,↪     'V ASC'  : 6
C ODE  7.1: The seven categories of skin lesions.
2
.Several possible  augmentations  ar e pr esented in Code (  7.2  ). It
is usual ly , that by trial and err or one finds the best possib le
augmentation for a tar get data-set. However , methods such as
AutoAugment m ay r ender the manual selection of augmentation s
obsolete  .
1 self  .  transforms =  []
2 if  rotate:
3 self  .  transforms .  append(RandomRotate())
4 if  flip:
5 self  .  transforms .  append(RandomFlip())
6 if  brightness != 0  :
7 self  .  transforms .  append(PILBrightness())
8 if  contrast != 0  :
9 self  .  transforms .  append(PILContrast())
1
0if  colorbalance != 0  :
1
1self  .  transforms .  append(PILColorBalance())
1
2if  sharpness != 0  :
1
3self  .  transforms .  append(PILSharpness())
CODE 7.2: Pseudeo code for augmentations.
3
.In contrast to the ResNet CNN which ends by an FC layer , the
ImageNet pr e-trained DPN CNN family , in this case the
pr etrainedmodels.dpn107, terminated by a Conv2d  layer and
hence m ust be adapted accor dingly if one wishes to change the
number fo classes fr om the 100 0 (ImageNet) classes to our skin
lession classification pr oblem (7  classes). Line 7 in Code (  7.3  )
demonstrated this idiom  .
1 import  tor ch2 class  Dpn107Finetune  (nn .  Module):
3 def  __init__  ( self  , num_classes: int  , net_kwards):
4 super  () .  __init__  ()
5 self  .  net =  pretrainedmodels .  dpn107( **  net_kwards)
6 self  .  net .  __name__  =  str  ( self  .  net)
7 self  .  net .  classifier =  torch .  nn .  Conv2d( 2688  ,
↪    num_classes,kernel_size =1  )
8 print  ( self  .  net)
C ODE  7.3: C hange between 1000 classes to 7 classes for the ImageNet pre-trained DPN
CNN family .
7.3.3   Neural style transfer
SOL-172  
  CH.SOL- 7.16.
The images ar e: a content image, a style image and lastly a
combined image  .
SOL-173  
  CH.SOL- 7.17.
The algorithm pr esented in the paper suggests how to
combine the content a first im age with the style of a second
image to generate a thir d, stylized image using CNNs  .
SOL-174  
  CH.SOL- 7.18.
The answers ar e as follows:
1
.The trai ning pipeline uses a com bined loss which consists of a
weighted average of the style loss and the content loss  .
2
.Differ ent CNN layers at differ en t levels ar e utilized to captur e
both fine-grained stylistic details as well as lar ger stylistic
featur es  .
SOL-175  
  CH.SOL- 7.19.
1
.The con tent loss is the mean squar e err or (MSE) calculated as
the differ ence between the CNN activations of the last
convolutional layer of both the content image and the style
images  .
2
.The style loss amalgamates the losses of several layers
together . For each layer , the gram matrix (see  7.2  ) for the
activations at that layer is obtai ned for both the style and the
combined image s. Then, just like in the content loss, the MSE
of the Gram matrices is calculated  .
C HAPTER
8     DEEP LEARNING
It is the weight, not numbers of experiments that is to be
r egar ded  .
— Isaac Newton.
C o n t e n t s
Intr oduction
Pr oblems
Cross V alidation
CV approaches
K-Fold CV
Stratification
LOOCV
Convolution and correlation
The convolution operator
The correlation operator
Padding and stride
Kernels and filters
Convolution and correlation in python
Separable convolutions
Similarity measures
Image, text similarity
Jacard similarity
The Kullback-Leibler DistanceMinHash
Perceptrons
The Single Layer Perceptron
The Multi Layer Perceptron
Activation functions in perceptrons
Back-propagation in perceptrons
The theory of perceptrons
Learning logical gates
Activation functions (rectification)
Sigmoid
T anh
ReLU
Swish
Performance Metrics
Confusion matrix, precision, recall
ROC-AUC
NN Layers, topologies, blocks
CNN arithmetics
Dropout
Convolutional Layer
Pooling Layers
MaxPooling
Batch normalization, Gaussian PDF
The Gaussian distribution
BN
Theory of CNN design
CNN residual blocks
T raining, hyperparameters
Hyperparameter optimization
Labelling and bias
V alidation curve ACCV alidation curve Loss
Inference
Optimization, Loss
Stochastic gradient descent, SGD
Momentum
Norms, L1, L2
Solutions
Cross V alidation
CV approaches
K-Fold CV
Stratification
LOOCV
Convolution and correlation
The convolution operator
The correlation operator
Padding and stride
Kernels and filters
Convolution and correlation in python
Separable convolutions
Similarity measures
Image, text similarity
Jacard similarity
The Kullback-Leibler Distance
MinHash
Perceptrons
The Single Layer Perceptron
The Multi Layer Perceptron
Activation functions in perceptrons
Back-propagation in perceptrons
The theory of perceptrons
Learning logical gatesActivation functions (rectification)
Sigmoid
T anh
ReLU
Swish
Performance Metrics
Confusion matrix, precision, recall
ROC-AUC
NN Layers, topologies, blocks
CNN arithmetics
Dropout
Convolutional Layer
Pooling Layers
MaxPooling
Batch normalization, Gaussian PDF
The Gaussian distribution
BN
Theory of CNN design
CNN residual blocks
T raining, hyperparameters
Hyperparameter optimization
Labelling and bias
V alidation curve ACC
V alidation curve Loss
Inference
Optimization, Loss
Stochastic gradient descent, SGD
Momentum
Norms, L1, L28 . 1   I n t r o d u c t i o n
T was Alex Kriz hevsky who first demonstrated that a convoluti onal
neural network (CNN) can be e f fectively trained on the ImageNet
lar ge sc ale visual recognition challenge. A CNN automatic ally
provides some degree of translation and assumes that we wish to
learn filters  , i n a da ta-driven fashion, as a m eans to extract features describing
the inpu ts. CNNs are applied to numerous computer vision, imaging, and
computer graphi cs tasks as in [  24  ], [  23  ], [  15  ], [  5  ]. Furthermore, they have
become extremely popular , and novel architectures and algorithms are
continually popping up overnight.
8 . 2   P r o b l e m s
8.2.1   Cross V alidation
On the significance of cross validation and stratification in pa rticular , refer to
“ A st udy of cr oss -validation and bootstrap for accuracy estimat ion and model
selection  ” [  17  ].
CV approaches
PRB-177  
  CH.PRB- 8.1.
Fig (  8.1  ) depicts two differ ent cr oss-validation appr oaches. Name them
.F IGURE  8.1: T wo CV approaches
PRB-178  
  CH.PRB- 8.2.
1
.What is the purpose of the following Python code snippet  8.2  ?
1 skf =  StratifiedKFold(y , n_folds =5  , random_state =989  ,
↪     shuf fle =  T rue  )
F IGURE  8.2: Stratified K-fold
2
.Explain the benefits of using the K-fold cr oss validation appr oach  .
3
.Explain the benefits of using the Stratified K-fold cr oss validation
appr oach  .
4
.State the  differ e nce between K-fold cr oss validation and stratified cr oss
validation  .
5
.Explain in your own wor ds what is meant by “W e adopted a 5-fold cr oss-
validation appr oach to estimate the testing err or of the model”  .K-Fold CV
PRB-179  
  CH.PRB- 8.3.
T rue or False  : In a K -fold CV  appr oach, the testing set is completely
excluded fr om the pr ocess and only the training and validation sets ar e
involved in this appr oach  .
PRB-180  
  CH.PRB- 8.4.
T rue or False  : In a K-fold CV appr oach, the final test err or is:
PRB-181  
  CH.PRB- 8.5.
Mark all the correct choices regarding a cross-validation approach:
(i) A 5-fold cr oss-validation appr oach r esults in 5-differ ent model
instances being fitted  .
(ii) A 5-fold cr oss-validation appr o ach r esults in 1 model instance being
fitted over and over again 5 times  .
(iii) A 5-fold cr oss-validation appr oach r esults in 5-differ ent model
instances being fitted over and over again 5 times  .
(iv) Uses K-differ ent data-folds  .
PRB-182  
  CH.PRB- 8.6.
Mark al l the corr ect choices r egar ding the appr oach that should be taken
to compute the performance of K-fold cr oss-validation:
(i) W e compute the cr oss-validation performance as the arithmetic mean
over the K performance estimates fr om the validation sets  .(ii) W e c ompute the cr oss-validation  performance as the best one over the K
performance estimates fr om the validation sets  .
Stratification
PRB-183  
  CH.PRB- 8.7.
A data- scientist who is inter ested in classifying cr oss sections of
histopathology image slices (  8.3  ) decides to adopt a cr oss-validation
appr oach he once r ead about in a book. Name the appr oach fr om the
following options:
F IGURE  8.3: A specific CV approach
(i) 3-fold CV
(ii) 3-fold CV with stratification
(iii) A (r epeated) 3-fold CV
LOOCV
PRB-184  
  CH.PRB- 8.8.
1
.T rue or false  : Th e lea ve-one-out cr oss-validation (LOOCV) appr oach is
a sub-ca se of k-fold cr oss-valida tion wher ein K equals N, the sample size
.2
.T rue or false  : It is always po ssible to find an optimal value n, K  = n in
K-fold cr oss-validation  .
8.2.2   Convolution and correlation
The convolution operator
PRB-185  
  CH.PRB- 8.9.
Equation  8.2  is commonly used in image pr ocessing:
1
.What does equation  8.2  r epr esent?
2
.What does g  ( t  ) r epr esent?
PRB-186  
  CH.PRB- 8.10.
A data-scientist assumes that:
i A convolution operation is both linear and shift invariant  .
ii A co nvolution operation is just like corr elation, except that we  flip over
the filter befor e applying the corr elation operator  .
iii The convolution  operation r eaches a maximum, only in cases w her e the
filter is mostly similar to a specific section of the input signal  .
Is he  right in assuming so? Explain in detail the meaning of these statements
.
The correlation operator
PRB-187  
  CH.PRB- 8.1 1.
Mark the corr ect choice(s):1
.The cr oss-corr e lation operator is used to find the location w her e two
differ ent signals ar e most similar  .
2
.The autocorr elation operator is used to find when a signal is similar to a
delayed version of itself  .
PRB-188  
  CH.PRB- 8.12.
A d ata-scientis t pr ovides you with a formulae for a dis cr ete 2D
convolution operation (  8.3  ):
Using only (  8.3  ), write the equivalent 2D corr elation operation  .
Padding and stride
Recommended r eading : “ A gu ide to convolution arithmetic for deep learning  ”
by V incent Dumoulin and Francesco V isin (2016) [  22  ].
PRB-189  
  CH.PRB- 8.13.
When designing a convolution al neural network layer , one must also
define how the filter or kernel slides thr ough the input signal. This is
contr olled by what is known as the stride and padding parameters or modes.
The two  most commonly used p adding appr oached in convolutions ar e the
V ALID  and the  SAME  modes. Given an input stride of  1:
1
.Define SAME
2
.Define V ALID
PRB-190  
  CH.PRB- 8.14.
T rue or False  : A valid  convolution is a type of convolution operation
that does not use any padding on the input  .PRB-191  
  CH.PRB- 8.15.
Y ou ar e pr ovided  with a K  × K input signal and a θ  × θ filter . The signal
is subjec ted to the  valid  padding mode convolution. What ar e the r esulting
dimensions?
PRB-192  
  CH.PRB- 8.16.
As d epicted in (  8.4  ), a filte r is applied to a  × 3 input signal. Identify the
corr ect choice given a stride of 1 and  Same  padding mode  .
F IGURE  8.4: A padding approach
PRB-193  
  CH.PRB- 8.17.
As depicted in in (  8.5  ), a filte r is applied to a  3 × 3 input signal, mark
the corr ect choices given a stride of 1  .
(i) A r epr esents a V ALID convolution and B r epr esents a SAME
convolution(ii) A r epr esents a SAME convolution and B r epr esents a V ALID
convolution
(iii) Both A and B r epr esent a V ALID convolution
(iv) Both A and B r epr esent a SAME convolution
F IGURE  8.5: A padding approach
PRB-194  
  CH.PRB- 8.18.
In t his question we discuss t he two most commonly used padding
appr oaches in convolutions;  V ALID  and  SAME  . Fig.  8.6  pr esents python
code for generating an input signal arr  001 and a convolution kernel filter
001. The input signal, arr  001 is first initialized to all zer os as follows:
1
.W ithout actuall y executing the code, determine what would be the
r esulting shape of the convolve  2 d  () operation  .
2
.Manually compute the r esult of convolving the input signal with the
pr ovided filter  .
3
.Elaborate why the size of the r esulting convolutions is  smaller  than the
size of the input signal  .
1 import  numpy
2 import  scipy .signal
3
4 arr01 =  numpy .  zeros(( 6  , 6  ),dtype =  float  )
5 print  (arr01)
6 arr01[:,: 3  ] = 3.0
7 arr01[:, 3  :] = 1.0
8
9 filter001 =  numpy .  zeros(( 3  , 3  ), dtype =  float  )
1
0filter001[:, 0  ] = 2.0
1
1filter001[:, 2  ] = -2.0
1
2
1
3output =  scipy .  signal .  convolve2d(arr01, filter  , mode =  'valid'  )
F IGURE  8.6: Convolution and correlation in python
Kernels and filters
PRB-195  
  CH.PRB- 8.19.
Equation  8.6  is the discr ete equivalent of equation  8.2  which is fr equently
used in image pr ocessing:
1 Given the following discr ete kernel in the X dir ection, what would be the. equivalent Y dir ection?
2
.Identify the discr ete convolution kernel pr esented in (  8.7  )  .
F IGURE  8.7: A 3 by 3 convolution kernel
PRB-196  
  CH.PRB- 8.20.
Given a n image of size w  × h, and a kernel with width K, how many
multiplications and additions ar e r equir ed to convolve the image?
Convolution and correlation in python
PRB-197  
  CH.PRB- 8.21.
Fig.  8.8  pr esents two built-in Python functions for the convolution and
corr elation operators  .1 import  nympy  as  np
2 np .  convolve (A, B, "full"  ) # for convolution
3 np .  correlate (A, B, "full"  ) # for cr oss corr elation
F IGURE  8.8: Convolution and correlation in python
1
.Implement the convolution operation fr om scratch in Python. Compar e it
with the  built-in numpy equivalent  .
2
.Implement the corr elation operation using the implementation of the
convolution operation. Compar e it with the built-in numpy equivalent  .
Separable convolutions
PRB-198  
  CH.PRB- 8.22.
The Gaussian distribution in th e 1D and 2D is shown in Equations  8.8
and  8.9  .
The Gaussian filter , is an operator that is used to blur images and r emove
detail and noise while acting like a low-pass filter . This is similar to the way
a m ean filter w orks, but the G aussian filter uses a differ ent kernel. This
kernel is r epr esented with a Gaussian bell shaped bump  .
Answer the following questions:
1
.Can  8.8  be used dir ectly on a 2D image?
2
.Can  8.9  be used dir ectly on a 2D image?
3
.Is the Gaussian filter separable? if so, what ar e the advantages of
separable filters  .8.2.3   Similarity measures
Image, text similarity
PRB-199  
  CH.PRB- 8.23.
A data scientist  extracts a feat ur e vector fr om an image usin g a pr e-
trained ResNet34 CNN (  9.5  )  .
1 import  tor chvision.models  as  models
2 ...
3 res_model =  models .  resnet34 (pretrained =  T rue  )
F IGURE  8.9: PyT orch declaration for a pre-trained ResNet34 CNN (simplified)  .
He t hen applies the following al gorithm, entitled xxx on the ima ge (  9.2  )
.
1 void  xxx  (std ::  vector <  float  >&  arr) {
2 float  mod = 0.0  ;
3 for  ( float  i  : arr) {
4 mod +=  i *  i;
5 }
6 float  mag =  std ::  sqrt(mod);
7 for  ( float  &  i  : arr) {
8 i /=  mag;
9 }
1
0}
An unknown algorithm in C++1 1
F IGURE  8.10: listing
Which r esults in this vector (  8.1 1  ):
F IGURE  8.1 1: A one-dimensional 512-element embedding for a single image from the ResNet34
architecture  .
Name the algorithm that he used and explain in detail why he used it  .
PRB-200  
  CH.PRB- 8.24.
Further to the above, the scientist then applies the following algorithm:
Algorithm 3:  Algo 1
Data:  T wo vectors v  1 and v  2 are provided
Apply algorithm xxx on the two vectors
Run algorithm 2
Algorithm 4:  Algo 2
1 float  algo2  ( const  std ::  vector <  float  >&  v1, const
↪     std ::  vector <  float  >&  v2){
2 double  mul = 0  ;
3 for  ( size_t  i = 0  ; i <  v1.size(); ++  i){
4 mul +=  v1[i] *  v2[i];
5 }
6 if     (mul < 0  ) {
7 r eturn  0  ;
8 }
9 r eturn  mul;
1
0}
F IGURE  8.12: An unknown algorithm
1
.Name th e algorithm algo2 that h e used and explain in detail what he used
it for  .
2
.W rite the mathematical formulae behind it  .
3
.What ar e the minimum and maximum values it can r eturn?4
.An alternative similarity measur es between two vectors is:
Name the measur e  .
Jacard similarity
PRB-201  
  CH.PRB- 8.25.
1
.What is the formulae for the Jaccar d similarity [  12  ] of two sets?:
2
.Explain the formulae in plain wor ds  .
3
.Find the Jacar d similarity given the sets depicted in (  8.13  )
F IGURE  8.13: Jaccard similarity  .
4
.Compute the Jaccar d similarity of each pair of the following sets:
i 12, 14, 16, 18  .
ii 1 1, 12, 13, 14, 15  .
iii 1 1, 16, 17  .The Kullback-Leibler Distance
PRB-202  
  CH.PRB- 8.26.
In this pr oblem, you have to actually r ead 4 differ ent papers, so you will
pr obably not encounter such a question during an intervi ew , however
r eading academic papers is an excellent skill to master for becoming a DL
r esear cher  .
Read the following papers which discuss aspects of the Kullback-Leibler
diver gence:
i Bennet [  2  ]
ii Ziv [  29  ]
iii Bigi [  3  ]
iv Jensen [  1  ]
The Kullback-Leibler  divergence  , which was discussed thor oughly in
chap  4  is a measur e of  how differ ent two pr obability distribution ar e. As
noted, the KL diver gence of the pr obability distributions P  , Q on a set X is
defined as shown in Equation  8.1 1  .
Note ho wever that since KL diver gence is a non-symmetric inf ormation
theor etical measur e of distance of P fr om Q, then it is not strictly a distance
metric. D uring the past years, v arious KL based distance measur es (rather
than div er gence based) have been intr oduced in the literatur e generalizing
this measur e  .
Name each of the following KL based distances:
MinHash
Read the paper entitled Detecting near -duplicates for web crawling  [ 12  ] and
answer the following questions.
PRB-203  
  CH.PRB- 8.27.
What is the go al of hashing? Draw a simple HashMap of keys and
values. Explain what is a collision and the notion of buckets. Explain what is
the goal of MinHash  .
PRB-204  
  CH.PRB- 8.28.
What is Locality Sensitive Hashing or LSH?
PRB-205  
  CH.PRB- 8.29.
Complete the sentence  : L SH main goa l is to [...] the pr obability of a
colliding, for similar items in a corpus  .
8.2.4   Perceptrons
The Single Layer Perceptron
PRB-206  
  CH.PRB- 8.30.
1
.complete the sentence  : In a single-layer feed-forwar d NN, ther e ar e [...]
input(s) and [...]. output layer(s) and no [...] connections at all  .
PRB-207  
  CH.PRB- 8.31.
In its simplest form, a  perceptron (  8.16  ) acc epts only a  binary input an d
emits a binary output. The output, can be evaluated as follows:Wher e  weights ar e deno ted by w  j  and  biases ar e denoted by b. Answer the
following questions:
1
.T rue or False  : If such  a per ceptr on is trained using a labelled corpus,
for eac h partic ipating neur on  the values w  j  and b ar e learned
automatically  .
2
.T rue or False  : If we instead u se a new per ce ptr on (sigmoidial) defined
as follows:
wher e σ is the  sigmoid function:
Then the  new per ceptr on can pr ocess inputs ranging between 0 and 1
and emit output ranging between 0 and 1  .
3
.W rite the cost function associated with the sigmoidial neur on  .
4
.If we want to train the per ceptr on in or der to obtain the bes t possible
weights and biases, which mathematical equation do we have to solve?
5
.Complete the sentence: T o solve this mathematical equation, we have to
apply [...]
6
.What does the following equation stands for?
Wher e:7
.Complete the sentence: Due to the time-consuming natur e of computing
gradients for each entry in the training corpus, modern DL libraries
utilize a  technique that gauges the gradient by first randomly sampling a
subset fr om the training corpus, and t hen averaging only this subset in
every ep och. This appr oach is known as [...]. The actual number of
randomly chosen samples in each epoch is termed [...]. The gradient
itself is obtained by an algorithm known as [...]  .
The Multi Layer Perceptron
PRB-208  
  CH.PRB- 8.32.
The following questions r efer to the MLP depicted in (  9.1  ). The inputs to
the MLP in (  9.1  ) ar e x  1  = 0.9 and x  2  = 0.7 r espectively , and the weights w  1
= −  0.3 and w  2  = 0.15 r espectively . Ther e is a single hidden node, H  1  . The
bias term, B  1 equals  0.001.
F IGURE  8.14: Several nodes in a MLP  .
1
.W e e xamine the mechanism of a single hidden node, H  1  . The inputs and
weights go thr ough a linear tr ansformation. What is the value of the
output (out  1 ) observed at the sum node?
2
.What is the value r esulting fr om the application the sum operator?
3
.V erify the corr ectness of your r esults using PyT or ch  .Activation functions in perceptrons
PRB-209  
  CH.PRB- 8.33.
The following questions r efer to the MLP depicted in (  8.15  )  .
1
.Further to the a bove, the ReLU non-linear activation function  g  ( z  ) =
max{0, z  } is applied (  8.15  ) to the output o f the linear transformation
What is the value of the output (out  2 ) now?
F IGURE  8.15: Several nodes in a MLP  .
2
.Confirm your manual calculation using PyT or ch tensors  .
Back-propagation in perceptrons
PRB-210  
  CH.PRB- 8.34.
Y our co-worker , an postgradua te student at M.I.T , suggests using the
following activation functions in a MLP . Which ones can ne ver be back-
pr opagated and why?
i
ii
iii
iv
PRB-21 1  
  CH.PRB- 8.35.
Y ou ar e pr ovided with the following MLP as depicted in  8.16  .
F IGURE  8.16: A basic MLP
The ReLU non-linear activation  function g  ( z  ) = max{ 0, z  } is applied to
the hidden layers H  1  ...H  3  and the bias term equals  0.001.
At a certain poin t in time it has the following values  8.17  all o f wh ich ar e
belong to the type tor ch.FloatT ensor:
1 import  tor ch
2 x =  torch .  tensor([ 0.9  , 0.7  ])   #   Input
3 w =  torch .  tensor([
4 [ -0.3  , 0.15  ],
5 [ 0.32  , -0.91  ],
6 [ 0.37  , 0.47  ],
7 ])   #   W eights8 B =  torch .  tensor([ 0.002  ])   #   Bias
F IGURE  8.17: MLP operations  .
1
.Using P ython, c alculate the output of the MLP at the hidden l ayers H  1
...H  3  .
2
.Further to the a bove, you discover that at a certain point in time that the
weights between the hidden layers and the output layers  γ1  have the
following values:
1 w1  =  tor ch  .  tensor([
2 [  0.15  ,  -0.46  ,  0.59  ],
3 [  0.10  ,  0.32  ,  -0.79  ],
4 )
What is the value observed at the output nodes γ  1  ..γ  2  ?
3
.Assume now that a Softmax activation is applied to the output. What ar e
the r esulting values?
4
.Assume now that a cr oss-entr o py loss is applied to the output of the
Softmax  .
What ar e the r esulting values?
The theory of perceptrons
PRB-212  
  CH.PRB- 8.36.  If someone is quoted saying:
MLP networks ar e universal function appr oximators  .
What does he mean?PRB-213  
  CH.PRB- 8.37.
T rue or False  : the output of a per ceptr on is 0 or 1  .
PRB-214  
  CH.PRB- 8.38.
T rue or False  : A multi-layer per ceptr on falls under the category of
supervised machine learning  .
PRB-215  
  CH.PRB- 8.39.
T rue or False  : Th e accuracy of a per ceptr o n is calculated as the
number of corr ectly classified  samples divided by the total number of
incorr ectly classified samples  .
Learning logical gates
PRB-216  
  CH.PRB- 8.40.
The follo wing qu estions r efer to the SLP depicted in (  8.18  ). The weights
in the SLP ar e w  1  = 1 and w  2  = 1 r espectively . Ther e is a single hidden
node, H  1  . The bias term, B  1 equals
F IGURE  8.18: A single layer perceptron  .
1
.Assuming the inputs to the SLP in (  8.18  ) ar e
i x  1  = 0.0 and x  2  = 0.0
iix  1  = 0.0 and x  2  = 1.0
iii x  1  = 1.0 and x  2  = 0.0
iv x  1  = 1.0 and x  2  = 1.0
What is the value r esulting fr om the application the sum operator?
2
.Repeat t he abov e, assuming now that the bias term B  1 was amended and
equals −  0.25.
3
.Define what is the per ceptr on learning rule  .
4
.What was the most crucial differ ence between Rosenblatt’ s original
algorithm and Hinton’ s fundamental papers of 1986:
“  Learning representations by back-propagating errors ” [  22  ] and
2012:
“  ImageNet Classification with Deep Convolutional Neural Networks ”
[  18  ]?
5
.The AND logic gate [  7  ] is defined by the following table (  8.19  ):
F IGURE  8.19: Logical AND gate
Can a per ceptr o n with only two  inputs and a single output function as
an A ND logic gate? If so, fin d the weights and the thr esho ld and
demonstrate the corr ectness of your answer using a truth table  .8.2.5   Activation functions (rectification)
W e conc entrate only on the mo st commonly used activation functions, those
which the reader is more likely to encounter or use during his daily work.
Sigmoid
PRB-217  
  CH.PRB- 8.41.
The Sigmoid  
  , also commonly known as the
logistic function (Fig.  8.20  ), is widely used in binary classification and as a
neur on activation function in artificial neural networks. T ypically , during the
training of an AN N, a Sigmoid la yer applies the Sigmoid function to elements
in the forwar d pass, while in the backwar d pass the chain rule is being
utilized as part of the backpr opa gation algorithm. In  8.20  the constant c was
selected arbitrarily as 2 and 5 r espectively  .
F IGURE  8.20: Examples of two sigmoid functions and an approximation  .
Digital  hardware  implementations of the sigmoid function do exist but
they ar e  expensi ve to compute a nd ther efor e several appr oxima tion methods
wer e intr oduced by the r esear ch community . The method by [  10  ] uses the
following formulas to appr oximate the exponential function:Based on this formulation, one can calculate the sigmoid function as:
1
.Code snippet  8.21  pr ovides a pur e C++ based (e.g . not using Autograd)
implementation of the forwar d pass for the Sigmoid function. Implement
the back war d pass that dir ectly c omputes the analytical gradients in C++
using Libtor ch [  19  ] style tensors  .
1 #include     <tor ch/script.h>
2 #include     <vector>
3
4 torch ::  T ensor sigmoid001( const  torch ::  T ensor &  x ){
5 torch ::  T ensor sig = 1.0  / ( 1.0 +  torch ::  exp(( -  x)));
6 r eturn  sig;
7 }
F IGURE  8.21: Forward pass for the Sigmoid function using Libtorch
2
.Code snippet  8.22  pr ovides a skeleton for printing the values of the
sigmoid and its derivative for a range of values contained in the vector v .
Complete the code (lines 7-8) so that the values ar e printed  .
1 #include     <tor ch/script.h>
2 #include     <vector>
3 int    main  ()    {
4 std ::  vector <  float  >  v{ 0.0  , 0.1  , 0.2  , 0.3  ,
↪     0.4  , 0.5  , 0.6  , 0.7  , 0.8  , 0.9  , 0.99  };
5 for     ( auto  it =  v .begin(); it !=  v .end(); ++  it)   {
6 torch ::  T ensor t0 = torch ::  tensor(( *  it));
7 ...
8 ...
9 }
1
0}.
F IGURE  8.22: Evaluation of the sigmoid and its derivative using Libtorch
3
.Manually derive the derivative of eq.  8.27  , e.g:
4
.Implement both the forwar d pass for the Sigmoid function appr oximation
eq.  8.27  that dir ectly computes the analytical gradients in C++ using
Libtor ch [  19  ]  .
5
.Print the values of the Sigm oid function and the Sigmoid function
appr oximation eq.  8.27  for the following vector:
T anh
PRB-218  
  CH.PRB- 8.42.
The Hyperbolic tangent nonlinearity , or the tanh function (Fig.  8.23  ),
is a widely used neur on activation function in artificial neural networks:
F IGURE  8.23: Examples of two tanh functions  .
1
.Manually derive the derivative of the tanh function  .
2
.Use the PyT or ch based tor ch.autograd.Function class to implement a
custom Functio n that implements the forwar d pass for the tanh
function in Python  .
PRB-219  
  CH.PRB- 8.43.
The code snippet in  8.24  makes use of the tanh function  .
1 import  tor ch
2
3 nn001 = nn .  Sequential(
4 nn .  Linear( 200  , 512  ),
5 nn .  T anh(),
6 nn .  Linear( 512  , 512  ),
7 nn .  T anh(),
8 nn .  Linear( 512  , 10  ),
9 nn .  LogSoftmax(dim =1  )
1
0)F IGURE  8.24: A simple NN based on tanh in PyT orch  .
1
.What type of a neural network does nn001 in  8.24  r epr esent?
2
.How many hidden layers does nn001 have?
PRB-220  
  CH.PRB- 8.44.
Y our friend, a veteran of the DL community claims that MLPs based
on tanh activation function, have a symmetry ar ound 0 and consequently
cannot be saturated. Saturation,  so he claims is a phenomenon typical of
the top hidden layers in sigmoid based MLPs. Is he right or wr ong?
PRB-221  
  CH.PRB- 8.45.
If we  ini tialize the weights of a tanh based NN, which of the foll owing
appr oaches will lead to the  vanishing gradients problem  ?  .
i Using the normal distribution, with parameter initialization m ethod
as suggested by Kaiming [  14  ]  .
ii Using th e unifor m distribution, with parameter initialization m ethod
as suggested by Xavier Glor ot [  9  ]  .
iii Initialize all parameters to a constant zer o value  .
PRB-222  
  CH.PRB- 8.46.
Y ou friend, who is experimenti ng with the tanh activation function
designed a small CNN with onl y one hidden layer and a linear output (
8.25  ):
F IGURE  8.25: A small CNN composed of tanh blocks  .He initialized all the weights  and biases (biases not show n for
br evity) to zer o. What is the  most significant design flaw in his
ar chitectur e?
Hint: think about back-pr opagation  .
ReLU
PRB-223  
  CH.PRB- 8.47.
The r ect ified line ar unit, or ReLU  g  ( z  ) = max{0, z  } is th e de fault for
many CNN ar chitectur es. It is defined by the following function:
Or:
1
.In what sense is the ReLU better than traditional sigmoidal activation
functions?
PRB-224  
  CH.PRB- 8.48.
Y ou ar e experim enting with the ReLU activation function, and you
design a small CNN (  8.26  ) which accepts an RGB image as an input.
Each CNN kernel is denoted by w  .
F IGURE  8.26: A small CNN composed of ReLU blocks  .
What is the shape of the r esulting tensor W ?PRB-225  
  CH.PRB- 8.49.
Name the following activation function wher e a  ∊  (0, 1):
Swish
PRB-226  
  CH.PRB- 8.50.
In many  intervi ews, you will be  given a paper that you have never
encounter ed befor e, and be r equir ed to r ead and subsequently discuss it.
Please r ead  Searching for Activation Functions [  21  ] befor e attempting
the questions in this question  .
1
.In [  21  ], r esear chers employed an aut omatic pipeline for sear ching
what exactly?
2
.What types of functions did the r esear chers include in their sear ch
space?
3
.What wer e the main findings of their r esear ch and why wer e the
r esults surprising?
4
.W rite the formulae for the Swish activation function  .
5
.Plot the Swish activation function  .
8.2.6   Performance Metrics
Comparing dif ferent machine learning models, tuning hyper parameters and
learning rates, finding optimal a ugmentations, are all important steps in ML
research. T ypically our goal is to find the best  model with the lowest errors
on both the training and valida tion sets. T o do so we need to be able to
measure the performance  of ea ch approach/model/parame ter setting etc. andcompare those measures. Fo r valuable reference, read: “ Evaluating
Learning Algorithms: A Classification Perspective  ” [ 22  ]
Confusion matrix, precision, recall
PRB-227  
  CH.PRB- 8.51.
Y ou des ign a binary classif ier for detecting the pr esence of
malfunctioning temperatur e sensors. Non-malfunctioning (N) devices ar e
the m ajority class in the trainin g corpus. While running infer ence on an
unseen t est-set, you discover that the Confusion Metrics (CM) has the
following values  8.27  :
F IGURE  8.27: A confusion m etrics for functioning (N) temperature sensors. P stands fo r
malfunctioning devices  .
1
.Find: TP , TN, FP , FN and corr ectly label the numbers in table  8.27  .
2
.What is the accuracy of the model?
3
.What is the pr ecision of the model?
4
.What is the r ecall of the model?
ROC-AUC
The area under the receiver operating characteristic (ROC) curve, 8.71
known as the AUC, is currently considered to be the standard method to
assess the accuracy of predictive distribution models.F IGURE  8.28: Receiver Operating Characteristic curve  .
PRB-228  
  CH.PRB- 8.52.
Complete the following sentences:
1
.Receiver Operating Characteristics of a classifier show s its
performance as a trade off between [...] and [...]  .
2
.It is a pl ot of [...] vs. the [...]. In  place of [...], one could also us e [...]
which ar e essentially {1 -  ‘true negatives’ }  .
3
.A ty pical ROC c urve has a conc ave shape with [...] as the beginning
and [...] as the end point
4
.The ROC curve of a ‘random guess classifier ’, when the classi fier is
completely conf used and cannot at all distinguish between the two
classes, has an AUC of [...] wh ich is the [...] line in an ROC  curve
plot  .
PRB-229  
  CH.PRB- 8.53.
The code  8.30  and Figur e  8.29  ar e the output fr om running
XGBOOST for a binary classification task  .F IGURE  8.29: RUC AUC
1 XGBClassifier(base_score =0.5  , colsample_bylevel =1  ,
↪     colsample_bytree =0.5  ,
2 gamma =0.017  , learning_rate =0.15  , max_delta_step =0  , max_depth =9  ,
3 min_child_weight =3  , missing =  None  , n_estimators =1000  , nthread =-1  ,
4 objective =  'binary:logistic'  , reg_alpha =0  , reg_lambda =1  ,
5 scale_pos_weight =1  , seed =0  , silent =1  ,
↪     subsample =0.9  )shape:( 316200  , 6  )
6
7 >  ROC AUC: 0.984439608912
8 >  LOG LOSS: 0.0421598347226
F IGURE  8.30: XGBOOST for binary classification  .
How would you describe the r esults of the classification?  .
8.2.7   NN Layers, topologies, blocks
CNN arithmetics
PRB-230  
  CH.PRB- 8.54.
Given an input of size of n  × n, filters of size f  × f and a stride of s
with padding of p, what is the output dimension?PRB-231  
  CH.PRB- 8.55.
Referring the code snippet in Fig. (  8.31  ), answer the following
questions r egar ding the VGG1 1 ar chitectur e [  25  ]:
1 import  tor chvision
2 import  tor ch
3 def  main  ():
4 vgg1 1 =  torchvision .  models .  vgg1 1(pretrained =  T rue  )
5 vgg_layers =  vgg1 1 .  features
6 for  param in  vgg_layers .  parameters():
7 param .  requires_grad =  False
8
9 example =  [torch .  rand( 1  , 3  , 224  , 224  ),
1
0torch .  rand( 1  , 3  , 512  , 512  ),
1
1torch .  rand( 1  , 3  , 704  , 1024  )]
1
2vgg1 1 .  eval()
1
3for  e in  example:
1
4out =  vgg_layers(e)
1
5print  (out .  shape)
1
6if  __name__  ==  "__main__"  :
1
7main()
F IGURE  8.31: CNN arithmetics on the VGG1 1 CNN model  .
1
.In each  case for the input variable  example  , determine the
dimensions of the tensor which is the output of applying the V GG1 1
CNN to the r espective input  .
2
.Choose the corr ect option. The last layer of the VGG1 1 ar chit ectur e
is:
i Conv2dii MaxPool2d
iii ReLU
PRB-232  
  CH.PRB- 8.56.
Still r efe rring the code snippet i n Fig. (  8.31  ), and specifically to line
7, the code is amended so that the line is r eplaced by the line:
vgg_layers=vgg1 1.featur es[:3]  .
1
.What typ e of block is now r epr esented by the new line? Print it  using
PyT or ch  .
2
.In each  case for the input variable  example  , determine the
dimensions of the tensor which is the output of applying the block:
vgg_layers=vgg1 1.featur es[:3]  to the r espective input  .
PRB-233  
  CH.PRB- 8.57.
T able (  8.1  ) pr esents an incomplete listing of the of the VGG1 1
ar chitectur e [  25  ]. As  depicted, for each layer the number of filters (i. e.,
neur ons with unique set of parameters) ar e pr esented  .
Layer #Filters
conv4_3 512
fc6 4,096
fc7 4,096
output 1,000
T ABLE  8.1: Incomplete listing of the VGG1 1 architecture  .
Complete the missing parts r egar ding the dimensions and arithmetics
of the VGG1 1 CNN ar chitectur e:
1
.The VGG1 1 ar chitectur e consists of [...] convolutional layers  .
2 Each convolutional layer is followed by a [...] activation function, and. five [...] operations thus r educin g the pr eceding featur e map siz e by a
factor of [...]  .
3
.All convolutional layers have a [...] kernel  .
4
.The first convolutional layer pr oduces [...] channels  .
5
.Subsequently as the network deepens, the number of channels [...]
after each [...] operation until it r eaches [...]  .
Dropout
PRB-234  
  CH.PRB- 8.58.
A Dr opout layer [  26  ] (Fig.  8.32  ) is com monly u sed to r egularize a
neural n etwork model by randomly equating several outputs (the cr ossed-
out  hidden  node H) to 0  .
F IGURE  8.32: A Dropout layer (simplified form)  .
For instance, in PyT or ch [  20  ], a Dr opout layer is declar ed as
follows (  8.2  ):
1 import  tor ch
2 import  tor ch.nn  as  nn
3 nn .  Dropout( 0.2  )
C ODE  8.2: Dropout in PyT orchWher e nn.Dr opout  (0.2) (Line #3 in  8.2  ) indicates that the pr obability
of zer oing an element is  0.2.
F IGURE  8.33: A Bayesian Neural Network Model
A new data scie ntist in your te am suggests the following pr ocedur e
for a  Dr opout la yer which is ba sed on Bayesian principles. Each of the
neur ons  θn  in the neural network in (Fig.  8.33  ) may dr op (or not)
independently of each other exactly like a Bernoulli trial.
During the training of a neural network, the Dr opout layer ran domly
dr ops out outputs of the pr evious layer , as indicated in (Fig.  8.32  ). Her e,
for illustration purposes, all fou r neur ons ar e dr opped as depic ted by the
cr ossed-out  hidden  nodes H  n  .
1
.Y ou ar e inter este d in the pr oport ion  θ of dr opped-out neur ons. Assume
that the chance of dr op-out  , θ, is  the same  for each neur on (e.g. a
uniform prior  for  θ ). Compute the  posterior  of  θ.
2
.Describe the similarities of dr opout to bagging  .
PRB-235  
  CH.PRB- 8.59.
A co -worker claims he discover ed an equivalence theor em wher e, two
consecutive Dr o pout layers [  26  ] c an be r eplace d and r epr esented by a
single Dr opout layer  8.34  .F IGURE  8.34: T wo consecutive Dropout layers
Hi r eali zed two consecutive layers in PyT or ch [  20  ], declar ed as
follows (  8.3  ):
1 import  tor ch
2 import  tor ch.nn  as  nn
3 nn .  Sequential(
4 nn .  Conv2d( 1024  ,   32  ),
5 nn .  ReLU(),
6 nn .  Dropout(p =  P ,   inplace =  T rue  ),
7 nn .  Dropout(p =  Q,   inplace =  T rue  )
8 )
C ODE  8.3: Consequtive dropout in PyT orch
Wher e nn.Dr opout  (0.1) (Line #6 in  8.3  ) indicates that the pr obability
of zer oing an element is  0.1.
1
.What do you think about his idea, is he right or wr ong?
2
.Either p r ove tha t he is right or pr ovide a single example that r efutes
his theor em  .
Convolutional Layer
The con volution layer is probably one of the most important layers in the
theory and practice of modern deep learning and computer vision in
particular .
T o study the optimal number of convolutional layers for the
classification of two dif ferent types of the Ebola virus, a researcher designsa binary  classifi cation pipeline using a small CNN with only a few layers
( 8.35  ):
F IGURE  8.35: A CNN based classification system  .
Answer the following questions while referring to ( 8.35  ):
PRB-236  
  CH.PRB- 8.60.
If he  uses the following filter for the convolutional operation, what
would be the r e sulting tensor after the application of the conv olutional
layer?
F IGURE  8.36: A small filter for a CNN
PRB-237  
  CH.PRB- 8.61.
What would be the r esulting ten sor after the application of the ReLU
layer (  8.37  )?F IGURE  8.37: The result of applying the filter  .
PRB-238  
  CH.PRB- 8.62.
What would be the r esulting tensor after the application of the
MaxPool layer (  8.76  )?
Pooling Layers
A po oling layer transforms the o utput of a convolutional layer , and neurons
in a poo ling lay er accept the ou tputs of a number of adjacent feature maps
and mer ge their outputs into a single number .
MaxPooling
PRB-239  
  CH.PRB- 8.63.
The following input  8.38  is subjected to a MaxPool2D(2,2) operation
having  2 × 2 max-pooling filter with a stride of 2 and no padding at all  .F IGURE  8.38: Input to MaxPool2d operation  .
Answer the following questions:
1
.What is the most common use of max-pooling layers?
2
.What is the r esult of applying the MaxPool2d operation on the input?
PRB-240  
  CH.PRB- 8.64.
While r eading a paper about the MaxPool operation, you encounter
the following code snippet  9.1  of a  PyT or ch m odule that the authors
implemented. Y ou download their pr e-trained model, and evaluate its
behaviour during infer ence:
1 import  tor ch
2 fr om  tor ch  import  nn
3 class  MaxPool001  (nn .  Module):
4 def  __init__  ( self  ):
5 super  (MaxPool001, self  ) .  __init__  ()
6 self  .  math =  torch .  nn .  Sequential(
7 torch .  nn .  Conv2d( 3  , 32  , kernel_size =7  , padding =2  ),
8 torch .  nn .  BatchNorm2d( 32  ),
9 torch .  nn .  MaxPool2d( 2  , 2  ),
1
0torch .  nn .  MaxPool2d( 2  , 2  ),
1 )1
1
2def  forward  ( self  ,   x):
1
3print     (x .  data .  shape)
1
4x =  self  .  math(x)
1
5print  (x .  data .  shape)
1
6x =  x .  view(x .  size( 0  ), -1  )
1
7print  ( "Final shape:  {}  "  ,x .  data .  shape)
1
8r eturn  x
1
9model =  MaxPool001()
2
0model .  eval()
2
1x =  torch .  rand( 1  , 3  , 224  , 224  )
2
2out =  model .  forward(x)
C ODE  8.4: A CNN in PyT orch
The ar chitectur e is pr esented in  9.2  :
F IGURE  8.39: T wo consecutive MaxPool layers  .
Please run the code and answer the following questions:
1
.In MaxPool2D(2,2), what ar e the parameters used for?
2
.After running line 8, what is the r esulting tensor shape?
3
.Why does line 20 exist at all?
4
.In line 9, ther e is a MaxPool2D(2,2) operation, followed by yet a
second MaxPool2D(2,2). What is the r esulting tensor shape after
running line 9? and line 10?
5
.A friend  who saw the PyT or ch implementation, suggests that l ines 9
and 10 may be r eplaced by a single MaxPool2D(4,4,) operation while
pr oducing the exact same r esults. Do you agr ee with him? Amend the
code and test your assertion  .
Batch normalization, Gaussian PDF
Recommended readings for th is topic are “  Batch Normalization:
Accelerating Deep Network T raining by Reducing Internal C ovariate
Shift  ” [  16  ] and “ Delving deep into r ectifiers: Su rpassing human-level
performance on imagenet classification  ” [ 14  ].
A discussion of batch normaliz ation (BN) would not be complete
without a discussion of the Gaussian normal distribution. Though it
would b e instructive to develop the forward and backwards functions for
a BN operation  from scratch, it would also be quite complex. As an
alternative we discuss several  aspects of the BN operation while
expanding on the Gaussian distribution.
The Gaussian distribution
PRB-241  
  CH.PRB- 8.65.
1
.What is batch normalization?2
.The normal distribution is defined as follows:
Generally i.i.d. X  ∼  N  ( µ, σ  2 ) however BN uses the standar d
normal distribution. What mean and variance does the standar d
normal distribution have?
3
.What is the mathematical pr ocess of normalization?
4
.Describe, how normalization works in BN  .
PRB-242  
  CH.PRB- 8.66.
In python, the pr obability density function for a normal
distribution is given by  8.40  :
1 import  scipy
2 scipy .  stats .  norm .  pdf(x, mu, sigma)
F IGURE  8.40: Normal distribution in Python  .
1
.W ithout using Scipy , implement the normal distribution fr om
scratch in Python  .
2
.Assume, you want to back pr op agate on the normal distribution,
and ther efor e you need the deriv ative. Using Scipy write a function
for the derivative  .
BN
PRB-243  
  CH.PRB- 8.67.
Y our frie nd, a novice data scien tist, uses an RGB image (  8.41  )
which he then subjects to BN as part of training a CNN  .F IGURE  8.41: A convolution and BN applied to an RGB image  .
1
.Help him understand, during BN, is the normalization applied
pixel-wise or per colour channel?
2
.In the PyT or ch implementation, he made a silly mistake  8.42  , help
him identify it:
1 import  tor ch
2 fr om  tor ch  import  nn
3 class  BNl001  (nn .  Module):
4 def  __init__  ( self  ):
5 super  (BNl001, self  ) .  __init__  ()
6 self  .  cnn =  torch .  nn .  Sequential(
7 torch .  nn .  Conv2d( 3  , 64  , kernel_size =3  , padding =2  ),
8 )
9 self  .  math =  torch .  nn .  Sequential(
1
0torch .  nn .  BatchNorm2d( 32  ),
1
1torch .  nn .  PReLU(),
1
2torch .  nn .  Dropout2d( 0.05  )
1
3)
1
4def  forward  ( self  , x):1
5...
F IGURE  8.42: A mistake in a CNN
Theory of CNN design
PRB-244  
  CH.PRB- 8.68.
T rue or false:  An activation function applied after a Dr opout, is
equivalent to an activation function applied befor e a dr opout  .
PRB-245  
  CH.PRB- 8.69.
Which o f the following cor e building blocks may be used to
construct CNNs? Choose all the options that apply:
i Pooling layers
ii Convolutional layers
iii Normalization layers
iv Non-linear activation function
v Linear activation function
PRB-246  
  CH.PRB- 8.70.
Y ou ar e designing a CNN which has a single BN layer . Which of
the f ollowing cor e CNN designs ar e valid? Choose all the option s that
apply:
i CONV  → act  → BN → Dr opout  → . . .
ii CONV  → act  → Dr opout  → BN → . . .
iii CONV  → BN  → act  → Dr opout  → . . .
iv BN  → CONV  → act  → Dr opout  → . . .v CONV  → Dr opout  → BN → act  → . . .
vi Dr opout  → CONV  → BN → act  → . . .
PRB-247  
  CH.PRB- 8.71.
The following operator is known as the Hadamar d pr oduct:
Wher e:
A scientist, constructs a Dr opout layer using the following
algorithm:
i Assign a pr obability of p for zer oing the output of any neur on  .
ii Accept an input tensor T , having a shape S
iii Generate a new tensor T  ‘ ∊  {0, 1}S
iv Assign each element in T  ‘ a randomly and independently sampled
value fr om a Bernoulli distribution:
v Calculate the OUT tensor as follows:
Y ou ar e surprised to find out th at his last step is to multiply the
output of a dr opout layer with:
Explain what is the purpose of multiplying by the term  
PRB-248  
  CH.PRB- 8.72.
V isualized in (  8.43  ) fr om a high-level view , is an MLP which
implements a well-known idiom in DL  .
F IGURE  8.43: A CNN block
1
.Name the idiom  .
2
.What can this type of layer learn?
3
.A fellow data scientist suggests amending the ar chitectur e as
follows (  8.44  )
F IGURE  8.44: A CNN block
Name one disadvantage of this new ar chitectur e  .
4
.Name one CNN ar chitectur e wher e the input equals the output  .CNN residual blocks
PRB-249  
  CH.PRB- 8.73.
Answer the following questions r egar ding r esidual networks ([  13
])  .
1
.Mathematically , the r esidual block may be r epr esented by:
What is the function  ℱ  ?
2
.In one sentence , what was the  main idea behind deep r esidu al
networks (ResNets) as intr oduced in the original paper ([  13  ])?
PRB-250  
  CH.PRB- 8.74.
Y our friend was thinking about ResNet blocks, and tried to
visualize them in (  8.45  )  .
F IGURE  8.45: A resnet CNN block
1
.Assuming a r esidual of the for m  y  = x  + ℱ  ( x  ), complete the
missing parts in Fig. (  8.45  )  .
2
.What does the symbol  ⊕  denotes?3
.A fellow  data scientist, who had coffee with you said that r esid ual
blocks m ay com pute the  identity function  . Explain what he meant
by that  .
8.2.8   T raining, hyperparameters
Hyperparameter optimization
PRB-251  
  CH.PRB- 8.75.
A certain training pipeline for the classification of lar ge imag es
(1024 x 1024) uses the following Hyperparameters (  8.46  ):
Hyperparameter V alue
Initial learning rate 0.1
W eight decay 0.0001
Momentum 0.9
Batch size 1024
1 optimizer =  optim .  SGD(model .  parameters(), lr =0.1  ,
2 momentum =0.9  ,
3 weight_decay =0.0001  )
4 ...
5 trainLoader =  torch .  utils .  data .  DataLoader(
6 datasets .  LARGE( '../data'  , train =  T rue  , download =  T rue  ,
7 transform =  transforms .  Compose([
8 transforms .  T oT ensor(),
9 ])),
1
0batch\_size =1024  , shuf fle =  T rue  )
F IGURE  8.46: Hyperparameters  .
In y our opinion, what could po ssibly go wr ong with this traini ng
pipeline?PRB-252  
  CH.PRB- 8.76.
A junio r data scientist in y our team who is inter ested in
Hyperparameter tuning, wr ote the following code (  8.5  ) for spiting
his corpus into two distinct sets and fitting an LR model:
1 fr om  sklearn.model_selection  import  train_test_split
2 dataset =  datasets .  load_iris()
3 X_train, X_test, y_train, y_test =
4 train_test_split(dataset .  data, dataset .  tar get, test_size =0.2  )
5 clf =  LogisticRegression(data_norm =12  )
6 clf .  fit(X_train, y_train)
C ODE  8.5: T rain and V alidation split.
He t hen evaluated the performance of the trained model on the X
test  set  .
1
.Explain why his methodology is far fr om perfect  .
2
.Help him  r esolve the pr oblem by utilizing a differ ence splitti ng
methodology  .
3
.Y our friend now amends the code an uses:
1 clf  =  GridSear chCV(method, params, scoring  =  'r oc_auc'  , cv  =5  )
2 clf  .  fit(train_X, train_y)
Explain why his new appr oach may work better  .
PRB-253  
  CH.PRB- 8.77.
In the context of Hyperpara meter optimization, explain the
differ ence between grid sear ch and random sear ch  .
Labelling and bias
Recommended reading:“ Added value of double r eading in diagnostic radiology , a systematic
r eview  ” [  8  ].
PRB-254  
  CH.PRB- 8.78.
Non-invasive me thods that for ecast the existence of lung nodule s (
8.47  ), is a pr ecurso r to lung cancer . Y et, in spite of acquisition
standar dization attempts, the manual detection of lung nodules still
r emains pr edisposed to inter mechanical and observer varia bility .
What is mor e, it is a highly laborious task  .
F IGURE  8.47: Pulmonary nodules  .
In th e majority o f cases, the training data is manually labelled by
radiologists who make mistak es. Imagine you ar e working on a
classification pr oblem and hir e two radiologists for lung cancer
scr eening based  on low-dose C T (LDCT). Y ou ask them to label the
data, the first ra diologist labels only the training set and the s econd
the validation set. Then you hir e a thir d radiologist to label the test set
.
1
.Do you think ther e is a design flow in the curation of the data sets?
2
.A friend suggests that all ther e radiologists r ead all the scans and
label the m inde pendently thus cr eating a majority vote. What do
you think about this idea?V alidation curve ACC
PRB-255  
  CH.PRB- 8.79.
Answer the following questions r egar ding the validation cur ve
visualized in (  8.48  ):
F IGURE  8.48: A validation curve  .
1
.Describe in one sentence, what is a validation curve  .
2
.Which hyperparameter is being used in the curve?
3
.Which well-know n metric is being used in the curve? Which other
metric is commonly used?
4
.Which positive phenomena happens when we train a NN longer?
5
.Which negative phenomena hap pens when we train a NN longer
than we should?
6
.How this negative phenomena is r eflected in  8.48  ?
V alidation curve LossPRB-256  
  CH.PRB- 8.80.
Refer to the validation log-loss curve visualized in (  8.49  ) and
answer the following questions:
F IGURE  8.49: Log-loss function curve  .
1
.Name th e pheno mena that starts happening right after the mar king
by the letter E and describe why it is happening  .
2
.Name thr ee differ ent weight initialization methods  .
3
.What is the main idea behind these methods?
4
.Describe several ways how this phenomena can be alleviated  .
5
.Y our friend, a f ellow data-scientist, inspects the code and sees the
following Hyper -parameters ar e being used:
Hyperparamete V aluer
Initial LR 0.00001
Momentum 0.9
Batch size 1024
He t hen tells you that the learning rate (LR) is constant and
suggests amending the training pipeline by adding the following
code (  8.50  ):
1 scheduler =  optim .  lr_scheduler .  ReduceLROnPlateau(opt)
F IGURE  8.50: A problem with the log-loss curve  .
What do you think about his idea?
6
.Pr ovide one r eason against the use of the log-loss curve  .
Inference
PRB-257  
  CH.PRB- 8.81.
Y ou finished training a face r ecognition algorithm, which uses a
featur e v ector of 128 elements. During infer ence, you notice that the
performance is not that good. A friend tells you that in computer
vision faces ar e gather ed in various poses and perspective s. He
ther efor e suggests that during infer ence you would augment the
incoming face five times, run inf er ence on each augmented image and
then fuse the output pr obability distributions by averaging  .
1
.Name the method he is suggesting  .
2
.Pr ovide several examples of augmentation that you might use
during infer ence  .
PRB-258  
  CH.PRB- 8.82.Complete the sentence: If the t raining loss is insignificant while
the test loss is s ignificantly hig her , the network has almost certainly
learned featur es which ar e not p r esent in an [...] set. This phenomena
is r eferr ed to as [...]
8.2.9   Optimization, Loss
Stochastic gradient descent, SGD
PRB-259  
  CH.PRB- 8.83.
What do es the term stochastic in SGD actually mean? Does it u se
any random number  generator?
PRB-260  
  CH.PRB- 8.84.
Explain why in SGD, the numbe r of epochs r equir ed to surpass  a
certain loss thr eshold  increases as the batch size decr eases?
Momentum
PRB-261  
  CH.PRB- 8.85.
How does momentum work? Explain the r ole of exponential decay
in the gradient descent update rule  .
PRB-262  
  CH.PRB- 8.86.
In your training  loop, you ar e using SGD and a logistic activati on
function which is known to suff er fr om the phenomenon of satu rated
units  .
1
.Explain the phenomenon  .
2
.Y ou swi tch to using the tanh activation instead of the logistic
activation, in your opinion does the phenomenon still exists?
3
.In your  opinion, is using the  tanh function makes the SGD
operation to conver ge better?PRB-263  
  CH.PRB- 8.87.
Which of the following statements holds true?
i In st ochastic gradient descent we first calculate the gradient and
only then adjust weights for each data point in the training set  .
ii In stoch astic gradient descent, the gradient for a single sample is
not so differ ent fr om the actual gradient, so this gives a mor e
stable value, and conver ges faster  .
iii SGD usually avoids the trap of poor local minima  .
iv SGD usually r equir es mor e memory  .
Norms, L1, L2
PRB-264  
  CH.PRB- 8.88.
Answer the following questions r egar ding norms  .
1
.Which norm does the following equation r epr esent?
2
.Which formulae does the following equation r epr esent?
3
.When yo ur r ead that someone penalized the L2 norm, was the
euclidean or the Manhattan distance involved?
4
.Compute both the Euclidean and Manhattan distance of the
vectors: x  1 = [6, 1, 4, 5] and x  2 = [2, 8, 3, −  1].
PRB-265  
  CH.PRB- 8.89.Y ou ar e pr ovided with a pur e P ython code implementation of the
Manhattan distance function (  8.51  ):
1 fr om  scipy  import  spatial
2 x1 =  [ 6  , 1  , 4  , 5  ]
3 x2 =  [ 2  , 8  , 3  , -1  ]
4 cityblock =  spatial .  distance .  cityblock(x1, x2)
5 print  ( "Manhattan:"  , cityblock)
F IGURE  8.51: Manhattan distance function  .
In many  cases, and for lar ge v ectors in particular , it is better  to
use a GPU for implementing numerical computations. PyT or ch has
full support for GPU’ s (and its my favourite DL library ...), us e it to
implement the Manhattan distance function on a GPU  .
PRB-266  
  CH.PRB- 8.90.
Y our fri end is training a logist ic r egr ession model for a binary
classification pr oblem using the L2 loss for optimization. Explain to
him why this is  a bad choice and which loss he should be using
instead  .
8 . 3   S o l u t i o n s
8.3.1   Cross V alidation
On the significa nce of cross va lidation and stratification in particular ,
refer to “ A study of cr o ss-validation a nd bootstrap for accuracy
estimation and model selection  ” [ 17  ].
CV approaches
SOL-177  
  CH.SOL- 8.1.
The first appr oach is a leave-one-out CV  (LOOCV)  and the
second is a  K-fold  cr oss-validation appr oach  .
SOL-178  
  CH.SOL- 8.2.
Cr oss V alidation is a cornerstone in machine learning, allowing
data scientists to take full g ain of r estricted training data. In
classification, effective cr oss validation is essential to making the
learning task efficient and mor e  accurate. A fr equently used fo rm of
the technique is identified as K-fold cr oss validation. Using this
appr oach, the full data set is divided into K randomly selected  folds,
occasionally  stratified, meaning that each fold has roughly the same
class dis tribution as the overall data set  . Subsequently , for each fold,
all the other  ( K −  1) folds ar e used for training, while the pr esent fold
is used for testin g. This pr ocess guarantees that sets used for te sting,
ar e not used by a classifier that also saw it during training  .
K-Fold CV
SOL-179  
  CH.SOL- 8.3.
T rue. W e never utilize the test set during a K-fold CV pr ocess  .
SOL-180  
  CH.SOL- 8.4.
T rue. Th is is the average of the individual err ors of K estimates of
the test err or:
SOL-181  
  CH.SOL- 8.5.
The corr ect answer is: A 5-fold cr oss-validation appr oach r esults
in 5-di ffer ent model instance s being fitted. It is a common
misconception t o think that in a K-fold appr oach the same model
instance is r epeatedly used. W e must cr eate a new model instance in
each fold  .
SOL-182  
  CH.SOL- 8.6.The co rr ect answer is: we compute the cr oss-validati on
performance as the  arithmetic mean  over th e K performance
estimates fr om the validation sets  .
Stratification
SOL-183  
  CH.SOL- 8.7.
The corr ect answer is: 3-fold C V . A k-fold cr oss-validation is a
special c ase of cr oss-validation wher e we iterate over a datase t set k
times. In  each r ound, we split the dataset into k parts: one part is used
for validation, and the r emain ing k −  1 parts ar e mer ged into a
training subset for model evaluation. Stratification is used to ba lance
the classes in the training and  validation splits in cases whe r e the
corpus is imbalanced  .
LOOCV
SOL-184  
  CH.SOL- 8.8.
1
.T rue  : In (LOOCV) K  = N the full sample size  .
2
.False  : Th er e i s no way of a-priori fin ding an optimal value for K,
and the r elationship  between the actual sample size and the
r esulting accuracy is unknown  .
8.3.2   Convolution and correlation
The convolution operator
SOL-185  
  CH.SOL- 8.9.
1
.This is the definition of a convolution operation on the two signals
f and g  .
2
.In image pr ocessing, the term g  ( t  ) r epr esents a  filtering  kernel  .SOL-186  
  CH.SOL- 8.10.
1
.T rue. These operations have two key featur es: they ar e sh ift
invariant, and they ar e linear .  Shift invariance means that we
perform the same operation at e very point in the image. Linearity
means that this operation is lin ear , that is, we r eplace every pi xel
with a linear combination of its neighbours
2
.T rue. See for instance Eq. (  8.3  )  .
3
.T rue  .
The correlation operator
SOL-187  
  CH.SOL- 8.1 1.
1
.T rue  .
2
.T rue  .
SOL-188  
  CH.SOL- 8.12.
A convolution operation is just like corr elation, except that we flip
over the filter both horizontally and vertically befor e corr elating  .
Padding and stride
Recommended reading : “ A guide to convolution arithmetic for deep
learning by V incent Dumoulin and Francesco V isin (2016)  ” [ 22  ].SOL-189  
  CH.SOL- 8.13.
1
.The V al id padding only uses values fr om the original inp ut;
however , when the data r esolut ion is not a multiple of the stride,
some boundary values ar e ignor ed entir ely in the featur e
calculation  .
2
.The Sam e padding ensur es that every input value is included, but
also adds zer os near the bound ary which ar e not in the origin al
input  .
SOL-190  
  CH.SOL- 8.14.
T rue. Contrast this with the two other types of convoluti on
operations  .
SOL-191  
  CH.SOL- 8.15.
SOL-192  
  CH.SOL- 8.16.
A is the corr ect choice  .
SOL-193  
  CH.SOL- 8.17.
A r epr esents the V ALID mode while B r epr esents the SAME mode  .
SOL-194  
  CH.SOL- 8.18.
1
.The r esulting output has a shape of  4 × 4.2
.Convolution operation
3
.By definition, convolutions in the  valid  mode, r e duce the size of the
r esulting input tensor  .
Kernels and filters
SOL-195  
  CH.SOL- 8.19.
1
.Flipping by 180 degr ees we get:
2
.The Sob el filter which is being fr equently used for edge detecti on
in classical computer vision  .
SOL-196  
  CH.SOL- 8.20.
The r esulting complexity is given by:
Convolution and correlation in python
SOL-197  
  CH.SOL- 8.21.
1
.Convolution operation:
1 import  numpy  as  np
2 def  convolution  (A,B):
3 l_A =  np .  size(A)
4 l_B =  np .  size(B)
5 C =  np .  zeros(l_A +  l_B -1  )
6
7 for  m in  np .  arange(l_A):
8 for  n in  np .  arange(l_B):
9 C[m +  n] =  C[m +  n] +  A[m] *  B[n]
1
0
1
1r eturn  C
F IGURE  8.52: Convolution and correlation in python
2
.Corr elation operation:
1 def  crosscorrelation  (A,B):
2 r eturn  convolution(np .  conj(A),B[:: -1  ])
F IGURE  8.53: Convolution and correlation in python
Separable convolutions
SOL-198  
  CH.SOL- 8.22.
1 No.Since images ar e usually stor ed as discr ete pixel values one. would have to use a discr ete  appr oximation of the Gaussian
function on the filtering mask befor e performing the convolution  .
2
.No  .
3
.Y es it is separ able, a factor that has gr eat implications. F or
instance, separa bility means that a 2D convolution can be r educed
to tw o consequent 1D convolutions r educing the computation al
runtime fr om  O (n2 m2 ) to  O (n2 m).
8.3.3   Similarity measures
Image, text similarity
SOL-199  
  CH.SOL- 8.23.
The algorithm pr esented in (  8.12  ) normalizes the input vector .
This is usually done prior to ap plying any other method to the vector
or befor e persisting a vector to a database of FVs  .
SOL-200  
  CH.SOL- 8.24.
1
.The algorithm pr esented in (  8.1  ) is one of the most commonly
used image similarity measur es and is entitled  cosine similarity  . It
can be applied to any pair of images  .
2
.The mathematical formulae behind it is:
The cosine similarity between two vectors:
u  = { u  1  , u  2  , . . ., u  N  } and v  = { v  1  , v  2  , . . ., v  N  } is defined
as:
Thus, the cosine similarity between two vectors measur es the
cosine of the angle  between the vectors irr espectiv e of their
magnitude. It is calculated as the dot pr oduct of two numeric
vectors, and is normalized by the pr oduct of the length of the
vectors  .
3
.The min imum and maximum values it can r eturn ar e 0 and 1
r espectively . Th us, a cosine similarity value which is close to  1
indicated a very high similarity while that close to 0 indicates a
very low similarity  .
4
.It r epr esents the negative distan ce in Euclidean space between the
vectors  .
Jacard similarity
SOL-201  
  CH.SOL- 8.25.
1
.The general formulae for the Jaccar d similarity of two sets is given
as follows:
2
.That is, the ratio of the size of the  intersection  of A and B to the
size of their  union  .
3
.The Jaccar d similarity equals:
4
.Given (  8.13  )
For the thr ee combinations of pairs above, we haveThe Kullback-Leibler Distance
SOL-202  
  CH.SOL- 8.26.
Each KLD corr esponds to the definition of:
i Jensen [  1  ]
ii Bennet [  2  ]
iii Bigi [  3  ]
iv Ziv [  29  ]
MinHash
Read the paper entitled Detecting near -duplicates for web crawling  [ 12  ]
and answer the following questions.
SOL-203  
  CH.SOL- 8.27.
A Hashi ng func tion (  8.54  ) ma ps a value into a constant length
string that can be compar ed with other hashed values  .F IGURE  8.54: The idea of hashing
The idea  behind hashing is tha t items ar e hashed into buckets,
such that  similar items  will hav e a higher pr obability o f hashing into
the  same buckets  .
The goal of Min Hash is to compute the Jaccar d similarity witho ut
actually computing the intersection and union of the sets, which
would be slower . The main idea behind MinHash is to de vise a
signatur e schem e such that the pr obability that ther e is a match
between the signatur es of two sets, S  1  and S  2  , is equal to the Jaccar d
measur e [  12  ]  .
SOL-204  
  CH.SOL- 8.28.
Locality-Sensitive Hashing (LSH) is a method which is used for
determining which items in a given set ar e similar . Rather than using
the n aive appr oach of comparin g all pairs of items within a set,  items
ar e h ashed into buckets, such that similar items will be mor e lik ely to
hash into the same buckets  .
SOL-205  
  CH.SOL- 8.29.
Maximise  .
8.3.4   Perceptrons
The Single Layer Perceptron
SOL-206  
  CH.SOL- 8.30.
Answer: one, one, feedback  .
SOL-207  
  CH.SOL- 8.31.
1
.T rue  .
2
.T rue  .
3  .
wher e w denotes the collection of all weights in the network, b
all t he b iases, n  is the total number of training inputs and a  ( x,
w , b  ) is the vector of outputs fr om the network which has
weights w , biases b and the input x  .
4  .
5
.Gradient descent  .
6
.The gradient  .
7
.Stochastic gradient descent. Batch size. Back-pr opagation  .
The Multi Layer Perceptron
SOL-208  
  CH.SOL- 8.32.
1
.This operation is a dot pr oduct with the given weights. Ther efor e:
2
.This ope ration (sum) is a dot pr oduct with the given weights  and
with the given bias added  . Ther efor e:
3
.Code snippet  8.55  pr ovides a pur e PyT or ch-based implementation
of the MLP operation  .
1 import  tor ch
2 # .type(tor ch.FloatT ensor)
3 x =  torch.tensor([ 0.9,0.7  ])
4 w =  torch.tensor([ -0.3,0.15  ])
5 B =  torch.tensor([ 0.001  ])
6 print  (torch .  sum(x *  w))
7 print  (torch .  sum(x *  w) +  B)
F IGURE  8.55: MLP operations  .
Activation functions in perceptrons
SOL-209  
  CH.SOL- 8.33.
1
.Since by definition:And the output of the linear sum operation was −  0.164 then,
the output out  2 = 0.
2
.Code snippet  8.56  pr ovides a pur e PyT or ch-based implementation
of the MLP operation  .
1 import  tor ch
2 x =  torch .  tensor([ 0.9,0.7  ])
3 w =  torch .  tensor([ -0.3,0.15  ])
4 B =  torch .  tensor([ 0.001  ])
5 print  (torch .  sum(x *  w))
6 print  (torch .  sum(x *  w) +  B)
7 print  (torch .  relu(torch .  sum(x *  w +  B)))
F IGURE  8.56: MLP operations  .
Back-propagation in perceptrons
SOL-210  
  CH.SOL- 8.34.  The answers ar e as follows:
1
.Non-differ entiable at 0  .
2
.Non-differ entiable at 0  .
3
.Even though for x  ≠ 0:
the function is still non-differ entiable at 0  .
4 Non-differ entiable at 0  ..
SOL-21 1  
  CH.SOL- 8.35.
1
.Fig  8.57  uses a loop (inefficient but easy to understand) to print
the values:
1 for  i in  range  ( 0  ,w .  size( 0  )):
2 print  (torch .  relu(torch .  sum(x *  w[i]) +  B))
3 >  tensor([ 0.  ])
4 >  tensor([ 0.  ])
5 >  tensor([ 0.6630  ])
F IGURE  8.57: MLP operations- values  .
2
.The values at each hidden layer ar e depicted in  8.58
F IGURE  8.58: Hidden layer values, simple MLP  .
3
.Fig  8.59  uses a loop (inefficient but easy to understand) to print
the values:
1 x1= torch .  tensor([ 0.0  , 0.0  , 0.6630  ]) # Input
2 w1= torch .  tensor([
3 [ 0.15  , -0.46  , 0.59  ],4 [ 0.10  , 0.32  , -0.79  ],
5 ]) .  type(torch .  FloatT ensor) # W eights
6 for  i in  range  ( 0  ,w1 .  size( 0  )):
7 print  (torch .  sum(x1 *  w1[i]))
8 >  tensor( 0.3912  )
9 >  tensor( -0.5238  )
F IGURE  8.59: MLP operations- values at the output  .
4
.W e can apply the Softmax function like so  8.60  :
1 x1 =  torch .  tensor([ 0.0  , 0.0  , 0.6630  ]) # Input
2 w1 =  torch .  tensor([
3 [ 0.15  , -0.46  , 0.59  ],
4 [ 0.10  , 0.32  , -0.79  ],
5 ]) .  type(torch .  FloatT ensor) # W eights
6 out1 =  torch .  tensor([[torch .  sum(x1 *  w1[ 0  ]) .  item()],
7 [torch .  sum(x1 *  w1[ 1  ]) .  item()]])
8 print  (out1)
9 yhat = torch .  softmax(out1, dim =0  )
1
0print  (yhat)
1
1>  tensor([[ 0.3912  ],
1
2[ -0.5238  ]])
1
3>  tensor([[ 0.7140  ],
1
4[ 0.2860  ]])
F IGURE  8.60: MLP operations- Softmax  .
5
.For the  cr oss- entr opy loss, we use the Softmax values a nd
calculate the r esult as follows:
The theory of perceptrons
SOL-212  
  CH.SOL- 8.36.
He means that theor etically [  6  ], a non-linear layer followed
by a linear laye r , can appr oximate any non-linear function w ith
arbitrary accur acy , pr ovided that ther e ar e enough non-line ar
neur ons
SOL-213  
  CH.SOL- 8.37.  T rue
SOL-214  
  CH.SOL- 8.38.  T rue
SOL-215  
  CH.SOL- 8.39.
False. Divided by the training samples, not the number of
incorr ectly classified samples  .
Learning logical gates
SOL-216  
  CH.SOL- 8.40.
1
.The values ar e pr esented in the following table (  8.61  ):
F IGURE  8.61: Logical AND: B=-2.52
.The values ar e pr esented in the following table (  8.62  ):
F IGURE  8.62: Logical AND: B=-0.25
3
.The per ceptr on learning rule is an algorithm that can
automatically compute optimal weights for the per ceptr on  .
4
.The main addition by [  22  ] and [  18  ] w as the intr odu ction of a
differ entiable activation function  .
5
.if we select w1 = 1;w2 = 1 and thr eshold=1. W e get:
Or summarized in a table (  8.63  ):F IGURE  8.63: Logical AND gate
8.3.5   Activation functions (rectification)
W e concentrate only on the most commonly used activation
functions, those  which the read er is more likely to encounter or use
during his daily work.
Sigmoid
SOL-217  
  CH.SOL- 8.41.
1
.Remember that the analytical derivative is of the sigmoid:
Code snippet  8.64  pr ovides a pur e C++ based
implementation of the backwar d pass that dir ectly computes
the analytical gradients in C++  .
1 #include  <tor ch/script.h>
2 #include  <vector>
3
4 torch ::  T ensor sigmoid001_d(torch ::  T ensor &  x) {
5 torch ::  T ensor s =  sigmoid001(x);
6 r eturn  ( 1 -  s) *  s;
7 }
F IGURE  8.64: Backward pass for the Sigmoid function using Libtorch  .
2
.Code snippet  8.65  depicts one way of printing the values  .
1 #include  <tor ch/script.h>
2 #include  <vector>
3 int  main  () {
4 std ::  vector <  float  >  v{ 0.0  , 0.1  , 0.2  , 0.3  ,
↪    0.4  , 0.5  , 0.6  , 0.7  , 0.8  , 0.9  , 0.99  };
5 for  ( auto  it =  v .begin(); it !=  v .end(); ++  it) {
6 torch ::  T ensor t0 =  torch ::  tensor(( *  it));
7 std ::  cout <<  ( *  it) <<  ","  <<
↪    sigmoid001(t0).data().detach().item()8 .toFloat() <<  ","
9 <<  sigmoid001_d (t0).data().detach().item().toFloat()
1
0<<  '\n'  ;
1
1}
1
2}
F IGURE  8.65: Evaluation of the sigmoid and its derivative in C++ using Libtorch  .
3
.The manual derivative of eq.  8.27  is:
4
.The forwar d pass for the Sigmoid function appr oximation eq.
8.27  is pr esented in code snippet  8.66  :
1 #include  <tor ch/script.h>
2 #include  <vector>
3 torch ::  T ensor sig_approx( const  torch ::  T ensor &  x ){
4 torch ::  T ensor sig = 1.0  / ( 1.0 +  torch ::  pow( 2  ,( -1.5*  x)));
5 r eturn  sig;
6 }
F IGURE  8.66: Forward pass for the Sigmoid function approximation in C++ using
Libtorch  .
5
.The values ar e  8.67  : :
1 #include  <tor ch/script.h>
2 #include  <vector>
3 int  main  () {
4 std ::  vector <  float  >  v{ 0.0  , 0.1  , 0.2  , 0.3  ,
↪    0.4  , 0.5  , 0.6  , 0.7  , 0.8  , 0.9  , 0.99  };
5 for  ( auto  it =  v .begin(); it !=  v .end(); ++  it) {
torch ::  T ensor t0 =  torch ::  tensor(( *  it));6
7 std ::  cout <<  ( *  it) <<  ","  <<
↪    sigmoid001(t0).data().detach().item()
8 .toFloat() <<  ","  <<  sig_approx (t0).data().detach().item().
9 toFloat() <<  '\n'  ;
1
0}
F IGURE  8.67: Printing the values for Sigmoid and Sigmoid function approximation in
C++ using Libtorch  .
An the values ar e pr esented in T able  8.2  :
V alue Sig Approx
0 0.5 0.5
0.1 0.524979 0.52597
0.2 0.549834 0.5518
0.3 0.574443 0.577353
0.4 0.598688 0.602499
0.5 0.622459 0.6271 15
0.6 0.645656 0.65109
0.7 0.668188 0.674323
0.8 0.689974 0.69673
0.9 0.710949 0.71824
0.99 0.729088 0.736785
T ABLE  8.2: Computed values for the Sigmoid and the Sigmoid approximation  .
T anh
SOL-218  
  CH.SOL- 8.42.The answers ar e as follows:
1
.The derivative is:
2
.In o r der to implement a PyT o r ch based tor ch.autograd.F
unction function such as tanh, we must pr ovide both the
forwar ds and backwar ds imple mentations. The mechanism
behind t his idiom in PyT or ch i s vis the use of a  context  ,
abbr eviated ctx which is like a s tate manages for automatic
differ entiation. The implementation is depicted in  8.68  :
1 import  tor ch
2
3 class  tanh001  (torch .  autograd .  Function):
4 @staticmethod
5 def  forward  (ctx, x):
6 ctx .  save_for_backward( x )
7 h =  x / 4.0
8 y = 4.0 *  h .  tanh()
9 r eturn  y
1
0
1
1@staticmethod
1
2def  backward  (ctx, dLdy):
1
3x, =  ctx.saved_tensors
1
4h =  x / 4.0
1
5dy_dx =  d_tanh( h )
1
6dLdx =  dLdy *  dy_dx
1
7r eturn  dLdx
1
8
1 def  tanh001_der  (x):9
2
0r eturn  1 / (x.cosh() ** 2  )
F IGURE  8.68: T anh in PyT orch  .
SOL-219  
  CH.SOL- 8.43.
1
.The type of NN is a MultiLayer Per ceptr on or MLP  .
2
.Ther e ar e two hidden layers  .
SOL-220  
  CH.SOL- 8.44.
He i s pa rtially c orr ect, see for example  Understanding the
dif ficulty of training deep feedforward neural networks  [  9  ]  .
SOL-221  
  CH.SOL- 8.45.
Initialize all parameters to a constant zer o value. When we
apply th e tanh function to an input which is very lar ge, the
output which is almost zer o, will be pr opagated to the
r emaining partial derivatives leading to the well known
phenomenon  .
SOL-222  
  CH.SOL- 8.46.
During the back-pr opagation  pr ocess, derivatives ar e
calculated with r espect to  ( W  (1) ) and also  ( W  (2) ). The design
flaw:
i Y our friend initialized all weights and biases to zer o  .
ii Ther efor e any gradient with r espect to  ( W  (2) ) would also
be zer o  .iii Subsequently  , ( W  (2) ) will never be updated  .
iv This would inadvertently cause the derivative with r espect to
( W  (1) ) to be always zer o  .
v Finally , would also never be updated  ( W  (1) ).
ReLU
SOL-223  
  CH.SOL- 8.47.
The ReL U function has the benefit of not saturating for
positive inputs since its derivative is one for any positive value  .
SOL-224  
  CH.SOL- 8.48.
The shape is:
SOL-225  
  CH.SOL- 8.49.
The activation function is a l eaky ReLU which in some
occasions may outperform the ReLU activation function  .
Swish
SOL-226  
  CH.SOL- 8.50.
1
.They intended to find new better -performing activation
functions  .
2
.They had a list of basic mathematical functions to choose
fr om, for instance the exponential families exp(), sin(), min
and max  .
3
.Pr evious r esear ch found se veral activation function
pr operties which  wer e consider ed very useful. For instance,gradient pr eserv ation and non-monotonicity . However the
surprising discovery was that the swish function violates
both of these pr eviously deemed useful pr operties  .
4
.The equation is:
5
.The plot is  8.69
F IGURE  8.69: A plot of the Swish activation function  .
8.3.6   Performance Metrics
Confusion matrix, precision, recall
SOL-227  
  CH.SOL- 8.51.
1
.The values ar e labelled inside  8.27  :F IGURE  8.70: TP , TN, FP , FN  .
2  .
3  .
4  .
ROC-AUC
The area under the receiver operating characteristic (ROC) curve,
8.71  known as the AUC, is currently considered to be the
standard method to assess the accuracy of predictive distribution
models.F IGURE  8.71: Receiver Operating Characteristic curve  .
SOL-228  
  CH.SOL- 8.52.
ROC al lows to attest the r elationship between sensitivity
and specificity of a binary class ifier . Sensitivity or true positive
rate mea sur es t he pr oportion of positives corr ectly classified;
specificity or true negative rate measur es the pr oportion of
negatives corr ectly classified. Conventionally , the true positive
rate tpr is plotted against the f alse positive rate fpr , which is
one minus true negative rate  .
1
.Receiver Operat ing Characteris tics of a classifier shows its
performance a s a trade off between  selectivity and
sensitivity .
2
.It is a plot of  ‘true positives’ vs. the  ‘true negatives’. In
place of  ‘true negatives’, one coul d also use  ‘false positives’
which ar e essentially 1 -  ‘true negatives’.
3
.A typical ROC curve has a concave shape with  (0,0) as the
beginning and  (1,1) as the end point
4
.The ROC curve of a ‘random guess classifier ’, when the
classifier is completely confused and cannot at all
distinguish between the two classes, has an AUC of 0.5, the
‘x = y’ line in an ROC curve plot  .SOL-229  
  CH.SOL- 8.53.
The ROC  curve of an ideal classifier (100% accuracy) has
an AUC  of 1, with 0.0  ‘false positives’ and 1.0  ‘true positives’.
The ROC  curve in our case, is almost ideal, which may indicate
over -fitting of the XGBOOST classifier to the training corpus  .
8.3.7   NN Layers, topologies, blocks
CNN arithmetics
SOL-230  
  CH.SOL- 8.54.
Output dimension: L  × L  × M wher e  
SOL-231  
  CH.SOL- 8.55.
The answers ar e as follows:
1
.Output dimensions:
i tor ch.Size([1, 512, 7, 7])
ii tor ch.Size([1, 512, 16, 16])
iii tor ch.Size([1, 512, 22, 40])
2
.The layer is MaxPool2d  .
SOL-232  
  CH.SOL- 8.56.
The answers ar e as follows:
1
.A convolutional block  8.72  .1 Sequential(
2 ( 0  ): Conv2d( 3  , 64  , kernel_size =  ( 3  , 3  ), stride =  ( 1  , 1  ), padding =  ( 1  ,
↪    1  ))
3 ( 1  ): ReLU(inplace =  T rue  )
4 ( 2  ): MaxPool2d(kernel_size =2  , stride =2  , padding =0  , dilation =1  ,
↪    ceil_mode =  False
5 )
F IGURE  8.72: Convolutional block from the VGG1 1 architecture  .
2
.The shapes ar e as follows:
i tor ch.Size([1, 64, 1 12, 1 12])
ii tor ch.Size([1, 64, 256, 256])
iii tor ch.Size([1, 64, 352, 512])
SOL-233  
  CH.SOL- 8.57.
The VG G1 1 a r chitectur e con tains  seven  convolutional
layers, each followed by a  ReLU  activation functi on, and five
max-polling operations  , eac h r educing the r espective f eatur e
map by a factor of  2  . All convolutional layers have a  3 × 3
kernel. T he first convolutional layer pr oduces  64  channels and
subsequently , as the network deepens, the number of channels
doubles  after each  max-pooling  operation until it r eaches  512  .
Dropout
SOL-234  
  CH.SOL- 8.58.
1
.The observed data, e.g the dr opped neur ons ar e distributed
accor ding to:
Denoting s and f as success and failur e r espectively , we
know that the likelihood is:
W ith the following parameters α  = β  = 1 the beta
distribution acts like Uniform prior:
Hence, the  prior  density is:
Ther efor e the  posterior  is:
2
.In dr opout, in every training epoch, neur ons ar e randomly
pruned with pr obability P  = p sampled fr om a Bernoulli
distribution. During infer ence, all the neur ons ar e used but
their output is m ultiplied by the a-priory pr obability P . This
appr oach r esembles to some degr ee the model averaging
appr oach of bagging  .
SOL-235  
  CH.SOL- 8.59.
The answers ar e as follows:
1
.The idea is true and a solid one  .
2
.The idiom may be exemplified as follows  8.73  :F IGURE  8.73: Equivalence of two consecutive dropout layers
The pr obabilities add up by multiplication at each layer ,
r esulting in a single dr opout layer with pr obability:
Convolutional Layer
SOL-236  
  CH.SOL- 8.60.
The r esult is (  8.74  ):
F IGURE  8.74: The result of applying the filter  .
SOL-237  
  CH.SOL- 8.61.
The r esult is (  8.75  ):F IGURE  8.75: The result of applying a ReLU activation  .
SOL-238  
  CH.SOL- 8.62.
The r esult is (  8.76  ):
F IGURE  8.76: The result of applying a MaxPool layer  .
Pooling Layers
MaxPooling
SOL-239  
  CH.SOL- 8.63.
The answers ar e as follows:
1
.A max-pooling layer is most commonly used after a
convolutional la yer in or der to r educe the spatial size of
CNN featur e maps  .2
.The r esult is  8.77  :
F IGURE  8.77: Output of the MaxPool2d operation  .
SOL-240  
  CH.SOL- 8.64.
1
.In MaxPool2D(2,2), the first parameter is the size of the
pooling operation and the second is the stride of the pooling
operation  .
2
.The BatchNorm2D operation does not change the shape of
the tensor fr om the pr evious layer and ther efor e it is:
tor ch.Size  ([1, 32, 222, 222]).
3
.During the training of a CNN we use model.train() so that
Dr opout layers ar e fir ed. However , in or der to run infer ence,
we woul d like to turn this firing mechanism off, and this is
accomplished by model.eval() instructing the PyT or ch
computation graph not to activate dr opout layers  .
4
.The r esulting tensor shape is:
tor ch.Size  ([1, 32, 55, 55])
If we r eshape the tensor like in line 17 using:
x  = x.view  ( x.size  (0), −  1)
Then the tensor shape becomes:
tor ch.Size  ([1, 96800])
5
.Y es, you should agr ee with him, as depicted by the following
plot  8.78  :F IGURE  8.78: A single MaxPool layer  .
Batch normalization, Gaussian PDF
The Gaussian distribution
SOL-241  
  CH.SOL- 8.65.
The answers ar e as follows:
1
.BN is a method  that normalizes the mean and variance of
each of the elements during training  .
2
.X ∼  N  (0, 1) a m ean of zer o and a variance of one. The
standar d normal distribution occurs when  ( σ  )2 = 1 and µ  =
0.
3
.In or der to normalize we:
i Step one is to subtract the mean to shift the distribution  .
ii Divide a ll the shifted values by their standar d deviation (the
squar e r oot of the variance)  .
4
.In BN, the normalization is applied on an element by
element basis. During training at each epoch, every element
in the batch has  to be shifted and scaled so that it has a zer o
mean and unit variance within the batch  .
SOL-242  
  CH.SOL- 8.66.
1
.One possible r ealization is as follows  8.79  :1 fr om  math  import  sqrt
2 import  math
3 def  normDist  (x, mu, sigSqrt):
4 r eturn  ( 1 /  sqrt( 2 *  math .  pi *  sigSqrt)) *  math.e **  (( -0.5  ) *
↪    (x - mu) ** 2 / sigSqrt)
F IGURE  8.79: Normal distribution in Python: from scratch  .
2
.The derivative is given by  8.80  :
1 scipy .  stats .  norm .  pdf(x, mu, sigma) *  (mu -  x)/sigma **2
F IGURE  8.80: The derivative of a Normal distribution in Python  .
BN
SOL-243  
  CH.SOL- 8.67.
1
.During training of a CNN, when a convolution is being
followed by a B N layer , for each of the thr ee RGB channels
a single separate mean and variance is being computed  .
2
.The mistake he made is using a BN with a batch size of 32,
while the output fr om the convolutional layer is 64  .
Theory of CNN design
SOL-244  
  CH.SOL- 8.68.
T rue  .
SOL-245  
  CH.SOL- 8.69.
All the options may be used to build a CNN  .
SOL-246  
  CH.SOL- 8.70.     While the original paper ([  16
]) s uggests that BN layers be used  before  an activation
function, it is also possible to  use BN after the activation
function. In some cases, it actua lly leads to better r esults ([  4  ])
.
SOL-247  
  CH.SOL- 8.71.
When dr opout is enabled during the training pr ocess, in
or der to  keep the expected output at the same value, the output
of a dr op out layer must be multiplied with this term. Of course,
during infer ence no dr opout is taking place at all  .
SOL-248  
  CH.SOL- 8.72.
1
.The idio m is a b ottleneck layer ([  27  ]), which may act much
like an autoencoder  .
2
.Reducing and then incr easing the activations, may for ce the
MLP to learn a mor e compr essed r epr esentation  .
3
.The ne w ar ch itectur e has far mor e connections and
ther efor e it would be pr one to over -fitting  .
4
.Once such ar chitectur e is an autoencoder ([  28  ])  .
CNN residual blocks
SOL-249  
  CH.SOL- 8.73.
1
.The function  ℱ  is the r esidual function  .
2
.The main idea was to add an  identity  connection which
skips two layers all together  .
SOL-250  
  CH.SOL- 8.74.
1
.The missing parts ar e visualized in (  8.81  )  .
F IGURE  8.81: A resnet CNN block
2
.The symbol r epr esents the addition operator  .
3
.Whenever  ℱ  r eturns a zer o, t hen the input X will r each the
output without being modified. Ther efor e, the term identity
function  .
8.3.8   T raining, hyperparameters
Hyperparameter optimization
SOL-251  
  CH.SOL- 8.75.
The question states that image size is quite lar ge, and the
batch size is 1024, ther efor e it may fail to allocate memory on
the GPU  with an Out Of Memory (OOM) err or message. This
is one of the m ost commonly faced err ors when junior data-
scientist start training models  .
SOL-252  
  CH.SOL- 8.76.1
.Since hs is tuning his Hyperparameters on the validation
set, he would m ost pr obably overfit to the validation set
which h e also used for evaluating the performance of the
model  .
2
.One way would be to amend the  splitting, is by first keeping
a fraction of the training set aside, for instance  0.1, and then
split the  r emaining .90 into a training and a validation set,
for instance 0.8 and 0.1  .
3
.His new appr oach uses GridS ear chCV with 5-fold cr oss-
validation to tune his Hyper -parameters. Since he is using
cr oss va lidation with five folds, his local CV metrics would
better r eflect the performance on an unseen data set  .
SOL-253  
  CH.SOL- 8.77.
In  grid search  , a set of pr e-determined values is selected by
a user for eac h dimension in his sear ch space, and then
thor oughly attempting each and every combination. Naturally ,
with suc h a lar ge sear ch spac e the number of the r equir ed
combinations that need to be evaluated scale exponentially in
the number of dimensions in the grid sear ch  .
In  random search  the m ain differ ence is that the a lgorithm
samples completely random points for each of the dimensions
in th e sear ch sp ace. Random sear ch is usually faster and may
even pr oduce better r esults  .
Labelling and bias
Recommended reading:
“ Added value of double r eading in diagnostic radiology , a
systematic r eview  ” [  8  ].
SOL-254  
  CH.SOL- 8.78.
Ther e is a poten tial for bias in certain settings such as this.
If the whole training set is label led only by a single radiologist,
it may be possible that his pr ofessional history would
inadvertently generate bias into  the corpus. Even if we use theform of radiolo gy r eport r eading known as double r eading it
would not be necessarily true t hat the annotated scans would
be devoid of bias or that the quality would be better [  8  ]  .
V alidation curve ACC
SOL-255  
  CH.SOL- 8.79.
The answers ar e as follows:
1
.A validation curve displays on  a single graph a chosen
hyperparameter on the horizontal axis and a chosen metric
on the vertical axis  .
2
.The hyperparameter is the number of epochs
3
.The quality metric is the err or (1 -accuracy). Accuracy ,
err or = (1`accuracy) or loss ar e typical quality metrics  .
4
.The longer the network is trained, the better it gets on the
training set  .
5
.At some point the network is fit too well to the training data
and loses its cap ability to generalize. While the classifier is
still imp r oving on the training set, it gets worse on the
validation and the test set  .
6
.At th is point the quality curve of the training set and the
validation set diver ge  .
V alidation curve Loss
SOL-256  
  CH.SOL- 8.80.
The answers ar e as follows:
1
.What we ar e witnessing is phenomena entitled a plateau.
This may happen when the optimization pr otocol can not
impr ove the loss for several epochs  .2
.Ther e possible methods ar e:
i Constant
ii Xavier/Glor ot uniform
iii Xavier/Glor ot normal
3
.Good initializat ion would optimally generate activations
that pr o duce initial gradients that ar e lar ger than zer o. One
idea is that the training pr oce ss would conver ge faster if
unit variance is achieved ([  16  ]). Mor eover , weights should
be selected car efully so that:
i They ar e lar ge enough thus pr eventing gradients fr om
decaying to zer o  .
ii They ar e not too lar ge causing activation functions to over
saturate  .
4
.Ther e ar e several ways to r educe the pr oblem of plateaus:
i Add some type of r egularization  .
ii In cases wher ein the plateau happens right at the
beginning, amend the way weights ar e initialized  .
iii Amending the optimization algorithm altogether , for
instance using SGD instead of Adam and vice versa  .
5
.Since the initial LR is alr eady very low , his suggestion may
worsen the situation since the optimiser would not be able to
jump off and escape the plateau  .
6
.In contr ast to accuracy , Log los s has no upper bounds and
ther efor e at times may be mor e d ifficult to understand and to
explain  .
InferenceSOL-257  
  CH.SOL- 8.81.
1
.Usually data augmentation, is a technique that is heavily
used du ring tra ining, especially for incr easing the number
of instances of minority classes. In this case, augmentations
ar e using during infer ence and this method is entitled T est
T ime Augmentation (TT A)  .
2
.Her e ar e several image augmentation methods for TT A, with
two augmentations shown also in PyT or ch  .
Horizontal flip
V ertical flip
Rotation
Scaling
Crops
1 transforms .  HorizolntalFlip(p =1  )(image)
2 transforms .  V erticalFlip(p =1  )(image)
F IGURE  8.82: Several image augmentation methods for TT A  .
SOL-258  
  CH.SOL- 8.82.
i Unseen
ii Overfitting
8.3.9   Optimization, Loss
Stochastic gradient descent, SGD
SOL-259  
  CH.SOL- 8.83.
Ther e is  no r elation to random number generation, the true
meaning is the use of batches during the training pr ocess  .
SOL-260  
  CH.SOL- 8.84.
A lar ger batch size decr eases t he variance of the gradient
estimation of SGD. Ther efor e, if your training loop uses lar ger
batches, the model will conver ge faster . On the other hand,
smaller  batch sizes incr ease the variance, leading to the
opposite phenomena; longer conver gence times  .
Momentum
SOL-261  
  CH.SOL- 8.85.
Momentum intr oduces an extra term which comprises a
moving average  which is used in gradient descent update rule
to exponentially decay the historical gradients Using such term
has been demonstrated to accelerate the training pr ocess ([  1 1
]) r equiring less epochs to conver ge  .
SOL-262  
  CH.SOL- 8.86.
The answers ar e as follows:
1
.The derivative of the logistic activation function is extr emely
small for either negtive or positive lar ge inputs  .
2
.The use of the t anh function does not alleviate the pr oblem
since we can sc ale and translate the sigmoid function to
r epr esent the tanh function:
While the sigmoid function is c entr ed ar ound 0.5, the tanh
activation is centr ed ar ound zer o. Similar to the application of
BN, centring the activations may aid the optimizer conver ge
faster . N ote: the r e is no r elation  to SGD; the issue exists when
using other optimization functions as well  .
SOL-263  
  CH.SOL- 8.87.
The answers ar e as follows:
i T rue  .
ii False. In stochastic gradient descent, the gradient for a
single sample is  quite  differ ent  fr om the actual gradient, so
this gives a mor e  noisy  value, and conver ges  slower
iii T rue  .
iv False. SGD r equir es less memory  .
Norms, L1, L2
SOL-264  
  CH.SOL- 8.88.
1
.The L2 norm  .
2
.The Euclidean distance which is calculated as the squar e
r oot of the sum of differ ences be tween each point in a set of
two points  .
3
.The Manhattan distance is an L1 norm (intr oduced by
Hermann Minkowski) while the Euclidean distance is an L2
norm  .
4
.The Manhattan distance is:
5
.The Euclidean distance is:SOL-265  
  CH.SOL- 8.89.
The PyT or ch implementation is in (  8.83  ). Note that we ar e
allocating tensors on a GPU b ut first they ar e cr eated on a
CPU using numpy . This is also always the interplay between
the CPU  and the GPU when tra ining NN models. Note that this
only wor k if you have GPU ava ilable; in case ther e is no GPU
detected, the code has a fallback to the CPU  .
1 %  reset -  f
2 import  tor ch
3 import  numpy
4
5 use_cuda =  torch .  cuda .  is_available()
6 device =  torch.device( "cuda"  if  use_cuda else  "cpu"  )
7 print  (device)
8 x1np =  numpy .  array([ 6  , 1  , 4  , 5  ])
9 x2np =  numpy .  array([ 2  , 8  , 3  , -1  ])
1
0x1t =  torch .  FloatT ensor(x1np) .  to(device) # Move to GPU if available
1
1x2t =  torch .  FloatT ensor(x2np) .  to(device)
1
2dist =  torch .  sqrt (torch.pow(x1t -  x2t, 2  ) .  sum())
1
3dist
1
4>  cuda
1
5>  tensor( 10.0995  , device =  'cuda:0'  )
F IGURE  8.83: Manhattan distance function in PyT orch  .
SOL-266  
  CH.SOL- 8.90.The L2 loss is suitable for a tar get, or a r esponse variable
that is continuous. On the other hand, in a binary classification
pr oblem using LR we would like the output to match either zer o
or o ne a nd a natural candidate for a loss function is the binary
cr oss-entr opy loss  .
Ref e r e n c e s
[
1
]F . T . B. Fuglede. ‘Jensen-Shannon Diver gence and Hilbert
space embeddin g’. In: IEEE Int Sym. Information Theory
(2004) (cit. on pp. 243, 295  ).
[
2
]C. B ennett. ‘Information Distan ce’. In: IEEE T rans. Pattern
Anal. Inform. Theory  . 4 4:4 (1998), pp . 1407–1423 (cit. on pp.
242, 296  ).
[
3
]B. Bigi. ‘Using Kullback-Leibler Distance for T ext
Categorization’. In: In Pr oceedings of the ECIR-2003, Lectur e
Notes in Computer Science, Springer -V erlag  2633 (2003), pp.
305–319 (cit. on pp. 243, 296  ).
[
4
]G. Chen. Rethinking the Usage of Batch  Normalization and
Dr opout in the T raining of Deep Neural Networks  . 2019.
arXiv: 1905.05928 [cs.LG]  (cit. on p. 322  ).
[
5
]Y . S . Chen et al. ‘Deep photo enhancer: Unpaired learning for
image e nhancement from photographs with gans’. In: IEEE
Confer ence on Computer V ision and Pattern Recognition  .
2018, p. 6306 (cit. on p. 229  ).
[
6
]I. Ci uca and J. A. W are. ‘Layered neural networks as universal
approximators’. In: Computational Intelligence Theory and
Applications  . Ed . by B. Reusch. Berlin, Heidelber g: Springer
Berlin Heidelber g, 1997, pp. 41 1–415 (cit. on p. 302  ).
[
7
]T . Floyd. Digital Fundamentals  . Prentice Hall, 2003 (cit. on p.
250  ).
[
8H. G eijer and M . Geijer . ‘Added value of double reading in
diagnostic radiology , a systematic review’. In: Insights into] Imaging  9 (Mar . 2018). DOI: 10.1007/s13244-018-0599-0  (cit.
on pp. 280, 324, 325  ).
[
9
]X. Glorot and Y . Bengio. ‘Understanding the dif ficulty of
training deep feedfor -ward neural networks’. In: Journal of
Machine Learn ing Resear ch - Pr oceedings T rack  9 (Jan.
2010), pp. 249–256 (cit. on pp. 256, 310  ).
[  10
]S. Gomar , M. Mirhassani and M. Ahmadi. ‘Precise digital
implementations of hyperbolic tanh and sigmoid function’.
In: 2016 50th Asilomar Confer ence on Signals, Systems and
Computers  (2016) (cit. on p. 252  ).
[  1 1
]I. Goodfellow , Y . Bengio an d A. Courville. Adaptive
computation and machine learning. MIT Press, 2016 (cit. on
p. 328  ).
[  12
]J. Gurm eet Sing h Manku. ‘Detecting near -duplicates for web
crawling’. In: Pr oceedings of the 16th International
Confer ence on W orld W ide W eb  (2007), p. 141 (cit. on pp.
242, 243, 296  ).
[  13
]K. He. Deep R esidual Learning for Image Recognition  .
2015. arXiv: 1512.03385  (cit. on p. 277  ).
[  14
]K. He et al. Delving Deep into Rectifiers: Surpassing
Human-Level Performance on ImageNet Classification  .
2015. arXiv: 1502.01852 [cs.CV]  (cit. on pp. 256, 271).
[  15
]A. Ignat ov et al. ‘Dslr -quality photos on mobile devices with
deep convolutional networks’. In: IEEE International
Confer ence on Computer V ision (ICCV)  . 2017, pp. 3297–
3305 (cit. on p. 229  ).
[  16
]S. Iof fe and C. Szegedy . ‘Batch Normalization’. In: CoRR
abs/1502.03167 (2015). arXiv: 1502.03167  (cit. on pp. 271,
322, 326  ).
[  17
]R. Kohavi. ‘A Study of Cross-V alidation and Bootstrap for
Accuracy Estim ation and Model Selection’. In: Mor gan
Kaufmann, 1995, pp. 1 137–1 143 (cit. on pp. 229, 287  ).
[  18 A. Krizhevsky , I. Sutskever and G. E. Hinton. ‘ImageNet] Classification w ith Deep Convolutional Neural Networks’.
In: Advances in Neural Information  Pr ocessing Systems  . Ed.
by F . Pereira et al. V ol. 25. Curr an Associates, Inc., 2012, pp.
1097–1 105 (cit. on pp. 250, 304  ).
[  19
]Libtor ch: The PyT or ch C++ fr ontend is a C++14 library for
CPU and GPU tensor computation  . 2020 (cit. on pp. 252,
254  ).
[  20
]A. Paszke et al. ‘Automatic dif ferentiation in PyT orch’. In:
31st Confer ence on Neural Information Pr ocessing Systems  .
2017 (cit. on pp. 264, 265  ).
[  21
]P . Ramachandran. Sear ching for Activation Functions  . 2017.
arXiv: 1710.05941 [cs.NE]  (cit. on pp. 257, 258  ).
[  22
]D. E. Rumelhart and G. E. Hinton. ‘Learning Representations
by Back Propagating Erro rs’. In: Neur ocomputing:
Foundations of Resear ch  . Cambridge, MA, USA: MIT
Press, 19 88, pp. 696–699 (cit. o n pp. 234, 250, 258, 290, 304
).
[  23
]S. Sengupta et al. ‘Sfsnet: Learning shape, reflectance and
illuminance of faces in the wild’. In: Computer V ision and
Pattern Regognition (CVPR)  . 2018 (cit. on p. 229  ).
[  24
]Z. Shu, E. Y um er and S. Hada p. ‘Neural face editing with
intrinsic image disentangling’. In: Computer V ision and
Pattern Recognition (CVPR) IEEE Confer ence  . 2017, pp.
5444–5453 (cit. on p. 229  ).
[  25
]K. Simo nyan and A. Zisserman. V ery Deep Convolutional
Networks for Lar ge-Scale Image Recognition  . 2014. arXiv:
1409.1556 [cs.CV]  (cit. on pp. 261, 263  ).
[  26
]P . Sl edzinski et al. ‘The current state and future perspectives
of ca nnabinoids  in cancer biolo gy’. In: Cancer Medicine  7.3
(2018), pp. 765–775 (cit. on pp. 264, 265  ).
[  27
]C. Szegedy et al. ‘Inception v4, Inception-ResNet and the
Impact o f Residual Connections  on Learning’. In: ICLR 2016
W orkshop  . 2016 (cit. on p. 322  ).[  28
]P . V incent et al. ‘Extracting and composing robust features
with denoising autoencoders’. In: Pr oceedings of the 25th
international confer ence on Machine learning  . 2008, pp.
1096–1 103 (cit. on p. 322  ).P A R T  V
PRA CTICE EXAMC HAPTER
9     JOB INTER VIEW MOCK EXAM
A man who dar es to waste  one hour  of time has not
discover ed the value of life  .
— Charles Darwin
C o n t e n t s
Rules
Pr oblems
Perceptrons
CNN layers
Classification, Logistic regression
Information theory
Feature extraction
Bayesian deep learning
Stressful events, such as a job interview , prompt concern and anxiety (as
they do for virtually every person), but it’ s the lack of prepara tion that fuels
unnecessary nervousness. Many perceive the interview as a potentially
threatening event. T esting your knowledge in AI using a mock exam, is an
ef fective way to not only identi fying your weaknesses and to p inpointing the
concepts and topics that need brushing up, but also to becoming more relaxed
in si milar situations. Remember  that at the heart of job interview confidence is
feeling relaxed.
Doing this test early enough, gives you a head-start before the actual
interview , so that you can tar get areas that require perfection. The exam
includes questio ns from a wide variety of topics in AI, so that these areas are
recognised and it would then be  a case of solving all the problems in this bookover a period of few months to be properly prepared. Do not worry even if you
can not solve a ny of the problems in the exam as some of t hem are quite
dif ficult.
D E E P  L E A RN I N G  J O B  I N T E R V I E W  M O C K  E X A M
E XAM  I NSTRUCTIONS  :
Y OU SHOULD NOT SEARCH FOR SOLUTIONS ON THE WEB  . M ORE GENERALL Y ,
YOU ARE URGED T O TR Y AND SO L VE THE PROBLEMS WITHOUT CONSUL TING ANY
REFERENCE MA TERIAL, AS WOULD BE THE CASE IN A REAL JOB INTER VIEW  .
9.0.1   Rules
REMARK  : In order to receive credits, you must:
i Show all work neatly .
ii A sheet of formulas and calculators are permitted but not notes or texts.
iii Read the problems CAREFULL Y
iv Do not get STUCK  at any problem (or in local mi nima ...) for too mu
time!
v After completing all problems, a double check is STRONGL Y  advised.
vi Y ou have thr ee hours  to complete all questions.
9 . 1   P r o b l e m s
9.1.1   Perceptrons
PRB-267  
  CH.PRB- 9.1.  [PERCEPTRONS]
The follo wing questions r efer to the MLP depicted in (  9.1  ).The inputs
to the MLP in (  9.1  ) ar e x  1  = 0.9 and x  2  = 0.7 r espectively , and theweights w  1  = −  0.3 and w  2  = 0.15 r espectively . Ther e is a single hidden
node, H  1  . The bias term, B  1 equals  0.001.
F IGURE  9.1: Several nodes in a MLP  .
1
.W e e xamine the mechanism of a single hidden node, H  1  . The inputs
and weig hts go thr ough a linear transformation. What is the value of the
output (out  1 ) observed at the sum node?
2
.What is the r esulting value fr om the application of the sum operator?
3
.Using PyT or ch tensors, verify the corr ectness of your answers  .
9.1.2   CNN layers
PRB-268  
  CH.PRB- 9.2.  [CNN LA YERS]
While r eading a paper about the  MaxPool operation, you encounter the
following code snippet  9.1  of a PyT or ch module that the authors
implemented. Y ou download their pr e-trained model, and examine its
behaviour during infer ence:
1 import  tor ch
2 fr om  tor ch  import  nn
3 class  MaxPool001  (nn .  Module):
4 def  __init__  ( self  ):
5 super  (MaxPool001, self  ) .  __init__  ()6 self  .  math =  torch .  nn .  Sequential(
7 torch .  nn .  Conv2d( 3  , 32  , kernel_size =7  , padding =2  ),
8 torch .  nn .  BatchNorm2d( 32  ),
9 torch .  nn .  MaxPool2d( 2  , 2  ),
1
0torch .  nn .  MaxPool2d( 2  , 2  ),
1
1)
1
2def  forward  ( self  , x):
1
3print  (x .  data .  shape)
1
4x =  self  .  math(x)
1
5print  (x .  data .  shape)
1
6x =  x .  view(x .  size( 0  ), -1  )
1
7print  ( "Final shape:  {}  "  ,x .  data .  shape)
1
8r eturn  x
1
9model =  MaxPool001()
2
0model .  eval()
2
1x =  torch .  rand( 1  , 3  , 224  , 224  )
2
2out =  model .  forward(x)
C ODE  9.1: A CNN in PyT orch
The ar chitectur e is pr esented in  9.2  :F IGURE  9.2: T wo consecutive MaxPool layers  .
Please run the code and answer the following questions:
1
.In MaxPool2D(2,2), what ar e the parameters used for?
2
.After running line 8, what is the r esulting tensor shape?
3
.Why does line 20 exist at all?
4
.In line 9, ther e is a MaxPool2D(2,2) operation, followed by yet a second
MaxPool2D(2,2). What is the r esulting tensor shape after running line
9? and line 10?
5
.A friend  who saw the PyT or ch im plementation, suggests that lines 9 and
10 may  be r eplaced by a single MaxPool2D(4,4,) operation while
pr oducing the exact same r esults. Do you agr ee with him? Amend the
code and test your assertion  .
9.1.3   Classification, Logistic regressionPRB-269  
  CH.PRB- 9.3.  [CLASSIFICA TION, LR]
T o study  factors that affect the survivability of humans infec ted
with COVID19 using logistic r egr ession, a r esear cher considers the
link betw een lung cancer and CO VID19 as a  plausible risk factor . The
pr edictor variable is a count of  removed  pulmonary nodules (Fig.  9.3  )
in the lungs  .
F IGURE  9.3: Pulmonary nodules  .
The r esponse variable Y measu r es whether the patient shows any
r emission (as in the manifestations of a disease, e. g. yes=1, no=0)
when the  pulmon ary nodules count shifts up or down. The output fr om
training a logistic r egr ession classifier is as follows:
Standard
Parameter DF Estimate Error
Intercept 1 -4.8792 1.0732
Pulmonary
nodules1 0.0258 0.0194
1
.Estimate the pr obability of impr ovement when the count of r em oved
pulmonary nodules of a patient is 33  .
2
.Find out the r emoved pulmonary nodules count at which  the
estimated pr obability of impr ovement is 0.5  .3
.Find out  the estimated odds ratio of impr ovement for an incr ease of
1  , in the total r emoved pulmonary nodule count  .
4
.Obtain a 99% confidence interval for the true odds ratio of
impr ovement incr ease of  1  in the total r emoved pulmonary nodule
count. Remember that The most common confidence levels ar e  90%,
95%, 99%, and  99.9%.
Confidence Level z
90% 1.645
95% 1.960
99% 2.576
99.9% 3.291
T ABLE  9.1: Common confidence levels
T able  9.1  lists the z values for these levels  .
9.1.4   Information theory
PRB-270  
  CH.PRB- 9.4.  [INFORMA TION THEOR Y]
This question discusses the li nk between binary classification,
information gain  and decision tr ees. Recent r esear ch suggests that the
co-existence of  influenza  (Fig.  9.4  ) and COVID19 virus may decr ease
the surv ivability of humans infected with the COVID 19 virus. The
data (T able  9.2  ) comprises a training set of featur e vectors with
corr esponding class labels which a r esear cher intents classifying using
a decision tr ee  .
T o s tudy factors affecting  COVID19 eradication  , the deep-
learning r esear c her collects data r egrading two independent binary
variables; θ  1  (T/F) indicating  whether the patient is a female, and θ  2
(T/F) indicating whether the human tested positive for the influenza
virus. The binary r esponse variable, γ, indicates whether eradication
was observed (e.g. eradication=+, no eradication=-)  .F IGURE  9.4: The influenza virus  .
Referring to T able (  9.2  ), ea ch r ow indi cates the observed values,
columns (θ  i  ) d enote  featur es and r ows (<  θ  i  , γ  i  >) denote labelled
instances while class label (γ) denotes whether eradication was
observed  .
γ θ  1 θ  2
+ T T
- T F
+ T F
+ T T
- F T
T ABLE  9.2: Decision trees and the COVID19 virus  .
1
.Describe what is meant by  information gain  .
2
.Describe in your own wor ds how does a decision tr ee work  .
3
.Using log  2  , and the pr ovided dataset, cal culate the sample
entr opy H  (γ).4
.What is the info rmation gain IG  ( X  1  ) H  ( γ  ) − H  ( |θ  1  ) for the
pr ovided training corpus?
PRB-271  
  CH.PRB- 9.5.
What is the entr opy of a  biased coin  ? Suppose a coin is biased
such that the pr obability of ‘heads’ is p  ( x  h  ) = 0.98.
1
.Complete the sentence:  W e can pr edict ‘heads’ for each flip with
an accuracy of [__-_]%  .
2
.Complete the sentence:  If the r esult of the coin toss is ‘heads’, the
amount of Shannon information gained is [___] bits  .
3
.Complete the sentence:  If the r esult of the coin toss is ‘tails’, the
amount of Shannon information gained is [___] bits  .
4
.Complete the sentence:  It is always true that the mor e in formation
is associated with an outcome, the  [more/less]  surprising it is  .
5
.Pr ovided that the ratio of tosses r esulting in ‘heads’ is p  ( x  h  ), and
the ratio of tosses r esulting in ‘tails’ is p  ( x  t  ), and also pr ovided
that p  ( x  h  ) + p  ( x  t  ) = 1, what is the formula for the  average
surprise  ?
6
.What is the value of the  average surprise  in bits?
PRB-272  
  CH.PRB- 9.6.
Complete the sentence:  The r elative entr opy D  ( p  ǁ  q  ) is the
measur e of (a) [___] between two distributions. It can also be
expr essed as a measur e of the (b)[___] of assuming that the
distribution is q when the (c)[___] distribution is p  .
9.1.5   Feature extractionPRB-273  
  CH.PRB- 9.7.  [FEA TURE EXTRACTION]
A data scientist extracts a featur e vector fr om an image using a
pr e-trained ResNet34 CNN (  9.5  )  .
1 import  tor chvision.models  as  models
2 ...
3 res_model =  models .  resnet34(pretrained =  T rue  )
F IGURE  9.5: PyT orch declaration for a pre-trained ResNet34 CNN (simplified)  .
He t hen applies the following a lgorithm, entitled xxx on the ima ge
(  9.2  )  .
C ODE  9.2: An unknown algorithm in C++1 1
1 void  xxx  (std  ::  vector  <  float  >&  arr){
2 float  mod  = 0.0  ;
3 for  (  float  i  : arr) {
4 mod  +=  i  *  i;
5 }
6 float  mag  =  std  ::  sqrt(mod);
7 for  (  float  &  i  : arr) {
8 i  /=  mag;
9 }
1
0}
Which r esults in this vector (  9.6  ):
F IGURE  9.6: A one-dimensio nal 512-element embedding for a single image from the
ResNet34 architecture  .Name the algorithm that he used and explain in detail why he used
it  .
PRB-274  
  CH.PRB- 9.8.
[FEA TURE EXTRACTION]
The following question discusses the method of fixed featur e
extraction fr om layers of the VG G19 ar chitectur e for the classification
of t he COVID19 pathogen. It depicts FE principles which ar e
applicable with minor modificat ions to other CNNs as well. Ther efor e,
if yo u ha ppen to  encounter a si milar question in a job intervie w , you
ar e likely be able to cope with it by utilizing the same logic  .
In (Fig.  9.7  ), 2 differ ent classes of human cells ar e displayed;
infected and not-infected, which wer e curated fr om a dataset  of  4 K
images l abelled by a majority vote of two expert vir ologists. Y our task
is to use FE to corr ectly classify the images in the dataset  .
F IGURE  9.7: A dataset of human cells infected by the COVID19 pathogen  .
T able (  9.3  ) pr esents an incomplete listing of the of the VGG19
ar chitectur e. As depicted, for each layer the number of filter s (i. e.
neur ons with unique set of parameters), learnable parameters (e. g.
weights and biases), and FV size ar e pr esented  .
Layer
name#Filters#Parameter
s# Features
conv4_3 512 2.3M 512
fc6 4,096 103M 4,096
fc7 4,096 17M 4,096
output 1,000 4M -T otal 13,416 138M 12,416
T ABLE  9.3: Incomplete listing of the of the VGG19 architecture
1
.Describe how the VGG19 CNN  may be used as fixed FE fo r a
classification ta sk. In your an swer be as detailed as possi ble
r egar ding the stages of FE and the method used for classification  .
2
.Referring to T able (  9.3  ), suggest  three  differ ent ways in which
featur es can be extracted fr om a trained VGG19 CNN model. In
each cas e, state the extracted featur e layer name and the size of  the
r esulting FE  .
3
.After suc cessfully extracting the  featur es for the 4k images fr om the
dataset, how can you now  classify  the images into their r espective
categories?
9.1.6   Bayesian deep learning
PRB-275  
  CH.PRB- 9.9.  [BA YESIAN DEEP LEARNING]
A r e cently published paper pr esents a new layer for Bayesian
neural networks (BNNs). The layer behaves as follows. During the
feed-forwar d op eration, each of the hidden neur ons H  n  , n  ∊  {1 , 2,}  in
the neural network in (Fig.  9.8  )  may , or may not  fir e, independently
of each other , accor ding to a known prior distribution  .
F IGURE  9.8: Likelihood in a BNN model  .
The cha nce of firing  , γ, is th e sa me for each hidden neur on. Using
the formal definition, calculate the  likelihood function  of each of the
following cases:1
.The hidd en neur on is distributed accor ding to X  ∼  B( n  , γ) random
variable and fir es with a pr obability of  γ. Ther e ar e 100 neur ons
and only 20 ar e  fired  .
2
.The hidd en neur on is distributed accor ding to X  ∼  U  (0, γ) random
variable and fir es with a pr obability of  γ.
PRB-276  
  CH.PRB- 9.10.
During pr egnancy , the Placenta Chorion T est is commonly used for
the diagnosis of her editary diseases (Fig.  9.9  )  .
F IGURE  9.9: Foetal surface of the placenta
Assume, that a new test entitled  the Placenta COVID19  T est has
the e xact same p r operties as the Placenta Chorion T est. The test has a
pr obability of 0.95 of being corr ect  whether or not  a COVID19
pathogen is pr esent. It is know n that 1/100 of pr egnancies r esult in
COVID19 virus being passed to foetal cells. Calculate the pr ob ability
of a test indicating that a COVID19 virus is pr esent  .
PRB-277  
  CH.PRB- 9.1 1.
A person who was unknowingly infected with the COVID19
pathogen takes a walk in a park cr owded with people. Let y be the
number of successful infections in  5 independent social interactions or
infection attempts (trials), wher e the pr obability of “success”
(infecting some one else) is  in each trial. Suppose your prior
distribution for  is as  fol lows: P  ( θ  = 1 /  2) = 0.25, P  ( θ  = 1 /  6) = 0.5,
and P  ( θ  = 1 /  4) = 0.25.1
.Derive the  posterior distribution  p  ( θ|y  ).
2
.Derive the  prior predictive distribution  for y  .P A R T  V I
V OL UME TW OC HAPTER
10     V OL UME TW O - PLAN
Nothing exists until it is measur ed  .
— Niels Bohr , 1985
C o n t e n t s
Intr oduction
AI system design
Advanced CNN topologies
1D CNN’ s
3D CNN’ s
Data augmentations
Object detection
Object segmentation
Semantic segmentation
Instance segmentation
Image classification
Image captioning
NLP
RNN
LSTMGANs
Adversarial attacks and defences
V ariational auto encoders
FCN
Seq2Seq
Monte carlo, ELBO, Re-parametrization
T ext to speech
Speech to text
CRF
Quantum computing
RL
1 0 . 1   I n t r o d u c t i o n
T is important a t the outset to understand we could not possibly
include everything we wanted to include in the first VOLUME of
this series. While the first volum e is meant to introduce many of
the core subjects in AI, the se cond volume takes another step
down that road and includes numerous, more advanced subjects. This is a
short glimpse into the plan fo r VOLUME-2 of this series. T his second
volume focuses on more advanced topics in AI
1 0 . 2   A I  s y s t e m  d e s i g n
1 0 . 3   A d v a n c e d  CN N  t o p o l o g i e s
1 0 . 4   1 D  CN N ’ s
1 0 . 5   3 D  CN N ’ s1 0 . 6   D a t a  a u g m e n t a t i o n s
1 0 . 7   O b j e c t  d e t e c t i o n
1 0 . 8   O b j e c t  s e g m e n t a t i o n
1 0 . 9   S e m a n t i c  s e g m e n t a t i o n
1 0 . 1 0   I n s t a n c e  s e g m e n t a t i o n
1 0 . 1 1   I m a g e  c l a s s i f i c a t i o n
1 0 . 1 2   I m a g e  c a p t i o n i n g
1 0 . 1 3   N L P
1 0 . 1 4   RN N
1 0 . 1 5   L S T M
1 0 . 1 6   G A N s
1 0 . 1 7   A d v e r s a r i a l  a t t a c k s  a n d  d e f e n c e s
1 0 . 1 8   V a r i a t i o n a l  a u t o  e n c o d e r s
1 0 . 1 9   F CN
1 0 . 2 0   S e q 2 S e q
1 0 . 2 1   M o n t e  c a r l o ,  E L BO ,  Re - p a r a m e t r i z a t i o n
1 0 . 2 2   T e x t  t o  s p e e c h
1 0 . 2 3   S p e e c h  t o  t e x t1 0 . 2 4   CRF
1 0 . 2 5   Q u a n t u m  c o m p u t i n g
1 0 . 2 6   RLList of T ables
T umour eradication statistics
Common confidence levels
T umour shrinkage in rats
Probability values of hereditary-disease detection
Decision trees and frogs
Decision trees and Cannabinoids administration
Decision trees and star expansion
Decision trees and radiation therapy
Splitting on θ  1
Splitting on θ  1
Splitting on θ  2
Forward-mode AD table for y  = g  ( x  1  , x  2  ) = ln( x  1  )+ x  1  x  2  evaluated at
( x  1  , x  2  ) = ( e  2 ; π  ) and setting ẋ  1  = 1 to compute 
Forward-mode AD table for y  = g  ( x  1  , x  2  ) = ln( x  1  )+ x  1  x  2  evaluated at
( x  1  , x  2  ) = ( e  2 ; π  ) and setting ẋ  1  = 1 (seed values are mentioned
here: 3) to compute 
ImageNet-pretrained CNNs. Ensembles of these CNN architectures have
been extensively studies and evaluated in various ensembling
approachesIncomplete listing of the VGG19 architecture
Incomplete listing of the VGG1 1 architecture
Computed values for the Sigmoid and the Sigmoid approximation
Common confidence levels
Decision trees and the COVID19 virus
Incomplete listing of the of the VGG19 architectureList of Figures
Examples of two sigmoid functions
Pulmonary nodules (left) and breast cancer (right)
A multi-detector positron scanner used to locate tumours
A dental amalgam
A chain of spherical bacteria
Cannabis
Logistic regression in CPP
A linear model in PyT orch
Logistic regression methods in Python
Logistic regression methods in Python
Logistic regression methods in Python
Odds vs. probability values
Binary entropy
Logistic regression in C++
Histopathology for pancreatic cancer cells
Bosons and fermions: particles with half-integer spin are fermions
Foetal surface of the placenta
The Dercum disease
The New Y ork Stock Exchange
Hedge funds and monkeys
Dialect detection
The Morse telegraph code
The Ebola virus
Likelihood in a BNN model
OnOf fLayer in a BNN model
A Dropout layer (simplified form)
A Bayesian Neural Network Model
The Maxwell-Boltzmann distribution
A QuantumDrop layerThe binomial distribution
Z-score
Conditional probability
V enn diagram of the intersected events A and B in probability space H
Annotated components of the Bayes formula  (eq. 3.23  )
Mutual information
Reflection on the motive power of fire
Natural (ln), binary (log2  ) and common (log10  ) logarithms
Cannabis
Shannon’ s five element communications system
An octahedral dice
Logarithms in information theory
H vs. Probability
Shannon information gain for a biased coin toss
A verage surprise
Entropy before splitting
Entropy before splitting
Mutual Information between H  ( S  ) & H  ( D  )
Intermediate value theorem
A Computation graph with intermediate values as nodes and operations as
arcs
An expression graph for g  ( x  ). Constants are shown in gray , crossed-out
since derivatives should not be propagated to constant operands
An expression graph for g  ( x  ). Constants are shown in gray , crossed-out
since derivatives should not be propagated to constant operands
x  2 Function
Forward pass for the sigmoid function
PyT orch syntax for autograd
A typical binary classification problem
An expression graph for g  ( x  ). Constants are shown in gray , crossed-out
since derivatives should not be propagated to constant operands
An expression graph for g  ( x  ). Constants are shown in gray , crossed-out
since derivatives should not be propagated to constant operands
A computation graph for g  ( x  )
A T angent lineForward and backward for the sigmoid activation function in pure Python  .
Forward and backward for the sigmoid function in Autograd
Forward and backward for the ReLU function in Autograd
Forward and backward for equation  (  5.22  )
Autograd
Autograd
A Computation graph for g  ( x  1  , x  2  ) in  5.1
A derivative graph for g  ( x  1  , x  2  ) in  5.1
Python code- AD of the function g  ( x  1  , x  2  )
Python code- AD of the function g  ( x  1  , x  2  )
Sigmoid in SymPy
Sigmoid gradient in SymPy
Sigmoid gradient in SymPy
SymPy gradient of the Sigmoid() function
SymPy imports
Likelihood function using SymPy
Beta distribution using SymPy
A plot of the Beta distribution
A plot of the Beta distribution
A plot of the Posterior with the provided data samples
A specific ensembling approach
A specific ensembling approach
Sampling approaches
Sampling approaches
PyT orch code snippet for an ensemble
A typical binary classification problem
PyT orch code snippet for an ensemble
PyT orch code snippet for an ensemble
PyT orch code snippet for an ensemble
A learning rate schedule
A one-dimensional 512-element embedding for a single image from the
ResNet34 architecture. While any neural network can be used for FE,
depicted is the ResNet CNN architecture with 34 layers
PyT orch decleration for a pre-trained ResNet34 CNN (simplified)A dataset of 4K histopathology WSI from three severity classes: A, B and
C  .
PyT orch code snippet for extracting the fc  7 layer from a pre-trained
VGG19 CNN model
PyT orch code skeleton for extracting a 512-dimensional FV from a pre-
trained ResNet34 CNN model
PyT orch code skeleton for extracting a 512-dimensional FV from a pre-
trained ResNet34 CNN model
Skin lesion categories.  An exemplary visualization of melanoma
Artistic style transfer using the style of Francis Picabia’ s Udnie painting
PyT orch declaration for a pre-trained ResNet34 CNN
PyT orch code snippet for extracting the fc  7 layer from a pre-trained
VGG19 CNN model
PyT orch code snippet for extracting the fc  7 layer from a pre-trained
VGG19 CNN model
T wo CV approaches
Stratified K-fold
A specific CV approach
A padding approach
A padding approach
Convolution and correlation in python
A 3 by 3 convolution kernel
Convolution and correlation in python
PyT orch declaration for a pre-trained ResNet34 CNN (simplified)
listing
A one-dimensional 512-element embedding for a single image from the
ResNet34 architecture
An unknown algorithm
Jaccard similarity
Several nodes in a MLP
Several nodes in a MLP
A basic MLP
MLP operations
A single layer perceptron
Logical AND gate
Examples of two sigmoid functions and an approximationForward pass for the Sigmoid function using Libtorch
Evaluation of the sigmoid and its derivative using Libtorch
Examples of two tanh functions
A simple NN based on tanh in PyT orch
A small CNN composed of tanh blocks
A small CNN composed of ReLU blocks
A confusion metrics for functioning (N) temperature sensors. P stands for
malfunctioning devices
Receiver Operating Characteristic curve
RUC AUC
XGBOOST for binary classification
CNN arithmetics on the VGG1 1 CNN model
A Dropout layer (simplified form)
A Bayesian Neural Network Model
T wo consecutive Dropout layers
A CNN based classification system
A small filter for a CNN
The result of applying the filter
Input to MaxPool2d operation
T wo consecutive MaxPool layers
Normal distribution in Python
A convolution and BN applied to an RGB image
A mistake in a CNN
A CNN block
A CNN block
A resnet CNN block
Hyperparameters
Pulmonary nodules
A validation curve
Log-loss function curve
A problem with the log-loss curve
Manhattan distance function
Convolution and correlation in python
Convolution and correlation in python
The idea of hashing
MLP operations
MLP operationsMLP operations- values
Hidden layer values, simple MLP
MLP operations- values at the output
MLP operations- Softmax
Logical AND: B=-2.5
Logical AND: B=-0.25
Logical AND gate
Backward pass for the Sigmoid function using Libtorch
Evaluation of the sigmoid and its derivative in C++ using Libtorch
Forward pass for the Sigmoid function approximation in C++ using
Libtorch
Printing the values for Sigmoid and Sigmoid function approximation in
C++ using Libtorch
T anh in PyT orch
A plot of the Swish activation function
TP , TN, FP , FN
Receiver Operating Characteristic curve
Convolutional block from the VGG1 1 architecture
Equivalence of two consecutive dropout layers
The result of applying the filter
The result of applying a ReLU activation
The result of applying a MaxPool layer
Output of the MaxPool2d operation
A single MaxPool layer
Normal distribution in Python: from scratch
The derivative of a Normal distribution in Python
A resnet CNN block
Several image augmentation methods for TT A
Manhattan distance function in PyT orch
Several nodes in a MLP
T wo consecutive MaxPool layers
Pulmonary nodules
The influenza virus
PyT orch declaration for a pre-trained ResNet34 CNN (simplified)
A one-dimensional 512-element embedding for a single image from the
ResNet34 architectureA dataset of human cells infected by the COVID19 pathogen
Likelihood in a BNN model
Foetal surface of the placenta
The Ebola virusAlphabetical Index
A 2D convolution ref1
A 512 dimension embedding ref1
A mathematical theory of communication ref1
A random forest ref1
ACC ref1
Accuracy ref1  , ref2  , ref3
Activation functions ref1  , ref2  , ref3  f., ref4  f., ref5  , ref6  , ref7  , ref8  , ref9
, ref10
Activation layer ref1  , ref2
AD ref1  f., ref2  , ref3  , ref4
Adam ref1
Additivity property ref1
AlexNet ref1  , ref2
Algorithmic dif ferentiation ref1  , ref2  , ref3  , ref4  , ref5  f., ref6  f., ref7  , ref8
Alzheimers disease ref1
Amalgam fillings ref1
Analytical gradients ref1
Analyze a paper ref1
AND logic gate ref1
ANN ref1  , ref2
Annotated probabilities ref1
Annotations ref1
ANNs ref1
ANOV A ref1
Approaches for combining predictors ref1  , ref2
Arithmetic operations ref1  , ref2
Arithmetical methods ref1Artificial neural networks ref1  , ref2
AUC ref1  , ref2
Augmentation ref1
Augmentations ref1
Auto correlation ref1  , ref2
AutoAugment ref1
Autoencoder ref1  , ref2
AutoGrad ref1  , ref2
Autograd ref1  f., ref2  , ref3  f., ref4  f., ref5  , ref6
Automatic dif ferentiation ref1  f., ref2
A veraging and majority voting ref1
Back-propagation in perceptrons ref1  , ref2
Back-propogation ref1
Backprop learning ref1
Backprop learning rule ref1
Backpropagation ref1  , ref2  , ref3
Backpropagation algorithm ref1  , ref2
Backward pass ref1  , ref2  , ref3  , ref4  , ref5  f., ref6  f., ref7  , ref8
Bagging ref1  , ref2  , ref3  , ref4
Basic laws of logarithms ref1
Batch normalization ref1  , ref2
BatchNorm2D ref1  , ref2
Bayes formulae ref1  , ref2
Bayes rule ref1  , ref2  , ref3  , ref4  f.
Bayes theorem ref1  , ref2  , ref3  f., ref4  f f.
Bayesian ref1
Bayesian analysis ref1  , ref2
Bayesian approximation ref1
Bayesian deep learning ref1  , ref2  , ref3
Bayesian dropout ref1
Bayesian inference ref1  , ref2
Bayesian machine learning ref1
Bayesian neural networks ref1  f., ref2
Bayesian paradigm ref1Bayesian statistical conclusions ref1
Bayesian statistics ref1  , ref2
Bernoulli ref1  , ref2
Bernoulli distribution ref1
Bernoulli random variable ref1
Bernoulli trial ref1  f., ref2  , ref3
Beta binomial ref1
Beta binomial distribution ref1  f.
Beta distribution ref1  , ref2  , ref3  , ref4
Beta prior ref1
Bias ref1
Biased coin ref1  , ref2
Biased coin toss ref1  , ref2
Biases ref1
Binary class ref1
Binary classification ref1  , ref2  , ref3  , ref4  , ref5  , ref6  f., ref7
Binary code ref1
Binary logistic regression ref1  , ref2
Binary options ref1  f.
Binary response ref1
Binary response variable ref1  , ref2  , ref3
Binomial ref1  , ref2  , ref3
Binomial distribution ref1  , ref2  , ref3  f., ref4  , ref5  , ref6
Binomial likelihood ref1  , ref2
Binomial random variable ref1  , ref2  f.
Blocks ref1
BN ref1  f., ref2  , ref3  f f.
BNN ref1  f., ref2
BNNs ref1
Bohm and Hiley ref1
Boltzmann ref1  , ref2  , ref3  f., ref4
Boltzmann entropy ref1
Boltzmann’ s constant ref1
Boltzmanns entropy ref1
Boosting ref1  , ref2  , ref3  , ref4  , ref5
Bootstrap aggregation ref1  , ref2
Bosons ref1Bosons and fermions ref1
Bottleneck ref1  , ref2
Brazilian rain forest ref1
Breast cancer ref1
Calculus ref1  , ref2  f., ref3
Calculus in deep learning ref1
Cancer ref1  , ref2  , ref3
Cannabinoids ref1
Cannabis ref1
CDC ref1
CE ref1
Chain of spherical bacteria ref1
Chain rule ref1
Chaotic distribution ref1
CI ref1
Class probabilities ref1  , ref2
Classic bagging ref1
Classic logistic regression ref1
Classic normalization ref1
Classical committee machines ref1
Classical machine learning ref1
Classical probability ref1
Classification ref1  , ref2  , ref3  , ref4  , ref5
Classification and information gain ref1  , ref2
Claud Shannon ref1
CM ref1  , ref2
CNN ref1  , ref2  , ref3  , ref4  f., ref5  , ref6  , ref7  , ref8  , ref9  , ref10  , ref1 1
CNN arithmetics ref1
CNN as Fixed Feature Extractor ref1  , ref2
CNN classifiers ref1
CNN feature extraction ref1
CNN layers ref1
CNN model predictions ref1
CNN parameters ref1CNN residual blocks ref1
Coef ficients ref1  , ref2  , ref3
Cof fee consumption ref1
Coin toss ref1
Coin toss probabillity ref1
Common confidence levels ref1
Complementary probability ref1
Computational graph ref1
Computational graphs ref1  , ref2  f f., ref3  , ref4
Concave ref1  , ref2
Concave and Convex functions ref1
Concavity ref1  , ref2
Concavity of the logarithm ref1
Conditional entropy ref1
Conditional independence ref1
Conditional probability ref1  , ref2  , ref3  , ref4
Confidence intervals ref1
Confusion matrics ref1  , ref2
Confusion matrix ref1
Conjugate prior ref1
Conjugate priors ref1  f., ref2
Content loss ref1  , ref2  , ref3  f.
Conv2D ref1  , ref2
Conv2d layer ref1
Conv4 ref1
Convex ref1  , ref2
Convex down function ref1
Convex functions ref1
ConvNet’ s as fixed feature extractors ref1
Convolution ref1  , ref2  , ref3
Convolution and correlation in python ref1
Convolution complexity ref1
Convolution layer ref1  , ref2
Convolutional layer ref1  , ref2  f.
Convolutional neural network ref1
Convolutional neural networks ref1  , ref2
Correlation ref1  f., ref2Cost ref1
Cost function ref1
Covariance ref1
Covariates ref1
COVID19 ref1  , ref2
CPP ref1  f., ref2  , ref3  , ref4  f., ref5  f., ref6  f., ref7  f f.
CPP hypothesis ref1
CPU ref1  , ref2
CPU tensor ref1
Cross correlation ref1  , ref2
Cross entropy ref1  f., ref2
Cross entropy loss ref1  , ref2
Cross validation ref1  , ref2  , ref3
Cross validation approaches ref1  , ref2
CUDA ref1
CV ref1  , ref2
CV approaches ref1
DAG ref1  , ref2  , ref3  , ref4
Data Science ref1
Decision boundary ref1
Decision tree ref1  f., ref2  , ref3  , ref4  , ref5
Decision trees ref1  f., ref2  , ref3
Decision trees and cannabinoids administration ref1
Deep Learning ref1
Deep learning ref1  , ref2  , ref3  , ref4  , ref5
Deep Learning Job Interviews ref1
Deep learning pipelines ref1
Dental amalgam ref1
Dercum disease ref1
Dif ferentiation ref1  , ref2  f., ref3
Dif ferentiation in deep learning ref1
Direct derivation ref1
Directed Acyclic Graph ref1
Directed acyclic graph ref1Directed acyclic graphs ref1  , ref2
Directional derivative ref1
Directional derivatives ref1
Distribution ref1
DL ref1  , ref2  , ref3  , ref4  , ref5
DL classification pipeline ref1
DL job interviews ref1
DN ref1  , ref2
Double reading ref1  f., ref2
DPN CNN ref1
Dropout ref1  , ref2  , ref3  f., ref4  , ref5  f., ref6  , ref7
Dropout as a bayesian approximation ref1
Dropout in PyT orch ref1
Dropout layer ref1  , ref2  f., ref3  f.
Dropped out neurons ref1
Dual numbers ref1  f f., ref2  f.
Dual numbers in AD ref1  , ref2
Ebola ref1  , ref2  f.
Embedding ref1
Encoded messages ref1
Encrypted communications ref1
Enigma machine ref1
Ensemble averaging ref1
Ensemble learning ref1  , ref2  , ref3
Ensemble methods ref1  , ref2
Ensembling ref1  f f., ref2  , ref3  f., ref4
Entropy ref1  , ref2  , ref3  f., ref4  , ref5  , ref6  , ref7  , ref8  , ref9  , ref10  , ref1 1
Entry ref1
Epidemic ref1
Equiprobable events ref1  f., ref2  , ref3  , ref4
Equiprobable sample ref1
Equivocation ref1
Eradication ref1
Eradication probabillity ref1Euclidean ref1  , ref2
Expansion of stars ref1
Expectation ref1
Expectation and variance ref1  , ref2
Explanatory variable ref1
Exponential family ref1
Fc7 ref1
Feature extraction ref1  f., ref2  f., ref3  , ref4
Feature vector ref1  , ref2
Feature vectors ref1
Feed forward neural networks ref1  , ref2
Fermions ref1
FFNN ref1
Filtering ref1
Filtering kernel ref1
Filters ref1  , ref2
Financial mathematics ref1
Fine tuning CNNs ref1  , ref2
Finite dif ference rule ref1  , ref2
Fisher ref1
Fisher information ref1  , ref2  , ref3  f.
Fisher score ref1
Fliping ref1
Forward mode ref1  , ref2  f., ref3
Forward mode AD ref1  f., ref2  , ref3  , ref4
Forward mode AD table construction ref1  , ref2
Forward pass ref1  , ref2  , ref3  , ref4  , ref5  f., ref6  f., ref7  , ref8
Gausiian distribution ref1  , ref2
Gaussian ref1
Gaussian bell ref1
Gaussian distribution ref1  , ref2Gaussian PDF ref1
General concepts ref1  , ref2
Generalization ref1  , ref2
Generalized delta rule ref1
generalized linear models ref1
GLM ref1
GLMs ref1
GPU ref1  , ref2  , ref3  , ref4
GPU tensor ref1
Gradient ref1  , ref2
Gradient descent ref1  , ref2  , ref3  , ref4  , ref5
Gradient descent algorithm ref1
Gradient descent and backpropagation ref1
Gradients ref1
Gram matrix ref1
Grid search ref1  , ref2
Gum bacteria ref1
GUR ref1
Hereditary disease ref1
Hereditary diseases ref1
Hessian ref1
Hessian matrix ref1
Heterogeneous ensembling ref1  , ref2
Hidden layer ref1  , ref2
Hidden layers ref1
Hidden node ref1  , ref2  , ref3
Hinton ref1
Histopathology ref1  , ref2  , ref3
Huang1704snapshot ref1
Human voice activity ref1
Hyperbolic tangent ref1
Hyperparameter optimization ref1  , ref2  f.
Hyperparameters ref1
Hypotheis ref1Ideal classifier ref1  , ref2
Identity connection ref1
Image analysis ref1
Image and text similarity ref1
Image processing ref1
ImageNet ref1  f., ref2  f.
ImageNet pre trained CNNs ref1
ImageNet pretrained CNN classifiers ref1
Improper prior ref1
Independent binary co variates ref1
Independent events ref1
Independent variables ref1  , ref2
Individual predictions ref1
Inductive inference ref1
Inference ref1  , ref2
Information gain ref1  f., ref2  , ref3  , ref4
Information gain values ref1
Information matrix ref1
Information theory ref1  , ref2  , ref3  , ref4  , ref5  , ref6
Interactions ref1
Intermediate value theorem ref1
Intersected events ref1
Introduction ref1  , ref2  , ref3  , ref4  , ref5  , ref6
Jacard similarity ref1
JAX ref1  , ref2
Jensen ref1  , ref2
Jensen’ s inequality ref1  , ref2
Job Interview ref1
John von Neumann ref1
Joint distribution ref1
Jupyter notebook ref1K Fold cross validation ref1
K way FC layer ref1
K-Fold cross validation ref1
Kaggle ref1  , ref2
Kaggle competitions ref1
Kaiming ref1
Kernel ref1
Kernels ref1  , ref2
KL diver gence ref1  , ref2  , ref3  f., ref4
KLD ref1  , ref2  , ref3
Kullback Leibler ref1
Kullback Leibler diver gence ref1  , ref2  , ref3
L1 ref1  , ref2
L2 ref1  , ref2  f.
Labelling and bias ref1
LaT eX ref1  , ref2
Law of total probability ref1  , ref2  , ref3
Laws of data compression ref1
LDCT ref1
Leaky ReLU ref1
Learning logical gates ref1
Learning rate schedules in ensembling ref1  , ref2
Leave one out CV ref1
Leave-one-out CV ref1
Libtorch ref1  f., ref2  , ref3  f f.
Likelihood ref1  f., ref2
Likelihood function ref1  , ref2  , ref3  , ref4  , ref5
Likelihood parameter ref1
Limits and continuity ref1  , ref2
Linear classifiers ref1
Linear combination of regression ref1
Linear decision boundary ref1Linear logistic regression model ref1
Linear model in PyT orch ref1
Linear regression ref1
Linear transformation ref1
Linearity ref1
Link function ref1
Local minima ref1  , ref2
Log likelihood ref1
Log likelihood function ref1  , ref2  , ref3  , ref4
Log loss ref1
Log odds ref1  f., ref2  f., ref3  f., ref4
Logarithm ref1  , ref2  , ref3
Logarithmic function ref1
Logarithms ref1  f., ref2  f., ref3
Logarithms in information theory ref1
Logic gate ref1
Logical gates ref1  , ref2
Logistic ref1
Logistic inverse ref1
Logistic regression ref1  , ref2  f f., ref3  f., ref4  , ref5  , ref6  , ref7
• Sigmoid ref1
Logistic regression classifier ref1
Logistic regression coef ficients ref1
Logistic regression implementation ref1  f.
Logistic regression in C++ ref1
Logistic regression in Python ref1
Logistic regression model ref1  , ref2
Logistic regression predictor variable ref1
Logistic regression threashold ref1
Logistic response function ref1
Logit ref1  , ref2
Logit equation ref1
Logit function ref1  , ref2  , ref3
Logit inverse ref1
Logit transformation ref1
Logit value ref1
LOOCV ref1  , ref2Loss ref1
Loss function ref1
Low model generalization ref1
Low standard error ref1
Lower entropy ref1
LR ref1  , ref2  , ref3  , ref4  , ref5  , ref6
LR coef ficients ref1
Lung cancer ref1  , ref2
M.Sc in Artificial Intelligence ref1
Machine learning ref1  f., ref2  , ref3  , ref4
Machine learning terminology ref1
MacLaurin expansion ref1
MacLaurin series ref1  f.
Magna Carta ref1
Majority voting ref1  , ref2  , ref3
Malignant tumour ref1
Malignant tumours ref1
Manhattan ref1  , ref2
Manual dif ferentiation ref1  , ref2
Masters programme ref1
Masters programme in Artificial Intelligence ref1
MathJax ref1
Maximum likelihood estimatator ref1
Maximum likelihood estimation ref1  , ref2
Maxpool2D ref1  , ref2
MaxPooling ref1
Maxwell Boltzmann distribution ref1
Maxwell distribution ref1
Mean filter ref1
Mean square error ref1
Measurement vector ref1
Mechanical statistics ref1  , ref2  , ref3
Medical AI ref1
Melanoma ref1Migraine probabillity ref1
MinHash ref1
ML ref1
MLE ref1  , ref2  , ref3  f., ref4
MLP ref1  f f., ref2  , ref3  , ref4
Momentum ref1
Monolithic and heterogeneous ensembling ref1  , ref2
Monolithic architectures ref1
Monolithic ensembling ref1
Monotonically increasing function ref1
Monte Carlo dropout ref1
MSE ref1  , ref2  , ref3
Multi class responses ref1
Multi Layer Perceptrons ref1
Multi layer perceptrons ref1
Multi model ensembling ref1  , ref2
Multiclass classification ref1
Multiclass classification problems ref1
Multivariable ref1
Multivariable methods ref1
Mutual information ref1  , ref2  , ref3  f f., ref4  , ref5
Mutual information formulae ref1
N dimensional feature vector ref1
Natural logistic function ref1
Natural logistic sigmoid ref1
Negative log likelihood ref1
Neural network ref1  , ref2
Neural network ensembles ref1  , ref2
Neural networks ref1  , ref2  , ref3  , ref4  , ref5  , ref6
Neural style transfer ref1  f.
Neuron activation function ref1
New Y ork stock exchange ref1
NLL ref1
NN ref1  , ref2  , ref3  , ref4  , ref5  , ref6NN Layers ref1
Noise ref1
Non convex neural networks ref1
Non informative prior ref1
Non informative priors ref1
Non interacting identical particles ref1
Non linearity ref1
Non-dif ferentiable ref1
Non-linearity ref1
Nonlinear layer ref1  , ref2
Normal distribution ref1  , ref2
Normalization constant ref1
NST 212 f.
Numerical Dif ferentiation ref1
Numerical dif ferentiation ref1  f f., ref2  , ref3
Numerical instability ref1
Numpy ref1
Octahedral dice ref1
Odds ref1  f., ref2
Odds of success in a binary response ref1
OnOf fLayer ref1  , ref2
OOM ref1  , ref2
Optimization ref1  , ref2
Optimization loss ref1
Ordinary predictors ref1
Out of memory ref1  , ref2
Overfitting ref1  , ref2  , ref3
P value ref1
Padding ref1  , ref2
Pancreactic cancer ref1
Pancreatic cancer classification ref1
Partial derivative ref1Partial derivatives ref1  f f., ref2  , ref3
Particle physics ref1
PDF ref1
Perceptron ref1  , ref2
Perceptron learning rule ref1
Perceptrons ref1  , ref2  , ref3  , ref4
Performance metrics ref1
Physical constants ref1
Placenta Chorion T est ref1
Placenta chorion test ref1  f.
Planck’ s constant ref1
Plateau ref1
MPF ref1  , ref2
Poisson ref1
Poisson distribution ref1
Pooling Layer ref1
Pooling layer ref1
Posterior ref1
Posterior and prior predictive distributions ref1
Posterior distribution ref1  , ref2  , ref3  , ref4
Posterior predictive distributions ref1
Pre trained CNN ref1
Pre trained CNNs ref1
Pre trained VGG19 CNN model ref1
Precision ref1
Predictor variables ref1
Prior ref1
Prior distribution ref1
Prior distributions ref1
Prior predictive distribution ref1  , ref2
Probabilistic programming ref1
Probability distribution ref1  , ref2
Probability mass function ref1  , ref2
Probability of failure ref1
Probability space ref1
Probability statements ref1
Problems ref1  , ref2  , ref3Proton theraphy ref1
Proton therapy ref1  , ref2
PT ref1  , ref2
Pulmonary nodules ref1  , ref2
PyMc3 ref1
Python ref1  , ref2  , ref3  f., ref4  , ref5  f., ref6  , ref7  , ref8  f., ref9  , ref10  ,
ref1 1  , ref12  f., ref13  f f., ref14  f f., ref15  , ref16  f f., ref17  , ref18  , ref19
, ref20  , ref21  , ref22  , ref23  , ref24  , ref25  , ref26  , ref27  f., ref28  ,
ref29  , ref30  f., ref31  , ref32  f f., ref33  f., ref34  f., ref35  f., ref36  f f.,
ref37  , ref38  , ref39  , ref40  f., ref41  , ref42  , ref43  f., ref44  , ref45  f.,
ref46
Python coin toss ref1
Python interpreter ref1
PyT orch ref1  , ref2  , ref3  f., ref4  f., ref5  , ref6  f., ref7  f f., ref8  , ref9  , ref10
f f., ref1 1  , ref12  , ref13  , ref14  , ref15  f., ref16  , ref17  , ref18  f f., ref19  ,
ref20  , ref21  f., ref22  , ref23  , ref24  f., ref25  , ref26  f., ref27  , ref28  ,
ref29  f f., ref30  f., ref31  f., ref32
Pytorch ref1
PyT orch code snippet for an ensemble ref1
PyT orch sequential ref1
PyT orch tanh ref1
Quadratic equation ref1
Quantum drop ref1
Quantum physics ref1
Quantum states ref1
Quantum term speed ref1
Radiation therapy ref1  , ref2
Radiation therapy planning ref1
Radiology ref1
Random guess classifier ref1  , ref2
Random number seeds ref1Random search ref1  , ref2
Recall ref1
Receiver Operating Characteristic ref1
Receiver operating characteristic ref1
Rectification ref1
Relative entropy ref1  f., ref2  , ref3  , ref4
Relative maxima and minima ref1
Relative risk ref1
Relative shrinkage frequency ref1
Relative star expansion frequency ref1
ReLU ref1  f., ref2  , ref3  f., ref4  , ref5  , ref6
Rendering sympy in Google colab ref1
ResNet ref1  , ref2  , ref3  , ref4  , ref5
ResNet152 ref1
ResNet18 ref1
ResNet34 ref1  , ref2  , ref3
ResNet34 CNN ref1
ResNetBottom ref1
ResNets ref1
Response variable ref1  f., ref2  , ref3
Reversing probabilities ref1
ROC ref1  , ref2
ROC AUC ref1
ROC-AUC ref1
Rosenblatt ref1
RR ref1
Russakovsky ref1
Russakovsky 2015 ref1
Saddle points ref1
Same padding ref1  , ref2  , ref3
Sample odds ratio ref1
Sampling approaches ref1
Sampling with replacement ref1  , ref2
Sampling without replacement ref1  , ref2Search space ref1  , ref2
Second derivative test ref1
Seed values in AD ref1  , ref2
Sequential ref1
SGD ref1  f., ref2  f f.
Shannon ref1  f., ref2  , ref3  , ref4  f., ref5
Shannon bit ref1
Shannon’ s famous general formulae ref1
Shannon’ s general formulae ref1
Shift-invariance ref1
Sigmoid ref1  , ref2  , ref3  , ref4  , ref5  , ref6  , ref7  , ref8  , ref9
Sigmoid activation function ref1  , ref2  , ref3  f., ref4
Sigmoid derivative ref1
Sigmoid function ref1
Sigmoid gradient ref1  , ref2
Sigmoid in SymPy ref1
Sigmoidal neuron ref1
Sigmoidal perceptron ref1
Similarity measures ref1
Simple dif ferentiation ref1  , ref2
Single Layer Perceptrons ref1
Single layer perceptrons ref1
Single model based AI systems ref1
Single predictors ref1
Skip connection ref1
Snapshot ensembling ref1  f., ref2  , ref3
Sobel filter ref1
Softmax ref1
softmax ref1
Softmax activation ref1
Softmax activation function ref1
Softmax derivation ref1
Softmax function ref1  , ref2
Softmax layers ref1
Softmax neurons ref1
Solutions ref1
Speech to text ref1Speed of light in vacum ref1
Sperable convolutions ref1  , ref2
Splitting criterion ref1
Stacking ref1  , ref2  , ref3
Stacking and bagging ref1
Stan ref1
Standard deviation ref1
Star density ref1
Star expansion ref1
Static committee machines ref1
Statistical distribution ref1
Statistical independence ref1
Statistical mechanics ref1  , ref2
Stochastic ref1
Stochastic gradient descent ref1  , ref2
Stochastic gradient descent, SGD ref1
Stock markets ref1
Stocks ref1
Stratification ref1  , ref2
Stratified K fold ref1
Stratified K-Fold ref1
Stride ref1  , ref2  , ref3
STT ref1
Style loss ref1  , ref2  , ref3  f.
Style transfer ref1  f., ref2  f.
Supervised learning ref1
Supervised machine learning ref1
Surprise ref1
Swish ref1  , ref2
Symbolic dif ferentiation ref1  f., ref2  f., ref3  f.
SymPy ref1  , ref2  f., ref3  , ref4  , ref5
T anh ref1
tanh ref1  , ref2  f., ref3
T aylor series ref1  , ref2  f.T aylor series and dual numbers ref1
T aylor series expansion ref1  f., ref2
T est set ref1
The backpropagation algorithm ref1
The bayesian school of thought ref1
The beta binomial model ref1  , ref2
The chain rule ref1  , ref2
The convolution operator ref1  , ref2
The correlation operator ref1  , ref2
The gaussian distribution ref1
The gradient descent algorithm ref1
The gram matrix ref1
The hyperplane ref1  , ref2
The Kullback Leibler distance ref1
The Likelihood function ref1
The logit function and entropy ref1
The multi layer perceptron ref1
The Sigmoid ref1
The sigmoid ref1
The sigmoid function ref1
The theory of perceptrons ref1
Theory of CNN design ref1
Thermodynamics ref1  , ref2  , ref3
T opologies ref1
T oxic mercury fumes ref1  f.
T rain validation split ref1
T raining corpus ref1  , ref2
T raining curve curve ref1  , ref2
T raining hyperparameters ref1
T raining validation epoch ref1
T ransformation ref1
T riangle inequality ref1
T rue probability distribution ref1
T ruly understanding LR ref1  , ref2
TTS ref1
T umors ref1
T umour eradication ref1  , ref2T umour shrinkage ref1
T umour shrinkage in rats ref1
T wo dimensional matrix ref1
Uncertainty ref1  f.
Universal function approximators ref1
V alid padding ref1  , ref2  , ref3
V alidation curve ref1  , ref2
V alidation curve ACC ref1
V alidation curve Loss ref1
V alidation set ref1
V anilla linear regression ref1
V anishing gradients ref1
V ariance ref1  f., ref2  , ref3  , ref4  , ref5
V enn diagram ref1  f., ref2
VGG ref1  , ref2
VGG conv43 layer ref1
VGG fc7 layer ref1
VGG Net ref1  , ref2
VGG16 ref1
VGG19 ref1  , ref2  , ref3
VGG19 architecture ref1
VGG19 CNN ref1  , ref2  , ref3
V oting power ref1
V umulative distribution ref1
W ald chi squared test ref1
W eight initialization ref1  , ref2  , ref3  f., ref4  f., ref5
W est African ebola ref1
WSI ref1WW2 ref1
Xavier ref1  , 326